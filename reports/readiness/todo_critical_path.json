{
  "training": [
    {
      "file": "training/quant_qat_int8.py",
      "line": 279,
      "comment": "For now, we'll use a simple approach: quantize the embedding weights",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bfor\\s+now\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "HIGH"
    },
    {
      "file": "training/quant_qat_int8.py",
      "line": 284,
      "comment": "For now, we'll leave it as-is and document the trade-off",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bfor\\s+now\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "training/run_toy_distill.py",
      "line": 178,
      "comment": "For now, assume 8-ball datasets may have them",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bfor\\s+now\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "training/run_toy_distill.py",
      "line": 313,
      "comment": "For now, weight all positions equally but this could be improved",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bfor\\s+now\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "HIGH"
    },
    {
      "file": "training/examples_priority3_integration.py",
      "line": 183,
      "comment": "Compute quality score for teacher output. This is a placeholder - in practice, you would use: - Human evaluation scores - Automated metrics (BLEU, ROUGE, etc.) - Model-based evaluation - Task-specific metrics Args: teacher_output: Teacher model generated text ground_truth: Optional ground truth text for comparison method: Scoring method (\"heuristic\", \"bleu\", \"rouge\", etc.) Returns: Quality score between 0.0 and 1.0",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b.*?:",
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.0,
      "risk": "HIGH"
    },
    {
      "file": "training/examples_priority3_integration.py",
      "line": 225,
      "comment": "This is a simplified BLEU approximation without nltk",
      "confidence": 0.94,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": -0.2,
      "risk": "MEDIUM"
    },
    {
      "file": "training/examples_priority3_integration.py",
      "line": 232,
      "comment": "Unigram precision (simplified BLEU-1)",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "training/examples_priority3_integration.py",
      "line": 239,
      "comment": "Brevity penalty (simplified)",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "training/examples_priority3_integration.py",
      "line": 244,
      "comment": "Simplified BLEU score (unigram precision with brevity penalty)",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "training/losses.py",
      "line": 892,
      "comment": "Combine penalties (sum with equal weights for now)",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bfor\\s+now\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "HIGH"
    },
    {
      "file": "training/distill_kd.py",
      "line": 1277,
      "comment": "For now, check if it exists as a closure variable or use fallback",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bfor\\s+now\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "training/claim_extraction.py",
      "line": 11,
      "comment": "Claim extraction utilities for training dataset generation and loss computation. Provides simplified claim extraction for training purposes, focusing on: - Verifiable content detection - Atomic claim extraction - Claim extraction success rate measurement Reference: CLAIM_EXTRACTION_SKEPTICISM_GUARD_RAILS.md @author: @darianrosebrook",
      "confidence": 0.85,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": -0.5,
      "risk": "MEDIUM"
    },
    {
      "file": "training/claim_extraction.py",
      "line": 20,
      "comment": "Simplified claim representation for training.",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "training/claim_extraction.py",
      "line": 36,
      "comment": "Simplified claim extractor for training purposes. Detects verifiable claims using heuristics: - Factual indicators (dates, quantities, code references) - Structured content (code blocks, lists, JSON) - Atomic statements (single facts, not compound)",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "training/claim_extraction.py",
      "line": 148,
      "comment": "Check if sentence has factual structure (simplified).",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    }
  ],
  "conversion": [
    {
      "file": "coreml/ane_checks.py",
      "line": 82,
      "comment": "Check for placeholder marker",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.0,
      "risk": "HIGH"
    },
    {
      "file": "coreml/probes/compare_probes.py",
      "line": 36,
      "comment": "Run CoreML model inference. Assumes placeholder check already done in main().",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.0,
      "risk": "HIGH"
    },
    {
      "file": "coreml/probes/compare_probes.py",
      "line": 60,
      "comment": "Check for placeholder marker",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.0,
      "risk": "HIGH"
    },
    {
      "file": "coreml/runtime/ane_monitor.py",
      "line": 127,
      "comment": "For now, we'll use a simplified approach:",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))",
          "\\bfor\\s+now\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "coreml/runtime/ane_monitor.py",
      "line": 131,
      "comment": "NOTE: This is an intentional fallback implementation, not a placeholder.",
      "confidence": 0.94,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": -0.2,
      "risk": "HIGH"
    },
    {
      "file": "coreml/runtime/ane_monitor.py",
      "line": 189,
      "comment": "This is a simplified heuristic - production would use more sophisticated analysis",
      "confidence": 0.94,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": -0.2,
      "risk": "CRITICAL"
    },
    {
      "file": "coreml/runtime/constrained_decode.py",
      "line": 150,
      "comment": "This FSM is deliberately simplified: we enforce key/value separators, brackets balance,",
      "confidence": 0.94,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": -0.2,
      "risk": "MEDIUM"
    },
    {
      "file": "coreml/runtime/speculative_decode.py",
      "line": 410,
      "comment": "Simplified threshold-based acceptance (fallback)",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "conversion/export_pytorch.py",
      "line": 108,
      "comment": "For now, keep returning tensor directly - CoreML will name it",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bfor\\s+now\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "conversion/convert_coreml.py",
      "line": 11,
      "comment": "Convert ONNX \u2192 CoreML (mlprogram). Uses public MIL converter API. Usage: python -m conversion.convert_coreml \\ --backend onnx \\ --in onnx/toy.sanitized.onnx \\ --out coreml/artifacts/toy/model.mlpackage \\ --target macOS13 \\ --allow-placeholder   # optional; otherwise fails loud",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.0,
      "risk": "HIGH"
    },
    {
      "file": "conversion/convert_coreml.py",
      "line": 80,
      "comment": "Convert PyTorch model (TorchScript or ExportedProgram) to CoreML. Args: pytorch_model: TorchScript module or torch.export.ExportedProgram output_path: Output path for .mlpackage compute_units: \"all\", \"cpuandgpu\", or \"cpuonly\" target: Deployment target (e.g., \"macOS13\") allow_placeholder: If True, create placeholder on failure instead of raising Returns: Path to converted model or None if placeholder created Note: torch and coremltools are imported inside the function to avoid import errors when these dependencies are not available. This allows the module to be imported even if CoreML conversion dependencies are missing, which is useful for environments that only need PyTorch export functionality.",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b.*?:",
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.3,
      "risk": "HIGH"
    },
    {
      "file": "conversion/convert_coreml.py",
      "line": 419,
      "comment": "Convert ONNX model to CoreML using public MIL converter API. Args: onnx_path: Path to ONNX model file output_path: Output path for .mlpackage compute_units: \"all\", \"cpuandgpu\", or \"cpuonly\" target: Deployment target (e.g., \"macOS13\") allow_placeholder: If True, create placeholder on failure instead of raising Returns: Path to converted model or None if placeholder created",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b.*?:",
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.0,
      "risk": "HIGH"
    },
    {
      "file": "conversion/convert_coreml.py",
      "line": 473,
      "comment": "ONNX is not a supported production path - always create placeholder",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.0,
      "risk": "CRITICAL"
    },
    {
      "file": "conversion/convert_coreml.py",
      "line": 505,
      "comment": "Create a placeholder .mlpackage for smoke tests.",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "conversion/convert_coreml.py",
      "line": 509,
      "comment": "Create .placeholder marker",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "conversion/convert_coreml.py",
      "line": 543,
      "comment": "Smoke test (creates placeholder on failure):",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b.*?:",
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "conversion/make_toy_block.py",
      "line": 45,
      "comment": "Simplified attention for parity testing (no GQA, no RoPE).",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bsimplified\\b(?!(_|\\.|anal|\\sanal|s))"
        ]
      },
      "context_score": 0.0,
      "risk": "MEDIUM"
    },
    {
      "file": "conversion/judge_export_coreml.py",
      "line": 40,
      "comment": "Convert Judge ONNX model to CoreML with INT8 quantization. Note: CoreMLTools does not natively support ONNX\u2192CoreML conversion. For production, convert ONNX\u2192PyTorch first, then use PyTorch\u2192CoreML. INT8 quantization should be applied at the ONNX level before conversion, or use PyTorch quantization APIs. Args: onnx_path: Path to Judge ONNX model output_path: Output path for CoreML model compute_units: Compute units (\"all\", \"cpuandgpu\", \"cpuonly\") target: Deployment target (e.g., \"macOS13\", \"macOS14\") allow_placeholder: If True, create placeholder on failure instead of raising",
      "confidence": 1.0,
      "matches": {
        "explicit_todos": [
          "\\bPLACEHOLDER\\b"
        ]
      },
      "context_score": 0.3,
      "risk": "CRITICAL"
    }
  ]
}