{
  "timestamp": "2025-11-15T09:20:18.505388",
  "version": "1.0",
  "git": {
    "commit": "30d3fc9ebeb0853bbc245f0c0a8eca23c054aec0",
    "branch": "main",
    "dirty": false
  },
  "test_status": {
    "unit": {
      "total": 0,
      "passed": 0,
      "failed": 0,
      "skipped": 0,
      "errors": 0,
      "failures": [],
      "duration_seconds": 30.0
    },
    "integration": {
      "total": 0,
      "passed": 0,
      "failed": 0,
      "skipped": 0,
      "errors": 0,
      "failures": [],
      "duration_seconds": 30.0
    },
    "mutation": {
      "modules_tested": 0,
      "scores": {},
      "survivors": {},
      "timeouts": 0,
      "targets_met": 0,
      "targets_missed": []
    }
  },
  "coverage": {
    "overall": {
      "line_percent": 0.0,
      "branch_percent": 0.0,
      "lines_covered": 0,
      "lines_total": 0,
      "branches_covered": 0,
      "branches_total": 0
    },
    "by_module": {},
    "thresholds": {
      "line": 80.0,
      "branch": 90.0
    },
    "meets_thresholds": false,
    "critical_modules_below_threshold": []
  },
  "todos": {
    "total": 4504,
    "blocking": 0,
    "critical": 0,
    "high_confidence": 4501,
    "in_training_path": [
      {
        "file": "training/quant_qat_int8.py",
        "line": "279",
        "text": "For now, we'll use a simple approach: quantize the embedding weights",
        "confidence": 1.0
      },
      {
        "file": "training/quant_qat_int8.py",
        "line": "284",
        "text": "For now, we'll leave it as-is and document the trade-off",
        "confidence": 1.0
      },
      {
        "file": "training/run_toy_distill.py",
        "line": "178",
        "text": "For now, assume 8-ball datasets may have them",
        "confidence": 1.0
      },
      {
        "file": "training/run_toy_distill.py",
        "line": "313",
        "text": "For now, weight all positions equally but this could be improved",
        "confidence": 1.0
      },
      {
        "file": "training/examples_priority3_integration.py",
        "line": "183",
        "text": "Compute quality score for teacher output. This is a placeholder - in practice, you would use: - Human evaluation scores - Automated metrics (BLEU, ROUGE, etc.) - Model-based evaluation - Task-specific",
        "confidence": 1.0
      },
      {
        "file": "training/examples_priority3_integration.py",
        "line": "225",
        "text": "This is a simplified BLEU approximation without nltk",
        "confidence": 0.94
      },
      {
        "file": "training/examples_priority3_integration.py",
        "line": "232",
        "text": "Unigram precision (simplified BLEU-1)",
        "confidence": 1.0
      },
      {
        "file": "training/examples_priority3_integration.py",
        "line": "239",
        "text": "Brevity penalty (simplified)",
        "confidence": 1.0
      },
      {
        "file": "training/examples_priority3_integration.py",
        "line": "244",
        "text": "Simplified BLEU score (unigram precision with brevity penalty)",
        "confidence": 1.0
      },
      {
        "file": "training/losses.py",
        "line": "892",
        "text": "Combine penalties (sum with equal weights for now)",
        "confidence": 1.0
      },
      {
        "file": "training/distill_kd.py",
        "line": "1277",
        "text": "For now, check if it exists as a closure variable or use fallback",
        "confidence": 1.0
      },
      {
        "file": "training/claim_extraction.py",
        "line": "11",
        "text": "Claim extraction utilities for training dataset generation and loss computation. Provides simplified claim extraction for training purposes, focusing on: - Verifiable content detection - Atomic claim ",
        "confidence": 0.85
      },
      {
        "file": "training/claim_extraction.py",
        "line": "20",
        "text": "Simplified claim representation for training.",
        "confidence": 1.0
      },
      {
        "file": "training/claim_extraction.py",
        "line": "36",
        "text": "Simplified claim extractor for training purposes. Detects verifiable claims using heuristics: - Factual indicators (dates, quantities, code references) - Structured content (code blocks, lists, JSON) ",
        "confidence": 1.0
      },
      {
        "file": "training/claim_extraction.py",
        "line": "148",
        "text": "Check if sentence has factual structure (simplified).",
        "confidence": 1.0
      }
    ],
    "in_conversion_path": [
      {
        "file": "coreml/ane_checks.py",
        "line": "82",
        "text": "Check for placeholder marker",
        "confidence": 1.0
      },
      {
        "file": "coreml/probes/compare_probes.py",
        "line": "36",
        "text": "Run CoreML model inference. Assumes placeholder check already done in main().",
        "confidence": 1.0
      },
      {
        "file": "coreml/probes/compare_probes.py",
        "line": "60",
        "text": "Check for placeholder marker",
        "confidence": 1.0
      },
      {
        "file": "coreml/runtime/ane_monitor.py",
        "line": "127",
        "text": "For now, we'll use a simplified approach:",
        "confidence": 1.0
      },
      {
        "file": "coreml/runtime/ane_monitor.py",
        "line": "131",
        "text": "NOTE: This is an intentional fallback implementation, not a placeholder.",
        "confidence": 0.94
      },
      {
        "file": "coreml/runtime/ane_monitor.py",
        "line": "189",
        "text": "This is a simplified heuristic - production would use more sophisticated analysis",
        "confidence": 0.94
      },
      {
        "file": "coreml/runtime/constrained_decode.py",
        "line": "150",
        "text": "This FSM is deliberately simplified: we enforce key/value separators, brackets balance,",
        "confidence": 0.94
      },
      {
        "file": "coreml/runtime/speculative_decode.py",
        "line": "410",
        "text": "Simplified threshold-based acceptance (fallback)",
        "confidence": 1.0
      },
      {
        "file": "conversion/export_pytorch.py",
        "line": "108",
        "text": "For now, keep returning tensor directly - CoreML will name it",
        "confidence": 1.0
      },
      {
        "file": "conversion/convert_coreml.py",
        "line": "11",
        "text": "Convert ONNX \u2192 CoreML (mlprogram). Uses public MIL converter API. Usage: python -m conversion.convert_coreml \\ --backend onnx \\ --in onnx/toy.sanitized.onnx \\ --out coreml/artifacts/toy/model.mlpackag",
        "confidence": 1.0
      },
      {
        "file": "conversion/convert_coreml.py",
        "line": "80",
        "text": "Convert PyTorch model (TorchScript or ExportedProgram) to CoreML. Args: pytorch_model: TorchScript module or torch.export.ExportedProgram output_path: Output path for .mlpackage compute_units: \"all\", ",
        "confidence": 1.0
      },
      {
        "file": "conversion/convert_coreml.py",
        "line": "419",
        "text": "Convert ONNX model to CoreML using public MIL converter API. Args: onnx_path: Path to ONNX model file output_path: Output path for .mlpackage compute_units: \"all\", \"cpuandgpu\", or \"cpuonly\" target: De",
        "confidence": 1.0
      },
      {
        "file": "conversion/convert_coreml.py",
        "line": "473",
        "text": "ONNX is not a supported production path - always create placeholder",
        "confidence": 1.0
      },
      {
        "file": "conversion/convert_coreml.py",
        "line": "505",
        "text": "Create a placeholder .mlpackage for smoke tests.",
        "confidence": 1.0
      },
      {
        "file": "conversion/convert_coreml.py",
        "line": "509",
        "text": "Create .placeholder marker",
        "confidence": 1.0
      },
      {
        "file": "conversion/convert_coreml.py",
        "line": "543",
        "text": "Smoke test (creates placeholder on failure):",
        "confidence": 1.0
      },
      {
        "file": "conversion/make_toy_block.py",
        "line": "45",
        "text": "Simplified attention for parity testing (no GQA, no RoPE).",
        "confidence": 1.0
      },
      {
        "file": "conversion/judge_export_coreml.py",
        "line": "40",
        "text": "Convert Judge ONNX model to CoreML with INT8 quantization. Note: CoreMLTools does not natively support ONNX\u2192CoreML conversion. For production, convert ONNX\u2192PyTorch first, then use PyTorch\u2192CoreML. INT8",
        "confidence": 1.0
      }
    ],
    "placeholders": 29,
    "mock_data": 1
  },
  "readiness": {
    "status": "partial",
    "score": 35.5,
    "blockers": [
      {
        "category": "coverage",
        "description": "Coverage below thresholds (line: 0.0%, branch: 0.0%)",
        "severity": "medium"
      },
      {
        "category": "todos",
        "description": "15 TODO(s) in training path",
        "severity": "high"
      },
      {
        "category": "todos",
        "description": "18 TODO(s) in conversion path",
        "severity": "high"
      }
    ],
    "warnings": [],
    "recommendations": [
      "Increase line coverage from 0.0% to 80%"
    ]
  },
  "metadata": {
    "python_version": "3.11.13",
    "platform": "Darwin",
    "assessment_duration_seconds": 673.951743
  }
}