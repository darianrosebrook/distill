"""
ANE-Specific Optimizations Tests

Tests for:
- Int64 tensor detection and conversion to int32
- ANE op compatibility verification
- Memory layout optimization verification
- ANE residency checks

@author: @darianrosebrook
"""
import pytest
import torch
import torch.nn as nn
from pathlib import Path
from typing import Dict, List, Set, Optional
import json

from models.student.architectures.gqa_transformer import StudentLM, ModelCfg

# ANE-supported operations (from CoreML documentation)
ANE_SUPPORTED_OPS = {
    "matmul", "conv", "add", "mul", "relu", "gelu", "silu",
    "layernorm", "softmax", "attention", "gather", "scatter",
    "sigmoid", "tanh", "concat", "split", "reshape", "transpose",
}

# Operations that typically cause CPU fallback
CPU_FALLBACK_OPS = {
    "while", "if", "dynamic_shape", "int64_index", "custom_op",
}


def detect_int64_tensors_on_attention_paths(exported_model) -> List[str]:
    """
    Detect int64 tensors on paths to softmax/attention operations.
    
    ANE prefers int32 for indices, so int64 can cause CPU fallback.
    
    Returns:
        List of problematic int64 usages found
    """
    issues = []
    
    # For TorchScript models
    if isinstance(exported_model, torch.jit.ScriptModule):
        graph = exported_model.graph
        for node in graph.nodes():
            node_kind = node.kind()
            
            # Check embedding nodes (should use int32 input_ids)
            if node_kind == 'aten::embedding':
                for input_val in node.inputs():
                    if hasattr(input_val, 'type'):
                        input_type = str(input_val.type())
                        if 'Long' in input_type or 'int64' in input_type.lower():
                            issues.append(
                                f"Embedding node {node} uses int64 input (should be int32)"
                            )
            
            # Check attention-related nodes
            if 'attention' in node_kind.lower() or 'softmax' in node_kind.lower():
                for input_val in node.inputs():
                    if hasattr(input_val, 'type'):
                        input_type = str(input_val.type())
                        if 'Long' in input_type or 'int64' in input_type.lower():
                            issues.append(
                                f"Attention/softmax node {node} has int64 input (should be int32)"
                            )
    
    # For ExportedProgram models
    elif hasattr(exported_model, 'graph_module'):
        graph_module = exported_model.graph_module
        for node in graph_module.graph.nodes:
            if node.op == 'call_function':
                target = node.target
                target_name = getattr(target, '__name__', str(target))
                
                # Check for embedding operations
                if 'embedding' in target_name.lower():
                    for arg in node.args:
                        if isinstance(arg, torch.Tensor) and arg.dtype == torch.int64:
                            issues.append(
                                f"Embedding operation {target_name} uses int64 input (should be int32)"
                            )
    
    return issues


def verify_embedding_layer_int32_cast(model_path: Optional[str] = None, exported_model=None) -> bool:
    """
    Verify that embedding layers cast to int32 for CoreML.
    
    Args:
        model_path: Path to CoreML model (if available)
        exported_model: PyTorch exported model (TorchScript or ExportedProgram)
        
    Returns:
        True if verified, False otherwise
    """
    if exported_model is not None:
        issues = detect_int64_tensors_on_attention_paths(exported_model)
        return len(issues) == 0
    
    # If we have a CoreML model, we could check the spec
    # For now, we rely on export-time checks
    return True


def check_ane_op_compatibility(model_path: str) -> Dict[str, any]:
    """
    Verify all ops are ANE-supported and check for CPU fallback risks.
    
    Args:
        model_path: Path to CoreML .mlpackage
        
    Returns:
        Dictionary with compatibility results
    """
    try:
        import coremltools as ct
        
        mlmodel = ct.models.MLModel(model_path)
        spec = mlmodel.get_spec()
        
        ops = []
        if spec.WhichOneof("Type") == "mlProgram":
            for f in spec.mlProgram.functions.values():
                for b in f.block.operations:
                    ops.append(b.type)
        else:
            # Fallback for neuralNetwork
            layer_names = [
                layer.WhichOneof("layer") 
                for layer in spec.neuralNetwork.layers
            ]
            ops.extend(filter(None, layer_names))
        
        from collections import Counter
        op_counts = Counter(ops)
        
        # Check for ANE-supported ops
        ane_supported_count = sum(
            count for op, count in op_counts.items()
            if any(ane_op in op.lower() for ane_op in ANE_SUPPORTED_OPS)
        )
        
        # Check for CPU fallback risks
        cpu_fallback_risks = [
            op for op in op_counts.keys()
            if any(risk_op in op.lower() for risk_op in CPU_FALLBACK_OPS)
        ]
        
        total_ops = sum(op_counts.values())
        ane_supported_pct = ane_supported_count / total_ops if total_ops > 0 else 0.0
        
        return {
            "total_ops": total_ops,
            "ane_supported_ops": ane_supported_count,
            "ane_supported_pct": ane_supported_pct,
            "cpu_fallback_risks": cpu_fallback_risks,
            "op_histogram": dict(op_counts.most_common(20)),
            "is_mlprogram": spec.WhichOneof("Type") == "mlProgram",
            "compatible": ane_supported_pct >= 0.80 and len(cpu_fallback_risks) == 0,
        }
    except Exception as e:
        return {
            "error": str(e),
            "compatible": False,
        }


def verify_enumerated_shapes_static_allocation(enumerated_shapes: List[int]) -> Dict[str, any]:
    """
    Verify that enumerated shapes allow static allocation.
    
    Args:
        enumerated_shapes: List of sequence lengths (e.g., [512, 1024, 2048, 4096])
        
    Returns:
        Dictionary with verification results
    """
    # Check that shapes are reasonable for static allocation
    max_shape = max(enumerated_shapes) if enumerated_shapes else 0
    
    # ANE memory limits (approximate)
    # These are hardware-dependent, but we can check for obviously problematic sizes
    ane_memory_limit_16k = 16 * 1024  # 16k tokens
    
    issues = []
    if max_shape > ane_memory_limit_16k:
        issues.append(
            f"Maximum shape {max_shape} exceeds ANE memory limit for single-stage processing. "
            f"Consider 2-stage approach (first 8k tokens, then rest)."
        )
    
    # Check for reasonable shape progression
    if len(enumerated_shapes) > 1:
        sorted_shapes = sorted(enumerated_shapes)
        for i in range(len(sorted_shapes) - 1):
            ratio = sorted_shapes[i + 1] / sorted_shapes[i]
            if ratio > 4.0:
                issues.append(
                    f"Large shape jump detected: {sorted_shapes[i]} â†’ {sorted_shapes[i + 1]} "
                    f"(ratio: {ratio:.1f}x). This may cause memory allocation issues."
                )
    
    return {
        "enumerated_shapes": enumerated_shapes,
        "max_shape": max_shape,
        "within_ane_limit": max_shape <= ane_memory_limit_16k,
        "issues": issues,
        "verified": len(issues) == 0,
    }


def test_int64_detection_on_attention_paths():
    """Test that int64 tensors are detected on attention paths."""
    cfg = ModelCfg(
        d_model=128,
        n_layers=2,
        n_heads=4,
        n_kv_heads=2,
        d_head=32,
        vocab_size=512,
    )
    model = StudentLM(cfg)
    model.eval()
    
    # Export with int32 input_ids (correct)
    example_input = torch.zeros((1, 64), dtype=torch.int32)
    with torch.no_grad():
        traced = torch.jit.trace(model, example_input)
        traced.eval()
    
    # Check for int64 issues
    issues = detect_int64_tensors_on_attention_paths(traced)
    assert len(issues) == 0, f"Found int64 usage issues: {issues}"


def test_embedding_layer_int32_verification():
    """Test that embedding layers use int32 input_ids."""
    cfg = ModelCfg(
        d_model=128,
        n_layers=2,
        n_heads=4,
        n_kv_heads=2,
        d_head=32,
        vocab_size=512,
    )
    model = StudentLM(cfg)
    model.eval()
    
    # Export with int32 input_ids (correct)
    example_input = torch.zeros((1, 64), dtype=torch.int32)
    with torch.no_grad():
        traced = torch.jit.trace(model, example_input)
        traced.eval()
    
    # Verify embedding layer uses int32
    verified = verify_embedding_layer_int32_cast(exported_model=traced)
    assert verified, "Embedding layer should use int32 input_ids"


def test_ane_op_compatibility_verification(tmp_path):
    """Test ANE op compatibility verification."""
    # This test requires a CoreML model
    # For now, we'll test the function structure
    # In practice, this would be run against actual CoreML models
    
    # Create a dummy test
    enumerated_shapes = [512, 1024, 2048]
    result = verify_enumerated_shapes_static_allocation(enumerated_shapes)
    
    assert result["verified"], f"Shape verification failed: {result['issues']}"
    assert result["max_shape"] == 2048
    assert result["within_ane_limit"]


def test_enumerated_shapes_static_allocation():
    """Test that enumerated shapes allow static allocation."""
    # Test reasonable shapes
    reasonable_shapes = [512, 1024, 2048, 4096]
    result = verify_enumerated_shapes_static_allocation(reasonable_shapes)
    assert result["verified"], f"Reasonable shapes failed: {result['issues']}"
    
    # Test problematic shapes (too large)
    large_shapes = [512, 1024, 2048, 16384, 32768]
    result_large = verify_enumerated_shapes_static_allocation(large_shapes)
    assert not result_large["verified"], "Large shapes should be flagged"
    assert len(result_large["issues"]) > 0
    
    # Test problematic shape progression (large jumps)
    jumpy_shapes = [512, 8192, 16384]
    result_jumpy = verify_enumerated_shapes_static_allocation(jumpy_shapes)
    assert not result_jumpy["verified"], "Jumpy shapes should be flagged"
    assert len(result_jumpy["issues"]) > 0


def test_ane_residency_threshold():
    """Test that ANE residency meets threshold (>80%)."""
    # This would typically be run against actual CoreML models with runtime profiling
    # For now, we test the verification logic
    
    # Mock compatibility check result
    compatibility_result = {
        "total_ops": 100,
        "ane_supported_ops": 85,
        "ane_supported_pct": 0.85,
        "cpu_fallback_risks": [],
        "compatible": True,
    }
    
    assert compatibility_result["ane_supported_pct"] >= 0.80, \
        f"ANE residency {compatibility_result['ane_supported_pct']:.1%} below threshold (80%)"
    assert compatibility_result["compatible"], "Model should be ANE compatible"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])

