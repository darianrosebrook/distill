"""
Pytest configuration and shared fixtures.
"""
import json
import tempfile
from pathlib import Path
from typing import Generator

import pytest
import torch

from models.student.architectures.gqa_transformer import StudentLM, ModelCfg


@pytest.fixture
def device():
    """Get device for testing."""
    if torch.cuda.is_available():
        return torch.device('cuda')
    elif torch.backends.mps.is_available():
        return torch.device('mps')
    else:
        return torch.device('cpu')


@pytest.fixture
def small_model_cfg():
    """Small model config for faster tests."""
    return ModelCfg(
        d_model=128,
        n_layers=2,
        n_heads=4,
        n_kv_heads=2,
        d_head=32,
        vocab_size=1000,
        rope_theta=10000.0,
        rope_scaling='none',
        dropout=0.0,
    )


@pytest.fixture
def small_model(small_model_cfg, device):
    """Create a small model for testing."""
    model = StudentLM(small_model_cfg)
    model = model.to(device)
    model.eval()
    return model


@pytest.fixture
def temp_jsonl_file() -> Generator[Path, None, None]:
    """Create a temporary JSONL file for testing."""
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        # Write some test data
        test_data = [
            {
                "prompt": "Hello world",
                "teacher_text": "Hello world, this is a test",
                "teacher_logits": None,
            },
            {
                "prompt": "Test prompt 2",
                "teacher_text": "Test response 2",
                "teacher_logits": None,
            },
        ]
        for item in test_data:
            f.write(json.dumps(item) + '\n')
        temp_path = Path(f.name)

    yield temp_path

    # Cleanup
    if temp_path.exists():
        temp_path.unlink()


@pytest.fixture
def sample_batch():
    """Create a sample batch for testing."""
    batch_size = 2
    seq_len = 10
    vocab_size = 1000

    return {
        "input_ids": torch.randint(0, vocab_size, (batch_size, seq_len)),
        "attention_mask": torch.ones(batch_size, seq_len),
        "labels": torch.randint(0, vocab_size, (batch_size, seq_len)),
        "teacher_logits": torch.randn(batch_size, seq_len, vocab_size),
    }


@pytest.fixture
def mock_tokenizer():
    """Create a mock tokenizer for testing."""
    class MockTokenizer:
        def __init__(self):
            self.vocab_size = 1000
            self.pad_token_id = 0
            self.eos_token_id = 1
            self.pad_token = None

        def encode(self, text, add_special_tokens=False, **kwargs):
            """Encode text to token IDs."""
            # Simple mock: tokenize by splitting on spaces
            tokens = text.split()
            token_ids = [abs(hash(t)) % self.vocab_size for t in tokens]

            if add_special_tokens:
                token_ids = [self.eos_token_id] + \
                    token_ids + [self.eos_token_id]

            return token_ids

        def __len__(self):
            """Return vocabulary size."""
            return self.vocab_size

    tokenizer = MockTokenizer()
    tokenizer.pad_token = tokenizer.eos_token_id  # Set pad_token attribute
    return tokenizer


# Integration test fixtures - provide paths or skip if resources not available

@pytest.fixture
def coreml_model_path():
    """Path to CoreML model for golden vector tests."""
    # In CI, this would be set by previous pipeline steps
    # In dev, we skip the test gracefully
    path = "coreml/artifacts/worker/model.mlpackage"
    if not Path(path).exists():
        pytest.skip(
            f"CoreML model not found: {path}. Run 'make coreml-worker' first.")
    return path


@pytest.fixture
def current_metrics_path():
    """Path to current performance metrics for regression testing."""
    # In CI, this would be generated by performance evaluation steps
    # In dev, we skip the test gracefully
    path = "eval/reports/current_performance.json"
    if not Path(path).exists():
        pytest.skip(
            f"Current performance metrics not found: {path}. Run performance evaluation first.")
    return path


@pytest.fixture
def fp16_model_path():
    """Path to FP16 PyTorch model for quantization testing."""
    # In CI, this would be generated by export steps
    # In dev, we skip the test gracefully
    path = "models/student/exported/student_fp16.pt"
    if not Path(path).exists():
        pytest.skip(
            f"FP16 model not found: {path}. Run 'make pytorch-worker' first.")
    return path


@pytest.fixture
def golden_vectors_dir():
    """Directory containing golden vectors for CoreML testing."""
    # In CI, this would be generated during model validation
    # In dev, we skip the test gracefully
    path = "coreml/golden_vectors"
    if not Path(path).exists():
        pytest.skip(
            f"Golden vectors directory not found: {path}. Generate golden vectors first.")
    return path


@pytest.fixture
def baseline_metrics_path():
    """Path to baseline performance metrics for regression testing."""
    # In CI, this would be stored as artifacts from previous successful runs
    # In dev, we skip the test gracefully
    path = "eval/baselines/performance_baseline.json"
    if not Path(path).exists():
        pytest.skip(
            f"Baseline performance metrics not found: {path}. Create baseline first.")
    return path
