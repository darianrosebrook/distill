Command: /Users/darianrosebrook/Desktop/Projects/distill/venv/bin/python -m pytest --cov=training --cov=models --cov=evaluation --cov=conversion --cov-report=json:coverage.json --cov-report=html:htmlcov --cov-report=term-missing --cov-fail-under=0 tests/
Return code: 1
Timestamp: 2025-11-13T16:39:43.215821

================================================================================
STDOUT:
================================================================================
============================= test session starts ==============================
platform darwin -- Python 3.11.14, pytest-9.0.1, pluggy-1.6.0 -- /Users/darianrosebrook/Desktop/Projects/distill/venv/bin/python
cachedir: .pytest_cache
hypothesis profile 'default'
rootdir: /Users/darianrosebrook/Desktop/Projects/distill
configfile: pytest.ini
plugins: anyio-4.11.0, hypothesis-6.147.0, cov-7.0.0
collecting ... collected 1329 items

tests/ci/test_broker_fixtures_hit_rate.py::test_broker_fixtures_hit_rate PASSED [  0%]
tests/ci/test_broker_fixtures_hit_rate.py::test_broker_miss_for_unknown_key PASSED [  0%]
tests/ci/test_sharding_determinism.py::test_stable_shard PASSED          [  0%]
tests/ci/test_sharding_determinism.py::test_stable_shard_distribution PASSED [  0%]
tests/ci/test_sharding_determinism.py::test_select_shard_with_sample_id PASSED [  0%]
tests/ci/test_sharding_determinism.py::test_select_shard_without_sample_id PASSED [  0%]
tests/ci/test_sharding_determinism.py::test_select_shard_single_shard PASSED [  0%]
tests/conversion/test_convert_coreml.py::TestLoadContract::test_load_contract_success PASSED [  0%]
tests/conversion/test_convert_coreml.py::TestLoadContract::test_load_contract_file_not_found FAILED [  0%]
tests/conversion/test_convert_coreml.py::TestLoadContract::test_load_contract_invalid_json PASSED [  0%]
tests/conversion/test_convert_coreml.py::TestConvertPyTorchToCoreML::test_convert_pytorch_to_coreml_success FAILED [  0%]
tests/conversion/test_convert_coreml.py::TestConvertPyTorchToCoreML::test_convert_pytorch_to_coreml_with_ane_optimization FAILED [  0%]
tests/conversion/test_convert_coreml.py::TestConvertPyTorchToCoreML::test_convert_pytorch_to_coreml_ane_incompatible FAILED [  0%]
tests/conversion/test_convert_coreml.py::TestConvertPyTorchToCoreML::test_convert_pytorch_to_coreml_conversion_failure FAILED [  1%]
tests/conversion/test_convert_coreml.py::TestConvertONNXToCoreML::test_convert_onnx_to_coreml_success FAILED [  1%]
tests/conversion/test_convert_coreml.py::TestConvertONNXToCoreML::test_convert_onnx_to_coreml_with_custom_target FAILED [  1%]
tests/conversion/test_convert_coreml.py::TestConvertONNXToCoreML::test_convert_onnx_to_coreml_file_not_found FAILED [  1%]
tests/conversion/test_convert_coreml.py::TestCreatePlaceholder::test_create_placeholder_success FAILED [  1%]
tests/conversion/test_convert_coreml.py::TestCreatePlaceholder::test_create_placeholder_directory_creation PASSED [  1%]
tests/conversion/test_convert_coreml.py::TestMainFunction::test_main_pytorch_conversion FAILED [  1%]
tests/conversion/test_convert_coreml.py::TestMainFunction::test_main_onnx_conversion PASSED [  1%]
tests/conversion/test_convert_coreml.py::TestMainFunction::test_main_conversion_failure_with_placeholder FAILED [  1%]
tests/conversion/test_convert_coreml.py::TestMainFunction::test_main_invalid_backend FAILED [  1%]
tests/conversion/test_convert_coreml.py::TestMainFunction::test_main_contract_not_found FAILED [  1%]
tests/conversion/test_convert_coreml.py::TestANEOptimizations::test_detect_int64_tensors_fallback FAILED [  1%]
tests/conversion/test_convert_coreml.py::TestANEOptimizations::test_check_ane_op_compatibility_fallback FAILED [  1%]
tests/conversion/test_convert_coreml.py::TestANEOptimizations::test_verify_enumerated_shapes_fallback PASSED [  2%]
tests/conversion/test_convert_coreml.py::TestCoreMLConversionIntegration::test_conversion_workflow_pytorch FAILED [  2%]
tests/conversion/test_convert_coreml.py::TestCoreMLConversionIntegration::test_conversion_error_handling PASSED [  2%]
tests/conversion/test_convert_coreml.py::TestCoreMLConversionIntegration::test_version_gate_integration PASSED [  2%]
tests/conversion/test_convert_coreml.py::TestCoreMLConversionIntegration::test_conversion_parameter_validation PASSED [  2%]
tests/conversion/test_export_onnx.py::TestDecodeWrapper::test_decode_wrapper_initialization PASSED [  2%]
tests/conversion/test_export_onnx.py::TestDecodeWrapper::test_decode_wrapper_forward_with_kv_cache PASSED [  2%]
tests/conversion/test_export_onnx.py::TestDecodeWrapper::test_decode_wrapper_forward_empty_cache PASSED [  2%]
tests/conversion/test_export_onnx.py::TestDecodeWrapper::test_decode_wrapper_forward_mixed_cache PASSED [  2%]
tests/conversion/test_export_onnx.py::TestDecodeWrapper::test_decode_wrapper_forward_missing_cache_args PASSED [  2%]
tests/conversion/test_export_onnx.py::TestDecodeWrapper::test_decode_wrapper_forward_single_token FAILED [  2%]
tests/conversion/test_export_onnx.py::TestDecodeWrapper::test_decode_wrapper_forward_batch_processing FAILED [  2%]
tests/conversion/test_export_onnx.py::TestMainFunction::test_main_both_modes FAILED [  2%]
tests/conversion/test_export_onnx.py::TestMainFunction::test_main_prefill_only PASSED [  3%]
tests/conversion/test_export_onnx.py::TestMainFunction::test_main_decode_only FAILED [  3%]
tests/conversion/test_export_onnx.py::TestMainFunction::test_main_config_not_found FAILED [  3%]
tests/conversion/test_export_onnx.py::TestMainFunction::test_main_invalid_config FAILED [  3%]
tests/conversion/test_export_onnx.py::TestMainFunction::test_main_fallback_config FAILED [  3%]
tests/conversion/test_export_onnx.py::TestMainFunction::test_main_model_loading_failure FAILED [  3%]
tests/conversion/test_export_onnx.py::TestMainFunction::test_main_onnx_export_failure FAILED [  3%]
tests/conversion/test_export_onnx.py::TestONNXExportIntegration::test_decode_wrapper_real_model_simulation PASSED [  3%]
tests/conversion/test_export_onnx.py::TestONNXExportIntegration::test_config_parsing_edge_cases FAILED [  3%]
tests/conversion/test_export_onnx.py::TestONNXExportIntegration::test_export_directory_creation PASSED [  3%]
tests/conversion/test_export_onnx.py::TestONNXExportIntegration::test_export_file_naming FAILED [  3%]
tests/conversion/test_export_onnx.py::TestONNXExportIntegration::test_wrapper_cache_handling_edge_cases PASSED [  3%]
tests/conversion/test_export_onnx.py::TestONNXExportIntegration::test_mode_validation PASSED [  3%]
tests/conversion/test_export_onnx.py::TestONNXExportIntegration::test_checkpoint_loading_and_model_creation FAILED [  3%]
tests/conversion/test_export_pytorch.py::TestPrefillWrapper::test_prefill_wrapper_init PASSED [  4%]
tests/conversion/test_export_pytorch.py::TestPrefillWrapper::test_prefill_wrapper_forward_without_halt FAILED [  4%]
tests/conversion/test_export_pytorch.py::TestPrefillWrapper::test_prefill_wrapper_forward_with_halt FAILED [  4%]
tests/conversion/test_export_pytorch.py::TestPrefillWrapper::test_prefill_wrapper_forward_without_attn_mask PASSED [  4%]
tests/conversion/test_export_pytorch.py::TestDecodeWrapper::test_decode_wrapper_init PASSED [  4%]
tests/conversion/test_export_pytorch.py::TestDecodeWrapper::test_decode_wrapper_forward_single_token FAILED [  4%]
tests/conversion/test_export_pytorch.py::TestDecodeWrapper::test_decode_wrapper_forward_with_kv_cache FAILED [  4%]
tests/conversion/test_export_pytorch.py::TestExportPrefill::test_export_prefill_success FAILED [  4%]
tests/conversion/test_export_pytorch.py::TestExportPrefill::test_export_prefill_trace_failure PASSED [  4%]
tests/conversion/test_export_pytorch.py::TestExportPrefill::test_export_prefill_with_halt_head FAILED [  4%]
tests/conversion/test_export_pytorch.py::TestExportDecode::test_export_decode_success FAILED [  4%]
tests/conversion/test_export_pytorch.py::TestExportDecode::test_export_decode_trace_failure PASSED [  4%]
tests/conversion/test_export_pytorch.py::TestExportDecode::test_export_decode_different_configs FAILED [  4%]
tests/conversion/test_export_pytorch.py::TestMainFunction::test_main_success FAILED [  5%]
tests/conversion/test_export_pytorch.py::TestMainFunction::test_main_checkpoint_without_config FAILED [  5%]
tests/conversion/test_export_pytorch.py::TestMainFunction::test_main_checkpoint_not_found PASSED [  5%]
tests/conversion/test_export_pytorch.py::TestModelConfigurations::test_prefill_wrapper_different_batch_sizes FAILED [  5%]
tests/conversion/test_export_pytorch.py::TestModelConfigurations::test_decode_wrapper_different_kv_configs FAILED [  5%]
tests/conversion/test_export_pytorch.py::TestExportEdgeCases::test_export_prefill_minimal_config FAILED [  5%]
tests/conversion/test_export_pytorch.py::TestExportEdgeCases::test_export_decode_minimal_config FAILED [  5%]
tests/conversion/test_export_pytorch.py::TestExportEdgeCases::test_wrapper_parameter_validation FAILED [  5%]
tests/conversion/test_onnx_surgery.py::TestForceInputDtype::test_force_input_dtype_success PASSED [  5%]
tests/conversion/test_onnx_surgery.py::TestForceInputDtype::test_force_input_dtype_not_found FAILED [  5%]
tests/conversion/test_onnx_surgery.py::TestForceInputDtype::test_force_input_dtype_multiple_inputs PASSED [  5%]
tests/conversion/test_onnx_surgery.py::TestForceOutputDtype::test_force_output_dtype_success PASSED [  5%]
tests/conversion/test_onnx_surgery.py::TestForceOutputDtype::test_force_output_dtype_not_found PASSED [  5%]
tests/conversion/test_onnx_surgery.py::TestForceOutputDtype::test_force_output_dtype_multiple_outputs PASSED [  6%]
tests/conversion/test_onnx_surgery.py::TestStripRedundantCasts::test_strip_redundant_casts_no_casts PASSED [  6%]
tests/conversion/test_onnx_surgery.py::TestStripRedundantCasts::test_strip_redundant_casts_redundant_cast FAILED [  6%]
tests/conversion/test_onnx_surgery.py::TestStripRedundantCasts::test_strip_redundant_casts_inference_failure PASSED [  6%]
tests/conversion/test_onnx_surgery.py::TestStripRedundantCasts::test_strip_redundant_casts_with_cast_nodes FAILED [  6%]
tests/conversion/test_onnx_surgery.py::TestCastInt64Initializers::test_cast_int64_initializers_no_initializers PASSED [  6%]
tests/conversion/test_onnx_surgery.py::TestCastInt64Initializers::test_cast_int64_initializers_int64_present FAILED [  6%]
tests/conversion/test_onnx_surgery.py::TestCastInt64Initializers::test_cast_int64_initializers_mixed_types FAILED [  6%]
tests/conversion/test_onnx_surgery.py::TestCastInt64Initializers::test_cast_int64_initializers_no_int64 FAILED [  6%]
tests/conversion/test_onnx_surgery.py::TestRunFunction::test_run_success_without_simplification FAILED [  6%]
tests/conversion/test_onnx_surgery.py::TestRunFunction::test_run_success_with_simplification FAILED [  6%]
tests/conversion/test_onnx_surgery.py::TestRunFunction::test_run_load_failure PASSED [  6%]
tests/conversion/test_onnx_surgery.py::TestRunFunction::test_run_processing_failure PASSED [  6%]
tests/conversion/test_onnx_surgery.py::TestMainFunction::test_main_success FAILED [  6%]
tests/conversion/test_onnx_surgery.py::TestMainFunction::test_main_run_failure FAILED [  7%]
tests/conversion/test_onnx_surgery.py::TestMainFunction::test_main_missing_required_args FAILED [  7%]
tests/conversion/test_onnx_surgery.py::TestONNXSurgeryIntegration::test_complete_surgery_workflow FAILED [  7%]
tests/conversion/test_onnx_surgery.py::TestONNXSurgeryIntegration::test_dtype_forcing_integration PASSED [  7%]
tests/conversion/test_onnx_surgery.py::TestONNXSurgeryIntegration::test_surgery_error_handling PASSED [  7%]
tests/conversion/test_onnx_surgery.py::TestONNXSurgeryIntegration::test_simplification_availability PASSED [  7%]
tests/conversion/test_onnx_surgery.py::TestONNXSurgeryIntegration::test_shape_inference_fallback PASSED [  7%]
tests/conversion/test_onnx_surgery.py::TestONNXSurgeryIntegration::test_initializer_casting_edge_cases PASSED [  7%]
tests/e2e/test_8_ball_pipeline.py::test_8_ball_pipeline_e2e FAILED       [  7%]
tests/e2e/test_code_mode.py::TestCodeModeLargeBlob::test_large_blob_requires_code_mode PASSED [  7%]
tests/e2e/test_code_mode.py::TestCodeModeMultiTool::test_multi_tool_requires_code_mode PASSED [  7%]
tests/e2e/test_code_mode.py::TestCodeModePII::test_pii_requires_code_mode PASSED [  7%]
tests/e2e/test_code_mode.py::TestCodeModeTokenEfficiency::test_code_mode_reduces_tokens PASSED [  7%]
tests/e2e/test_code_mode.py::TestCodeModeExecutionCorrectness::test_execution_correctness_gate PASSED [  8%]
tests/e2e/test_code_mode.py::TestCodeModeSingleToolExemption::test_single_tool_not_penalized PASSED [  8%]
tests/e2e/test_code_mode.py::test_single_small_tool_exempt PASSED        [  8%]
tests/e2e/test_code_mode.py::test_adversarial_printing PASSED            [  8%]
tests/e2e/test_code_mode.py::test_pii_binding_path_no_leak PASSED        [  8%]
tests/e2e/test_latent_reasoning.py::TestLatentReasoningE2E::test_training_with_latent_curriculum PASSED [  8%]
tests/e2e/test_latent_reasoning.py::TestLatentReasoningE2E::test_inference_with_latent_spans PASSED [  8%]
tests/e2e/test_latent_reasoning.py::TestLatentReasoningE2E::test_caws_budget_enforcement PASSED [  8%]
tests/e2e/test_latent_reasoning.py::TestLatentReasoningE2E::test_latent_spans_respect_caws_tier PASSED [  8%]
tests/e2e/test_latent_reasoning.py::TestLatentReasoningE2E::test_toy_halt_head_integration PASSED [  8%]
tests/e2e/test_latent_reasoning.py::TestLatentReasoningE2E::test_toy_training_inference_loop_mismatch PASSED [  8%]
tests/e2e/test_latent_reasoning.py::TestLatentReasoningE2E::test_toy_progressive_curriculum PASSED [  8%]
tests/e2e/test_production_pipeline.py::test_production_pipeline_e2e SKIPPED [  8%]
tests/e2e/test_production_pipeline.py::test_production_pipeline_template PASSED [  9%]
tests/e2e/test_production_shapes.py::test_production_shapes_validation SKIPPED [  9%]
tests/e2e/test_production_shapes.py::test_production_shapes_configuration PASSED [  9%]
tests/e2e/test_production_shapes.py::test_shape_validator_helpers PASSED [  9%]
tests/e2e/test_production_shapes.py::test_production_shape_values[512] PASSED [  9%]
tests/e2e/test_production_shapes.py::test_production_shape_values[1024] PASSED [  9%]
tests/e2e/test_production_shapes.py::test_production_shape_values[2048] PASSED [  9%]
tests/e2e/test_production_shapes.py::test_production_shape_values[4096] PASSED [  9%]
tests/e2e/test_production_shapes.py::test_shape_validation_error_handling PASSED [  9%]
tests/e2e/test_token_reduction.py::TestTokenReductionE2E::test_baseline_direct_cot PASSED [  9%]
tests/e2e/test_token_reduction.py::TestTokenReductionE2E::test_latent_mode_token_reduction PASSED [  9%]
tests/e2e/test_token_reduction.py::TestTokenReductionE2E::test_token_reduction_at_equal_accuracy PASSED [  9%]
tests/e2e/test_token_reduction.py::TestTokenReductionE2E::test_efficiency_curves FAILED [  9%]
tests/e2e/test_toy_ces_measurement.py::TestCESMeasurement::test_ces_baseline_vs_code_mode PASSED [ 10%]
tests/e2e/test_toy_ces_measurement.py::TestCESMeasurement::test_ces_baseline_vs_latent PASSED [ 10%]
tests/e2e/test_toy_ces_measurement.py::TestCESMeasurement::test_ces_combined_milestones FAILED [ 10%]
tests/e2e/test_toy_ces_measurement.py::TestCESMeasurement::test_ces_non_eligible_scenario PASSED [ 10%]
tests/e2e/test_toy_code_mode.py::test_toy_training_without_code_mode FAILED [ 10%]
tests/e2e/test_toy_code_mode.py::test_toy_training_with_code_mode_disabled PASSED [ 10%]
tests/e2e/test_toy_code_mode.py::test_toy_training_with_code_mode_enabled FAILED [ 10%]
tests/e2e/test_toy_code_mode.py::test_toy_training_env_var_doesnt_break PASSED [ 10%]
tests/e2e/test_toy_code_mode.py::test_toy_code_mode_with_span_targets FAILED [ 10%]
tests/e2e/test_toy_code_mode.py::test_toy_code_mode_weight_scheduler_integration FAILED [ 10%]
tests/e2e/test_toy_combined_milestones.py::TestCombinedMilestones::test_training_with_both_features FAILED [ 10%]
tests/e2e/test_toy_combined_milestones.py::TestCombinedMilestones::test_code_mode_with_latent_spans FAILED [ 10%]
tests/e2e/test_toy_combined_milestones.py::TestCombinedMilestones::test_caws_budget_with_code_mode PASSED [ 10%]
tests/e2e/test_toy_combined_milestones.py::TestCombinedMilestones::test_mixed_batch_eligibility FAILED [ 10%]
tests/e2e/test_toy_combined_milestones.py::TestCombinedMilestones::test_full_pipeline_integration FAILED [ 11%]
tests/e2e/test_toy_pipeline.py::test_toy_pipeline_e2e FAILED             [ 11%]
tests/e2e/test_toy_pipeline.py::test_toy_pipeline_with_code_mode FAILED  [ 11%]
tests/e2e/test_toy_pipeline.py::test_toy_pipeline_with_latent_mode FAILED [ 11%]
tests/e2e/test_toy_pipeline.py::test_toy_pipeline_with_both_features FAILED [ 11%]
tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_eight_ball_answers_count PASSED [ 11%]
tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_eight_ball_answers_content PASSED [ 11%]
tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_eight_ball_token_ids_range PASSED [ 11%]
tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_id_to_answer_mapping FAILED [ 11%]
tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_id_to_answer_coverage PASSED [ 11%]
tests/evaluation/test_8ball_eval.py::TestDataClasses::test_prediction_result_creation FAILED [ 11%]
tests/evaluation/test_8ball_eval.py::TestDataClasses::test_evaluation_metrics_creation FAILED [ 11%]
tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_json_file FAILED [ 11%]
tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_text_file FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_nonexistent_file FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_invalid_json PASSED [ 12%]
tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_success FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_empty_questions FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_tokenizer_failure FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_success FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_file_not_found FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_success PASSED [ 12%]
tests/evaluation/test_8ball_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_subprocess_failure FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_invalid_json FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_identical FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_different FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_empty_lists FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_different_lengths FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_token_distribution FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_pytorch_evaluation FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_coreml_evaluation FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_ollama_evaluation FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_invalid_backend FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_missing_eval_file FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_eight_ball_answer_consistency PASSED [ 13%]
tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_prediction_result_validation FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_evaluation_metrics_calculation FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_evaluation_workflow FAILED [ 13%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_within_limits FAILED [ 13%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_exceeds_loc_limit FAILED [ 13%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_exceeds_files_limit FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_empty_diff FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_multiple_files FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_binary_files FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_edge_cases FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_all_pass FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_tests_fail FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_lint_fail FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_coverage_fail FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_missing_fields FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_complete FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_missing_rationale FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_missing_evidence FAILED [ 14%]
tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_no_diff FAILED [ 15%]
tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_whitespace_only FAILED [ 15%]
tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_all_pass FAILED [ 15%]
tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_gates_fail FAILED [ 15%]
tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_provenance_fail FAILED [ 15%]
tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_budget_fail FAILED [ 15%]
tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_multiple_failures FAILED [ 15%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_working_spec_success PASSED [ 15%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_working_spec_not_found FAILED [ 15%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_file_content_success PASSED [ 15%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_file_content_not_found PASSED [ 15%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_json_file_success PASSED [ 15%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_json_file_invalid_json FAILED [ 15%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_tests_success FAILED [ 16%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_tests_failure FAILED [ 16%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_linter_success FAILED [ 16%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_coverage_success FAILED [ 16%]
tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_success FAILED [ 16%]
tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_missing_spec FAILED [ 16%]
tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_test_failure FAILED [ 16%]
tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_complete_caws_evaluation_workflow FAILED [ 16%]
tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_caws_evaluation_with_violations FAILED [ 16%]
tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_budget_adherence_edge_cases FAILED [ 16%]
tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_gate_integrity_thresholds FAILED [ 16%]
tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_provenance_clarity_validation FAILED [ 16%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvalResult::test_claim_extraction_eval_result_creation PASSED [ 16%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvalResult::test_claim_extraction_eval_result_default_values PASSED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvalResult::test_claim_extraction_eval_result_calculated_fields PASSED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluator_initialization_with_extractor PASSED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluator_initialization_default PASSED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_success FAILED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_empty_outputs PASSED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_single_output PASSED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_mismatched_lengths FAILED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_with_none_outputs PASSED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_extractor_failure PASSED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_complete_evaluation_workflow FAILED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_evaluation_with_realistic_outputs PASSED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_metrics_calculation_edge_cases FAILED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_evaluator_reuse PASSED [ 17%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_evaluator_with_different_extractors PASSED [ 18%]
tests/evaluation/test_classification_eval.py::TestClassificationConfig::test_classification_config_creation PASSED [ 18%]
tests/evaluation/test_classification_eval.py::TestClassificationConfig::test_classification_config_validation PASSED [ 18%]
tests/evaluation/test_classification_eval.py::TestPredictionResult::test_prediction_result_creation PASSED [ 18%]
tests/evaluation/test_classification_eval.py::TestPredictionResult::test_prediction_result_without_probabilities PASSED [ 18%]
tests/evaluation/test_classification_eval.py::TestEvaluationMetrics::test_evaluation_metrics_creation FAILED [ 18%]
tests/evaluation/test_classification_eval.py::TestEvaluationMetrics::test_evaluation_metrics_minimal FAILED [ 18%]
tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_success FAILED [ 18%]
tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_file_not_found FAILED [ 18%]
tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_invalid_json FAILED [ 18%]
tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_missing_fields FAILED [ 18%]
tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_success FAILED [ 18%]
tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_empty_questions FAILED [ 18%]
tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_tokenizer_failure FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_success FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_file_not_found FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_success FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_subprocess_failure FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_invalid_json FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_identical FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_different FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_empty_lists FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_different_lengths FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_with_probabilities FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_without_probabilities FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_pytorch_evaluation FAILED [ 19%]
tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_coreml_evaluation FAILED [ 20%]
tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_invalid_backend FAILED [ 20%]
tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_config_not_found FAILED [ 20%]
tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_complete_evaluation_workflow FAILED [ 20%]
tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_evaluation_metrics_calculation FAILED [ 20%]
tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_config_validation PASSED [ 20%]
tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_prediction_result_validation PASSED [ 20%]
tests/evaluation/test_perf_mem_eval.py::TestHardwareInfo::test_hardware_info_creation PASSED [ 20%]
tests/evaluation/test_perf_mem_eval.py::TestHardwareInfo::test_hardware_info_default_export_path PASSED [ 20%]
tests/evaluation/test_perf_mem_eval.py::TestStepAdapter::test_step_adapter_creation FAILED [ 20%]
tests/evaluation/test_perf_mem_eval.py::TestStepAdapter::test_step_adapter_defaults FAILED [ 20%]
tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_macos FAILED [ 20%]
tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_non_macos FAILED [ 20%]
tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_no_coremltools FAILED [ 20%]
tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_single_max PASSED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_multiple_same PASSED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_single_element PASSED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_empty_array FAILED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_negative_values FAILED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_valid_simple PASSED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_valid_complex PASSED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_invalid_syntax FAILED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_missing_fields FAILED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_empty_json PASSED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_non_json PASSED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_success FAILED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_model_load_failure FAILED [ 21%]
tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_no_coremltools FAILED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_success FAILED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_file_not_found FAILED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_invalid_json FAILED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_missing_tokens_field FAILED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestMainFunction::test_main_success FAILED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestMainFunction::test_main_missing_required_args FAILED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_hardware_detection_integration PASSED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_tokenization_workflow FAILED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_greedy_argmax_properties PASSED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_json_validation_comprehensive FAILED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_performance_metrics_calculation PASSED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_end_to_end_workflow_simulation FAILED [ 22%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_error_handling_robustness PASSED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestLoadModel::test_load_model_with_config FAILED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestLoadModel::test_load_model_without_config FAILED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestLoadModel::test_load_model_checkpoint_not_found PASSED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_basic FAILED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_with_eos FAILED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_temperature FAILED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_max_length FAILED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_valid_simple PASSED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_valid_complex PASSED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_invalid_syntax PASSED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_empty_string PASSED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_whitespace_only PASSED [ 23%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_non_json_text PASSED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_partial_json PASSED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_valid_json PASSED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_nested_structure PASSED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_invalid_json PASSED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_no_tool_call PASSED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_empty_json PASSED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_malformed_tool_call PASSED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_multiple_tools PASSED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_success FAILED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_invalid_json FAILED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_wrong_tool FAILED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_model_load_failure PASSED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_empty_test_cases FAILED [ 24%]
tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_success FAILED [ 25%]
tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_checkpoint_not_found FAILED [ 25%]
tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_config_not_found FAILED [ 25%]
tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_json_validation_edge_cases PASSED [ 25%]
tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_tool_call_extraction_variations PASSED [ 25%]
tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_complete_evaluation_workflow FAILED [ 25%]
tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_evaluation_metrics_calculation PASSED [ 25%]
tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_error_handling_robustness PASSED [ 25%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_init_no_limit PASSED [ 25%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_init_with_limit PASSED [ 25%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_add_sample_api_call PASSED [ 25%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_add_sample_cached PASSED [ 25%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_budget_limit_enforcement PASSED [ 25%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_budget_limit_not_exceeded PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_multiple_samples PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_mixed_cached_and_api_samples PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_get_estimate PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_get_status PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_get_status_no_limit PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetTracker::test_budget_exceeded_exact_limit PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetCostCalculation::test_cost_calculation_api_call PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetCostCalculation::test_cost_calculation_cached PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetCostCalculation::test_cost_calculation_fractional_tokens PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetEstimation::test_estimate_small_dataset PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetEstimation::test_estimate_large_dataset PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetEstimation::test_estimate_custom_token_counts PASSED [ 26%]
tests/integration/test_budget_tracking.py::TestBudgetEstimation::test_estimate_matches_actual_cost PASSED [ 27%]
tests/integration/test_budget_tracking.py::TestBudgetLimitScenarios::test_budget_limit_zero SKIPPED [ 27%]
tests/integration/test_budget_tracking.py::TestBudgetLimitScenarios::test_budget_limit_very_small PASSED [ 27%]
tests/integration/test_budget_tracking.py::TestBudgetLimitScenarios::test_budget_limit_large PASSED [ 27%]
tests/integration/test_budget_tracking.py::TestBudgetLimitScenarios::test_budget_exceeded_after_multiple_samples PASSED [ 27%]
tests/integration/test_contextual_pipeline.py::test_full_pipeline_small PASSED [ 27%]
tests/integration/test_contextual_pipeline.py::test_full_pipeline_large PASSED [ 27%]
tests/integration/test_contextual_pipeline.py::test_pipeline_determinism PASSED [ 27%]
tests/integration/test_contextual_pipeline.py::test_determinism_full_pipeline PASSED [ 27%]
tests/integration/test_contextual_pipeline.py::test_pipeline_error_recovery PASSED [ 27%]
tests/integration/test_contextual_pipeline.py::test_pipeline_performance PASSED [ 27%]
tests/integration/test_contextual_pipeline.py::test_pipeline_memory_usage PASSED [ 27%]
tests/integration/test_performance_contextual.py::test_generation_performance PASSED [ 27%]
tests/integration/test_performance_contextual.py::test_extraction_performance PASSED [ 27%]
tests/integration/test_performance_contextual.py::test_verification_performance PASSED [ 28%]
tests/integration/test_performance_contextual.py::test_memory_usage_large_dataset PASSED [ 28%]
tests/integration/test_process_step_integration.py::test_dataset_loads_process_step_targets PASSED [ 28%]
tests/integration/test_process_step_integration.py::test_batch_contains_process_step_targets FAILED [ 28%]
tests/integration/test_process_step_integration.py::test_process_supervision_loss_with_token_ids FAILED [ 28%]
tests/integration/test_process_step_integration.py::test_training_step_with_process_step_targets FAILED [ 28%]
tests/integration/test_resume_checkpoint.py::TestCheckpointManager::test_init PASSED [ 28%]
tests/integration/test_resume_checkpoint.py::TestCheckpointManager::test_save_checkpoint PASSED [ 28%]
tests/integration/test_resume_checkpoint.py::TestCheckpointManager::test_load_checkpoint PASSED [ 28%]
tests/integration/test_resume_checkpoint.py::TestCheckpointManager::test_load_checkpoint_nonexistent PASSED [ 28%]
tests/integration/test_resume_checkpoint.py::TestCheckpointManager::test_clear_checkpoint PASSED [ 28%]
tests/integration/test_resume_checkpoint.py::TestCheckpointManager::test_checkpoint_preserves_results PASSED [ 28%]
tests/integration/test_resume_checkpoint.py::TestBudgetTrackerCheckpoint::test_budget_state_preserved PASSED [ 28%]
tests/integration/test_resume_checkpoint.py::TestResumeWorkflow::test_resume_skips_completed_indices PASSED [ 29%]
tests/integration/test_resume_checkpoint.py::TestResumeWorkflow::test_resume_preserves_budget PASSED [ 29%]
tests/integration/test_resume_checkpoint.py::TestCacheIntegration::test_cache_validation PASSED [ 29%]
tests/integration/test_resume_checkpoint.py::TestCacheIntegration::test_cache_corruption_handling PASSED [ 29%]
tests/integration/test_resume_checkpoint.py::TestCheckpointInterval::test_checkpoint_interval_logic PASSED [ 29%]
tests/integration/test_resume_checkpoint.py::TestCheckpointInterval::test_final_checkpoint_always_saved PASSED [ 29%]
tests/integration/test_speed_optimization_integration.py::TestEnumeratedShapeTrainingIntegration::test_shape_sampling_in_training_loop PASSED [ 29%]
tests/integration/test_speed_optimization_integration.py::TestEnumeratedShapeTrainingIntegration::test_multiple_shapes_in_sequence PASSED [ 29%]
tests/integration/test_speed_optimization_integration.py::TestLatencyAwareLossesIntegration::test_length_aware_loss_in_training_step FAILED [ 29%]
tests/integration/test_speed_optimization_integration.py::TestLatencyAwareLossesIntegration::test_early_tool_loss_in_training_step FAILED [ 29%]
tests/integration/test_speed_optimization_integration.py::TestLatencyAwareLossesIntegration::test_latency_losses_with_combined_kd FAILED [ 29%]
tests/integration/test_speed_optimization_integration.py::TestQATIntegration::test_qat_enablement_timing PASSED [ 29%]
tests/integration/test_speed_optimization_integration.py::TestQATIntegration::test_qat_disabled_by_default PASSED [ 29%]
tests/integration/test_speed_optimization_integration.py::TestSpeedMetricsIntegration::test_speed_metrics_with_model PASSED [ 30%]
tests/integration/test_speed_optimization_integration.py::TestSpeedMetricsIntegration::test_speed_metrics_aggregation FAILED [ 30%]
tests/integration/test_speed_optimization_integration.py::TestTrainingStepWithSpeedOptimizations::test_training_step_with_length_loss FAILED [ 30%]
tests/integration/test_speed_optimization_integration.py::TestTrainingStepWithSpeedOptimizations::test_training_step_with_early_tool_loss FAILED [ 30%]
tests/integration/test_training_pipeline.py::TestDatasetModelIntegration::test_dataset_model_forward PASSED [ 30%]
tests/integration/test_training_pipeline.py::TestDatasetModelIntegration::test_dataset_dataloader_model PASSED [ 30%]
tests/integration/test_training_pipeline.py::TestKDTrainingIntegration::test_kd_loss_computation PASSED [ 30%]
tests/integration/test_training_pipeline.py::TestKDTrainingIntegration::test_kd_training_step PASSED [ 30%]
tests/integration/test_training_pipeline.py::TestKDTrainingIntegration::test_checkpoint_save_load PASSED [ 30%]
tests/integration/test_training_pipeline.py::TestProcessSupervisionIntegration::test_process_supervision_loss_with_model PASSED [ 30%]
tests/models/test_halt_head.py::TestHaltHead::test_halt_head_initialized PASSED [ 30%]
tests/models/test_halt_head.py::TestHaltHead::test_halt_head_not_initialized PASSED [ 30%]
tests/models/test_halt_head.py::TestHaltHead::test_forward_returns_halt_logits PASSED [ 30%]
tests/models/test_halt_head.py::TestHaltHead::test_forward_hidden_processes_without_lm_head PASSED [ 31%]
tests/models/test_halt_head.py::TestHaltHead::test_halt_head_output_shape PASSED [ 31%]
tests/models/test_halt_head.py::TestHaltHead::test_halt_logits_are_differentiable PASSED [ 31%]
tests/models/test_teacher_client.py::TestAPITier::test_tier_enum_values PASSED [ 31%]
tests/models/test_teacher_client.py::TestAPITier::test_tier_limits_structure FAILED [ 31%]
tests/models/test_teacher_client.py::TestTeacherClientInitialization::test_init_http_backend PASSED [ 31%]
tests/models/test_teacher_client.py::TestTeacherClientInitialization::test_init_http_endpoint_normalization PASSED [ 31%]
tests/models/test_teacher_client.py::TestTeacherClientInitialization::test_init_api_key_from_env FAILED [ 31%]
tests/models/test_teacher_client.py::TestTeacherClientInitialization::test_init_hf_backend FAILED [ 31%]
tests/models/test_teacher_client.py::TestTeacherClientInitialization::test_init_invalid_backend PASSED [ 31%]
tests/models/test_teacher_client.py::TestTeacherClientInitialization::test_init_hf_unavailable PASSED [ 31%]
tests/models/test_teacher_client.py::TestTeacherClientFactoryMethods::test_from_endpoint_basic PASSED [ 31%]
tests/models/test_teacher_client.py::TestTeacherClientFactoryMethods::test_from_endpoint_with_options PASSED [ 31%]
tests/models/test_teacher_client.py::TestTeacherClientFactoryMethods::test_from_hf_basic PASSED [ 31%]
tests/models/test_teacher_client.py::TestTierDetection::test_get_tier PASSED [ 32%]
tests/models/test_teacher_client.py::TestTierDetection::test_get_tier_limits PASSED [ 32%]
tests/models/test_teacher_client.py::TestTierDetection::test_update_tier_from_response_headers FAILED [ 32%]
tests/models/test_teacher_client.py::TestTierDetection::test_update_tier_unknown_header FAILED [ 32%]
tests/models/test_teacher_client.py::TestRetrySession::test_setup_retry_session FAILED [ 32%]
tests/models/test_teacher_client.py::TestRetrySession::test_get_retry_after_header PASSED [ 32%]
tests/models/test_teacher_client.py::TestRetrySession::test_get_retry_after_no_header PASSED [ 32%]
tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_single_prompt_success FAILED [ 32%]
tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_with_logits FAILED [ 32%]
tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_retry_on_failure FAILED [ 32%]
tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_rate_limit_handling FAILED [ 32%]
tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_max_retries_exceeded FAILED [ 32%]
tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_multi_prompt_batch FAILED [ 32%]
tests/models/test_teacher_client.py::TestHuggingFaceBackend::test_hf_sample_basic ERROR [ 33%]
tests/models/test_teacher_client.py::TestCircuitBreaker::test_try_fallback_api_success FAILED [ 33%]
tests/models/test_teacher_client.py::TestHealthCheck::test_health_check_success PASSED [ 33%]
tests/models/test_teacher_client.py::TestHealthCheck::test_health_check_failure FAILED [ 33%]
tests/models/test_teacher_client.py::TestMultiStepSampling::test_sample_multi_step_basic FAILED [ 33%]
tests/models/test_teacher_client.py::TestErrorHandling::test_sample_empty_prompts FAILED [ 33%]
tests/models/test_teacher_client.py::TestErrorHandling::test_sample_malformed_response FAILED [ 33%]
tests/models/test_teacher_client.py::TestErrorHandling::test_sample_timeout FAILED [ 33%]
tests/property/test_span_invariants.py::test_span_round_trip_ascii SKIPPED [ 33%]
tests/property/test_span_invariants.py::test_normalization_stability PASSED [ 33%]
tests/property/test_span_invariants.py::test_normalization_crlf_variants PASSED [ 33%]
tests/property/test_span_invariants.py::test_normalization_nfd_nfc_variants PASSED [ 33%]
tests/property/test_span_invariants.py::test_span_round_trip_accented PASSED [ 33%]
tests/property/test_span_invariants.py::test_span_round_trip_emoji PASSED [ 34%]
tests/runtime/test_caws_budget_enforcement.py::TestCAWSBudgetEnforcement::test_tier_1_limits FAILED [ 34%]
tests/runtime/test_caws_budget_enforcement.py::TestCAWSBudgetEnforcement::test_tier_2_limits FAILED [ 34%]
tests/runtime/test_caws_budget_enforcement.py::TestCAWSBudgetEnforcement::test_tier_3_limits FAILED [ 34%]
tests/runtime/test_caws_budget_enforcement.py::TestCAWSBudgetEnforcement::test_budget_breach_forces_halt FAILED [ 34%]
tests/runtime/test_caws_budget_enforcement.py::TestCAWSBudgetEnforcement::test_latent_spans_respect_tier PASSED [ 34%]
tests/runtime/test_latent_mode.py::TestLatentModeEngine::test_bot_token_switches_to_latent_mode PASSED [ 34%]
tests/runtime/test_latent_mode.py::TestLatentModeEngine::test_eot_token_switches_to_language_mode PASSED [ 34%]
tests/runtime/test_latent_mode.py::TestLatentModeEngine::test_latent_mode_processes_hidden_state PASSED [ 34%]
tests/runtime/test_latent_mode.py::TestLatentModeEngine::test_max_latent_length_safety_check PASSED [ 34%]
tests/runtime/test_latent_mode.py::TestLatentModeEngine::test_unmatched_bot_token_error PASSED [ 34%]
tests/runtime/test_latent_mode.py::TestLatentModeEngine::test_unmatched_eot_token_error PASSED [ 34%]
tests/runtime/test_latent_mode.py::TestLatentModeEngine::test_max_latent_spans_limit FAILED [ 34%]
tests/runtime/test_latent_mode.py::TestLatentModeEngine::test_latent_mode_disabled PASSED [ 34%]
tests/runtime/test_latent_smoke.py::TestLatentSmoke::test_smoke_sentinel_parsing PASSED [ 35%]
tests/runtime/test_latent_smoke.py::TestLatentSmoke::test_smoke_multiple_spans PASSED [ 35%]
tests/runtime/test_refinement_controller.py::TestRefinementController::test_should_halt_on_max_loops PASSED [ 35%]
tests/runtime/test_refinement_controller.py::TestRefinementController::test_should_halt_on_judge_score_and_delta_shrinking PASSED [ 35%]
tests/runtime/test_refinement_controller.py::TestRefinementController::test_should_halt_on_halt_head_threshold PASSED [ 35%]
tests/runtime/test_refinement_controller.py::TestRefinementController::test_should_continue_when_conditions_not_met PASSED [ 35%]
tests/runtime/test_refinement_controller.py::TestRefinementController::test_tier_limits_enforced PASSED [ 35%]
tests/runtime/test_refinement_controller.py::TestRefinementController::test_reset_clears_state PASSED [ 35%]
tests/runtime/test_safety_edge_cases.py::TestSafetyEdgeCases::test_nested_bot_without_eot PASSED [ 35%]
tests/runtime/test_safety_edge_cases.py::TestSafetyEdgeCases::test_unmatched_eot PASSED [ 35%]
tests/runtime/test_safety_edge_cases.py::TestSafetyEdgeCases::test_excess_latent_spans PASSED [ 35%]
tests/runtime/test_safety_edge_cases.py::TestSafetyEdgeCases::test_max_latent_length_enforcement PASSED [ 35%]
tests/runtime/test_safety_edge_cases.py::TestSafetyEdgeCases::test_missing_hidden_state_fallback PASSED [ 35%]
tests/runtime/test_safety_edge_cases.py::TestSafetyEdgeCases::test_forward_hidden_error_fallback PASSED [ 36%]
tests/runtime/test_safety_edge_cases.py::TestSafetyEdgeCases::test_generation_ends_in_latent_mode FAILED [ 36%]
tests/test_ane_optimizations.py::test_int64_detection_on_attention_paths PASSED [ 36%]
tests/test_ane_optimizations.py::test_embedding_layer_int32_verification PASSED [ 36%]
tests/test_ane_optimizations.py::test_ane_op_compatibility_verification PASSED [ 36%]
tests/test_ane_optimizations.py::test_enumerated_shapes_static_allocation PASSED [ 36%]
tests/test_ane_optimizations.py::test_ane_residency_threshold PASSED     [ 36%]
tests/test_caws_compliance_gates.py::test_privacy_gate_pii_detection PASSED [ 36%]
tests/test_caws_compliance_gates.py::test_control_integration_gate PASSED [ 36%]
tests/test_caws_compliance_gates.py::test_json_validity_gate PASSED      [ 36%]
tests/test_caws_compliance_gates.py::test_fixture_hit_rate_gate PASSED   [ 36%]
tests/test_caws_compliance_gates.py::test_broker_fixtures_hit_rate SKIPPED [ 36%]
tests/test_caws_compliance_gates.py::test_integration_f1_gate PASSED     [ 36%]
tests/test_caws_compliance_gates.py::test_gate_thresholds_documented PASSED [ 37%]
tests/test_claims_pipeline_toy.py::test_stage_1_disambiguation PASSED    [ 37%]
tests/test_claims_pipeline_toy.py::test_stage_2_qualification PASSED     [ 37%]
tests/test_claims_pipeline_toy.py::test_stage_3_decomposition PASSED     [ 37%]
tests/test_claims_pipeline_toy.py::test_stage_4_verification PASSED      [ 37%]
tests/test_claims_pipeline_toy.py::test_full_pipeline_end_to_end PASSED  [ 37%]
tests/test_claims_pipeline_toy.py::test_pipeline_with_policy_gating PASSED [ 37%]
tests/test_claims_pipeline_toy.py::test_pipeline_determinism FAILED      [ 37%]
tests/test_claims_pipeline_toy.py::test_pipeline_error_handling PASSED   [ 37%]
tests/test_claims_pipeline_toy.py::test_pipeline_coverage_requirements PASSED [ 37%]
tests/test_claims_pipeline_toy.py::test_pipeline_outcome_distribution FAILED [ 37%]
tests/test_claims_pipeline_toy.py::test_pipeline_fingerprints PASSED     [ 37%]
tests/test_claims_policy.py::test_status_claim_requires_artifacts_short_circuits PASSED [ 37%]
tests/test_claims_policy.py::test_numeric_claim_requires_json_field PASSED [ 37%]
tests/test_claims_policy.py::test_superlative_is_always_blocked PASSED   [ 38%]
tests/test_claims_policy.py::test_claim_with_required_artifacts_passes_policy_gate PASSED [ 38%]
tests/test_claims_policy.py::test_benchmark_claim_requires_bench_json PASSED [ 38%]
tests/test_claims_toy_model_integration.py::test_toy_model_claims_extraction PASSED [ 38%]
tests/test_claims_toy_model_integration.py::test_toy_model_eval_harness_pattern PASSED [ 38%]
tests/test_claims_toy_model_integration.py::test_toy_model_claims_with_policy PASSED [ 38%]
tests/test_claims_toy_model_integration.py::test_toy_model_determinism PASSED [ 38%]
tests/test_claims_toy_model_integration.py::test_toy_model_coverage_scenarios PASSED [ 38%]
tests/test_coreml_golden_vectors.py::test_coreml_golden_vectors SKIPPED  [ 38%]
tests/test_coreml_golden_vectors.py::test_coreml_io_contract SKIPPED     [ 38%]
tests/test_entailment_calibration.py::test_calibration_script_reduces_mean_nll PASSED [ 38%]
tests/test_export_contracts.py::test_export_prefill_model_no_unsupported_ops PASSED [ 38%]
tests/test_export_contracts.py::test_export_model_int32_input_ids PASSED [ 38%]
tests/test_export_contracts.py::test_attention_mask_dtype_consistency PASSED [ 39%]
tests/test_export_contracts.py::test_softmax_inputs_not_int PASSED       [ 39%]
tests/test_performance_regression.py::test_performance_regression_gates SKIPPED [ 39%]
tests/test_performance_regression.py::test_memory_budget_compliance SKIPPED [ 39%]
tests/test_placeholder_detection.py::test_placeholder_marker_detection PASSED [ 39%]
tests/test_placeholder_detection.py::test_production_model_not_placeholder PASSED [ 39%]
tests/test_placeholder_detection.py::test_placeholder_detection_integration PASSED [ 39%]
tests/test_precision_islands.py::test_precision_islands_config_loading PASSED [ 39%]
tests/test_precision_islands.py::test_precision_islands_verification SKIPPED [ 39%]
tests/test_prefill_decode_consistency.py::test_kv_cache_index_advancement FAILED [ 39%]
tests/test_prefill_decode_consistency.py::test_prefill_decode_consistency_pytorch PASSED [ 39%]
tests/test_prefill_decode_consistency.py::test_prefill_decode_consistency_coreml SKIPPED [ 39%]
tests/test_prefill_decode_consistency.py::test_multi_batch_kv_cache_handling PASSED [ 39%]
tests/test_quantization_ab.py::test_quantization_ab_golden_vectors SKIPPED [ 40%]
tests/test_quantization_ab.py::test_quantization_behavioral_correctness SKIPPED [ 40%]
tests/test_quantization_export.py::test_int8_weights_verification SKIPPED [ 40%]
tests/test_quantization_export.py::test_qat_module_folding SKIPPED (...) [ 40%]
tests/test_quantization_export.py::test_quantization_recipe_validation SKIPPED [ 40%]
tests/test_tokenizer_contract.py::test_special_token_ids_match_constants FAILED [ 40%]
tests/test_tokenizer_contract.py::test_special_tokens_are_single_tokens FAILED [ 40%]
tests/test_tokenizer_contract.py::test_round_trip_stability FAILED       [ 40%]
tests/test_tokenizer_contract.py::test_masking_safety FAILED             [ 40%]
tests/test_tokenizer_contract.py::test_loss_masking_never_hides_supervised_tokens PASSED [ 40%]
tests/test_training_reproducibility.py::test_dataset_fingerprint_validation PASSED [ 40%]
tests/test_training_reproducibility.py::test_deterministic_model_initialization PASSED [ 40%]
tests/test_training_reproducibility.py::test_checkpoint_reproducibility PASSED [ 40%]
tests/test_training_reproducibility.py::test_seed_management PASSED      [ 41%]
tests/test_training_reproducibility.py::test_dataset_sharding_determinism PASSED [ 41%]
tests/test_verification_outcomes.py::test_outcome_1_identical_claim_and_cmax_verified_when_covered PASSED [ 41%]
tests/test_verification_outcomes.py::test_outcome_2_full_support_including_c_entails_cmax PASSED [ 41%]
tests/test_verification_outcomes.py::test_outcome_3_right_answer_wrong_rationale_insufficient PASSED [ 41%]
tests/test_verification_outcomes.py::test_outcome_4_retrieval_mismatch_insufficient PASSED [ 41%]
tests/test_verification_outcomes.py::test_outcome_5_contradiction_has_precedence_over_support PASSED [ 41%]
tests/test_verification_outcomes.py::test_outcome_6_insufficient_evidence_when_all_below_thresholds PASSED [ 41%]
tests/test_verification_outcomes.py::test_binding_aware_coverage_requires_spo_same_sentence PASSED [ 41%]
tests/test_verification_outcomes.py::test_negation_mismatch_penalizes_coverage_and_blocks_verified PASSED [ 41%]
tests/test_verification_outcomes.py::test_operator_thresholds_and_precedence_included_in_fingerprint PASSED [ 41%]
tests/tokenizer/test_sentinel_tokens.py::TestSentinelTokens::test_token_ids_defined PASSED [ 41%]
tests/tokenizer/test_sentinel_tokens.py::TestSentinelTokens::test_token_strings_defined PASSED [ 41%]
tests/tokenizer/test_sentinel_tokens.py::TestSentinelTokens::test_tokens_in_special_tokens_map PASSED [ 41%]
tests/tokenizer/test_sentinel_tokens.py::TestSentinelTokens::test_tokens_in_tokenizer_config PASSED [ 42%]
tests/training/test_distill_kd.py::TestConfigOperations::test_load_config_valid_yaml PASSED [ 42%]
tests/training/test_distill_kd.py::TestConfigOperations::test_load_config_nonexistent_file PASSED [ 42%]
tests/training/test_distill_kd.py::TestConfigOperations::test_merge_configs_basic PASSED [ 42%]
tests/training/test_distill_kd.py::TestConfigOperations::test_merge_configs_overrides PASSED [ 42%]
tests/training/test_distill_kd.py::TestConfigOperations::test_merge_configs_env_overrides PASSED [ 42%]
tests/training/test_distill_kd.py::TestModelCreation::test_create_model_basic PASSED [ 42%]
tests/training/test_distill_kd.py::TestModelCreation::test_create_model_with_quantization FAILED [ 42%]
tests/training/test_distill_kd.py::TestModelCreation::test_create_model_invalid_config FAILED [ 42%]
tests/training/test_distill_kd.py::TestOptimizerCreation::test_create_optimizer_adamw FAILED [ 42%]
tests/training/test_distill_kd.py::TestOptimizerCreation::test_create_optimizer_default_config FAILED [ 42%]
tests/training/test_distill_kd.py::TestOptimizerCreation::test_create_optimizer_invalid_type FAILED [ 42%]
tests/training/test_distill_kd.py::TestSequenceLength::test_get_sequence_length_basic PASSED [ 42%]
tests/training/test_distill_kd.py::TestSequenceLength::test_get_sequence_length_with_curriculum PASSED [ 43%]
tests/training/test_distill_kd.py::TestSequenceLength::test_sample_enumerated_shape PASSED [ 43%]
tests/training/test_distill_kd.py::TestQATOperations::test_should_enable_qat_false PASSED [ 43%]
tests/training/test_distill_kd.py::TestQATOperations::test_should_enable_qat_true PASSED [ 43%]
tests/training/test_distill_kd.py::TestQATOperations::test_should_enable_qat_no_config PASSED [ 43%]
tests/training/test_distill_kd.py::TestQATOperations::test_apply_qat_to_model FAILED [ 43%]
tests/training/test_distill_kd.py::TestQATOperations::test_check_qat_stability_valid FAILED [ 43%]
tests/training/test_distill_kd.py::TestQATOperations::test_check_qat_stability_nan_weights FAILED [ 43%]
tests/training/test_distill_kd.py::TestTrainingStep::test_train_step_basic PASSED [ 43%]
tests/training/test_distill_kd.py::TestTrainingStep::test_train_step_vocab_clamping FAILED [ 43%]
tests/training/test_distill_kd.py::TestTrainingStep::test_train_step_cot_free_validation PASSED [ 43%]
tests/training/test_distill_kd.py::TestTrainingStep::test_train_step_with_intermediate_layers PASSED [ 43%]
tests/training/test_distill_kd.py::TestTrainingStep::test_train_step_with_self_evaluation FAILED [ 43%]
tests/training/test_distill_kd.py::TestTrainingStep::test_train_step_with_halt_head PASSED [ 44%]
tests/training/test_distill_kd.py::TestBatchOperations::test_compute_required_fields_present_basic PASSED [ 44%]
tests/training/test_distill_kd.py::TestBatchOperations::test_truncate_batch_to_shape PASSED [ 44%]
tests/training/test_distill_kd.py::TestCheckpointOperations::test_save_checkpoint_basic FAILED [ 44%]
tests/training/test_distill_kd.py::TestCheckpointOperations::test_save_checkpoint_with_metadata FAILED [ 44%]
tests/training/test_distill_kd.py::TestConfigValidation::test_validate_config_valid PASSED [ 44%]
tests/training/test_distill_kd.py::TestConfigValidation::test_validate_config_missing_model PASSED [ 44%]
tests/training/test_distill_kd.py::TestConfigValidation::test_validate_config_missing_training PASSED [ 44%]
tests/training/test_distill_kd.py::TestConfigValidation::test_validate_config_invalid_lr PASSED [ 44%]
tests/training/test_distill_kd.py::TestConfigValidation::test_validate_config_invalid_vocab_size PASSED [ 44%]
tests/training/test_distill_process.py::TestConfigOperations::test_load_config_success PASSED [ 44%]
tests/training/test_distill_process.py::TestConfigOperations::test_load_config_file_not_found PASSED [ 44%]
tests/training/test_distill_process.py::TestConfigOperations::test_load_config_invalid_yaml PASSED [ 44%]
tests/training/test_distill_process.py::TestConfigOperations::test_merge_configs_empty_list PASSED [ 44%]
tests/training/test_distill_process.py::TestConfigOperations::test_merge_configs_single_config PASSED [ 45%]
tests/training/test_distill_process.py::TestConfigOperations::test_merge_configs_multiple_configs PASSED [ 45%]
tests/training/test_distill_process.py::TestConfigOperations::test_merge_configs_nested_merge FAILED [ 45%]
tests/training/test_distill_process.py::TestModelLoading::test_load_model_success FAILED [ 45%]
tests/training/test_distill_process.py::TestModelLoading::test_load_model_without_config FAILED [ 45%]
tests/training/test_distill_process.py::TestModelLoading::test_load_model_checkpoint_not_found PASSED [ 45%]
tests/training/test_distill_process.py::TestTextGeneration::test_generate_text_from_logits_basic PASSED [ 45%]
tests/training/test_distill_process.py::TestTextGeneration::test_generate_text_from_logits_temperature FAILED [ 45%]
tests/training/test_distill_process.py::TestTextGeneration::test_generate_text_from_logits_top_k FAILED [ 45%]
tests/training/test_distill_process.py::TestTextGeneration::test_generate_text_from_logits_top_p FAILED [ 45%]
tests/training/test_distill_process.py::TestTextGeneration::test_generate_text_from_logits_greedy FAILED [ 45%]
tests/training/test_distill_process.py::TestTextGeneration::test_generate_text_from_logits_empty_logits PASSED [ 45%]
tests/training/test_distill_process.py::TestTextGeneration::test_generate_text_from_logits_single_token PASSED [ 45%]
tests/training/test_distill_process.py::TestTrainingStep::test_train_step_process_basic FAILED [ 46%]
tests/training/test_distill_process.py::TestTrainingStep::test_train_step_process_zero_weights FAILED [ 46%]
tests/training/test_distill_process.py::TestTrainingStep::test_train_step_process_no_grad FAILED [ 46%]
tests/training/test_distill_process.py::TestTrainingStep::test_train_step_process_missing_process_labels FAILED [ 46%]
tests/training/test_distill_process.py::TestMainFunction::test_main_success FAILED [ 46%]
tests/training/test_distill_process.py::TestMainFunction::test_main_config_load_failure FAILED [ 46%]
tests/training/test_distill_process.py::TestMainFunction::test_main_model_load_failure FAILED [ 46%]
tests/training/test_distill_process.py::TestProcessSupervisionIntegration::test_process_supervision_config_structure PASSED [ 46%]
tests/training/test_distill_process.py::TestProcessSupervisionIntegration::test_process_labels_structure PASSED [ 46%]
tests/training/test_distill_process.py::TestProcessSupervisionIntegration::test_training_config_validation PASSED [ 46%]
tests/training/test_export_student.py::TestTorchScriptExport::test_export_torchscript_success FAILED [ 46%]
tests/training/test_export_student.py::TestTorchScriptExport::test_export_torchscript_directory_creation PASSED [ 46%]
tests/training/test_export_student.py::TestTorchScriptExport::test_export_torchscript_trace_failure PASSED [ 46%]
tests/training/test_export_student.py::TestExportedProgramExport::test_export_exported_program_success PASSED [ 47%]
tests/training/test_export_student.py::TestExportedProgramExport::test_export_exported_program_directory_creation PASSED [ 47%]
tests/training/test_export_student.py::TestExportedProgramExport::test_export_exported_program_no_torch_export PASSED [ 47%]
tests/training/test_export_student.py::TestContractCreation::test_create_contract_success FAILED [ 47%]
tests/training/test_export_student.py::TestContractCreation::test_create_contract_directory_creation FAILED [ 47%]
tests/training/test_export_student.py::TestContractCreation::test_create_contract_file_writing FAILED [ 47%]
tests/training/test_export_student.py::TestMainFunction::test_main_torchscript_export FAILED [ 47%]
tests/training/test_export_student.py::TestMainFunction::test_main_exported_program_export FAILED [ 47%]
tests/training/test_export_student.py::TestMainFunction::test_main_checkpoint_not_found PASSED [ 47%]
tests/training/test_export_student.py::TestMainFunction::test_main_invalid_export_type FAILED [ 47%]
tests/training/test_export_student.py::TestExportIntegration::test_export_workflow FAILED [ 47%]
tests/training/test_export_student.py::TestExportIntegration::test_export_error_handling PASSED [ 47%]
tests/training/test_export_student.py::TestConfigurationHandling::test_contract_creation_with_different_configs FAILED [ 47%]
tests/training/test_export_student.py::TestConfigurationHandling::test_export_with_different_input_shapes FAILED [ 48%]
tests/training/test_feature_flags.py::TestFeatureFlagEnum::test_feature_flag_values PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureFlagEnum::test_feature_flag_count PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureFlagEnum::test_feature_flag_uniqueness PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureConfig::test_feature_config_creation PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureConfig::test_feature_config_empty_sets PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_initialization PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_get_enabled_features PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_is_enabled PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_enable_feature PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_disable_feature PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_enable_with_dependencies FAILED [ 48%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_enable_with_conflicts PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_validate_configuration PASSED [ 48%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_get_feature_config FAILED [ 49%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_list_features FAILED [ 49%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_reset_to_defaults FAILED [ 49%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_load_from_environment PASSED [ 49%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_save_to_environment FAILED [ 49%]
tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_get_feature_stats FAILED [ 49%]
tests/training/test_feature_flags.py::TestInitializeFeatures::test_initialize_features_no_config PASSED [ 49%]
tests/training/test_feature_flags.py::TestInitializeFeatures::test_initialize_features_with_config PASSED [ 49%]
tests/training/test_feature_flags.py::TestInitializeFeatures::test_initialize_features_invalid_config PASSED [ 49%]
tests/training/test_feature_flags.py::TestFeatureInteractions::test_circular_dependencies_prevention PASSED [ 49%]
tests/training/test_feature_flags.py::TestFeatureInteractions::test_feature_enable_disable_cycle PASSED [ 49%]
tests/training/test_feature_flags.py::TestFeatureInteractions::test_multiple_features_enable_disable PASSED [ 49%]
tests/training/test_feature_flags.py::TestFeatureInteractions::test_feature_manager_thread_safety FAILED [ 49%]
tests/training/test_feature_flags.py::TestEnvironmentIntegration::test_environment_variable_override PASSED [ 50%]
tests/training/test_feature_flags.py::TestEnvironmentIntegration::test_environment_variable_case_insensitive PASSED [ 50%]
tests/training/test_feature_flags.py::TestEnvironmentIntegration::test_environment_variable_invalid_values PASSED [ 50%]
tests/training/test_feature_flags.py::TestEnvironmentIntegration::test_environment_variable_precedence PASSED [ 50%]
tests/training/test_latent_curriculum.py::TestLatentCurriculum::test_curriculum_applies_latent_slots FAILED [ 50%]
tests/training/test_latent_curriculum.py::TestLatentCurriculum::test_curriculum_creates_loss_mask FAILED [ 50%]
tests/training/test_latent_curriculum.py::TestLatentCurriculum::test_curriculum_respects_probability PASSED [ 50%]
tests/training/test_latent_curriculum.py::TestLatentCurriculum::test_curriculum_handles_missing_cot_steps PASSED [ 50%]
tests/training/test_latent_curriculum.py::TestLatentCurriculum::test_loss_mask_masks_latent_spans FAILED [ 50%]
tests/training/test_logging_utils.py::TestStructuredLogger::test_structured_logger_initialization PASSED [ 50%]
tests/training/test_logging_utils.py::TestStructuredLogger::test_structured_logger_no_duplicate_handlers PASSED [ 50%]
tests/training/test_logging_utils.py::TestStructuredLogger::test_structured_logger_log_methods PASSED [ 50%]
tests/training/test_logging_utils.py::TestStructuredLogger::test_structured_logger_propagate_disabled PASSED [ 50%]
tests/training/test_logging_utils.py::TestStructuredLogger::test_structured_logger_formatter_assignment PASSED [ 51%]
tests/training/test_logging_utils.py::TestStructuredFormatter::test_structured_formatter_format_basic FAILED [ 51%]
tests/training/test_logging_utils.py::TestStructuredFormatter::test_structured_formatter_format_with_extra PASSED [ 51%]
tests/training/test_logging_utils.py::TestStructuredFormatter::test_structured_formatter_format_all_levels PASSED [ 51%]
tests/training/test_logging_utils.py::TestStructuredFormatter::test_structured_formatter_format_with_exception FAILED [ 51%]
tests/training/test_logging_utils.py::TestStructuredFormatter::test_structured_formatter_format_message_formatting PASSED [ 51%]
tests/training/test_logging_utils.py::TestStructuredFormatter::test_structured_formatter_timestamp_format PASSED [ 51%]
tests/training/test_logging_utils.py::TestStructuredFormatter::test_structured_formatter_json_validity FAILED [ 51%]
tests/training/test_logging_utils.py::TestSetupTrainingLogging::test_setup_training_logging_basic FAILED [ 51%]
tests/training/test_logging_utils.py::TestSetupTrainingLogging::test_setup_training_logging_with_file FAILED [ 51%]
tests/training/test_logging_utils.py::TestSetupTrainingLogging::test_setup_training_logging_different_levels FAILED [ 51%]
tests/training/test_logging_utils.py::TestLogTrainingStep::test_log_training_step_basic FAILED [ 51%]
tests/training/test_logging_utils.py::TestLogTrainingStep::test_log_training_step_with_additional_metrics FAILED [ 51%]
tests/training/test_logging_utils.py::TestLogTrainingStep::test_log_training_step_zero_values FAILED [ 51%]
tests/training/test_logging_utils.py::TestLogValidationMetrics::test_log_validation_metrics_basic FAILED [ 52%]
tests/training/test_logging_utils.py::TestLogValidationMetrics::test_log_validation_metrics_empty FAILED [ 52%]
tests/training/test_logging_utils.py::TestLogValidationMetrics::test_log_validation_metrics_complex FAILED [ 52%]
tests/training/test_logging_utils.py::TestLogCheckpointSaved::test_log_checkpoint_saved_basic FAILED [ 52%]
tests/training/test_logging_utils.py::TestLogCheckpointSaved::test_log_checkpoint_saved_different_paths FAILED [ 52%]
tests/training/test_logging_utils.py::TestLogError::test_log_error_basic FAILED [ 52%]
tests/training/test_logging_utils.py::TestLogError::test_log_error_no_context FAILED [ 52%]
tests/training/test_logging_utils.py::TestLogError::test_log_error_different_exceptions FAILED [ 52%]
tests/training/test_logging_utils.py::TestLoggingIntegration::test_structured_logging_workflow FAILED [ 52%]
tests/training/test_logging_utils.py::TestLoggingIntegration::test_logging_error_handling PASSED [ 52%]
tests/training/test_logging_utils.py::TestLoggingIntegration::test_logging_with_special_characters PASSED [ 52%]
tests/training/test_logging_utils.py::TestLoggingIntegration::test_logging_performance PASSED [ 52%]
tests/training/test_loss_mask_correctness.py::TestLossMaskCorrectness::test_loss_mask_masks_latent_tokens PASSED [ 52%]
tests/training/test_loss_mask_correctness.py::TestLossMaskCorrectness::test_loss_mask_alignment_after_padding PASSED [ 53%]
tests/training/test_loss_mask_correctness.py::TestLossMaskCorrectness::test_loss_mask_excludes_latent_spans_from_supervision PASSED [ 53%]
tests/training/test_loss_mask_correctness.py::TestLossMaskCorrectness::test_loss_mask_handles_empty_sequences PASSED [ 53%]
tests/training/test_loss_mask_correctness.py::TestLossMaskCorrectness::test_loss_mask_applied_to_combined_loss PASSED [ 53%]
tests/training/test_monitoring.py::TestMetricPoint::test_metric_point_creation PASSED [ 53%]
tests/training/test_monitoring.py::TestMetricPoint::test_metric_point_default_tags PASSED [ 53%]
tests/training/test_monitoring.py::TestHealthStatus::test_health_status_creation PASSED [ 53%]
tests/training/test_monitoring.py::TestHealthStatus::test_health_status_default_details PASSED [ 53%]
tests/training/test_monitoring.py::TestMetricsCollector::test_metrics_collector_initialization FAILED [ 53%]
tests/training/test_monitoring.py::TestMetricsCollector::test_add_metric FAILED [ 53%]
tests/training/test_monitoring.py::TestMetricsCollector::test_add_metric_eviction FAILED [ 53%]
tests/training/test_monitoring.py::TestMetricsCollector::test_get_metrics_by_name FAILED [ 53%]
tests/training/test_monitoring.py::TestMetricsCollector::test_get_metrics_by_tags FAILED [ 53%]
tests/training/test_monitoring.py::TestMetricsCollector::test_get_latest_metric FAILED [ 54%]
tests/training/test_monitoring.py::TestMetricsCollector::test_get_metric_statistics FAILED [ 54%]
tests/training/test_monitoring.py::TestMetricsCollector::test_clear_metrics FAILED [ 54%]
tests/training/test_monitoring.py::TestMetricsCollector::test_thread_safety FAILED [ 54%]
tests/training/test_monitoring.py::TestHealthChecker::test_health_checker_initialization FAILED [ 54%]
tests/training/test_monitoring.py::TestHealthChecker::test_add_check FAILED [ 54%]
tests/training/test_monitoring.py::TestHealthChecker::test_run_check FAILED [ 54%]
tests/training/test_monitoring.py::TestHealthChecker::test_run_check_not_found FAILED [ 54%]
tests/training/test_monitoring.py::TestHealthChecker::test_run_all_checks FAILED [ 54%]
tests/training/test_monitoring.py::TestHealthChecker::test_get_component_status FAILED [ 54%]
tests/training/test_monitoring.py::TestHealthChecker::test_get_overall_health FAILED [ 54%]
tests/training/test_monitoring.py::TestSystemHealthChecks::test_system_health_checks_initialization FAILED [ 54%]
tests/training/test_monitoring.py::TestSystemHealthChecks::test_cpu_usage_check FAILED [ 54%]
tests/training/test_monitoring.py::TestSystemHealthChecks::test_memory_usage_check FAILED [ 55%]
tests/training/test_monitoring.py::TestSystemHealthChecks::test_gpu_check_no_gpu FAILED [ 55%]
tests/training/test_monitoring.py::TestSystemHealthChecks::test_gpu_check_with_gpu FAILED [ 55%]
tests/training/test_monitoring.py::TestSystemHealthChecks::test_disk_usage_check FAILED [ 55%]
tests/training/test_monitoring.py::TestSystemHealthChecks::test_network_connectivity_check FAILED [ 55%]
tests/training/test_monitoring.py::TestSystemHealthChecks::test_run_system_checks FAILED [ 55%]
tests/training/test_monitoring.py::TestTrainingMonitor::test_training_monitor_initialization FAILED [ 55%]
tests/training/test_monitoring.py::TestTrainingMonitor::test_start_monitoring FAILED [ 55%]
tests/training/test_monitoring.py::TestTrainingMonitor::test_stop_monitoring FAILED [ 55%]
tests/training/test_monitoring.py::TestTrainingMonitor::test_log_metric FAILED [ 55%]
tests/training/test_monitoring.py::TestTrainingMonitor::test_log_training_step FAILED [ 55%]
tests/training/test_monitoring.py::TestTrainingMonitor::test_log_validation_metrics FAILED [ 55%]
tests/training/test_monitoring.py::TestTrainingMonitor::test_check_system_health PASSED [ 55%]
tests/training/test_monitoring.py::TestTrainingMonitor::test_get_monitoring_stats FAILED [ 55%]
tests/training/test_monitoring.py::TestTrainingMonitor::test_save_load_metrics FAILED [ 56%]
tests/training/test_monitoring.py::TestTrainingMonitor::test_alert_on_condition FAILED [ 56%]
tests/training/test_monitoring.py::TestInitializeMonitoring::test_initialize_monitoring_no_log_dir PASSED [ 56%]
tests/training/test_monitoring.py::TestInitializeMonitoring::test_initialize_monitoring_with_log_dir PASSED [ 56%]
tests/training/test_monitoring.py::TestConcurrencyAndPerformance::test_concurrent_metric_logging FAILED [ 56%]
tests/training/test_monitoring.py::TestConcurrencyAndPerformance::test_monitoring_performance FAILED [ 56%]
tests/training/test_quality_scoring.py::TestHeuristicQualityScore::test_compute_heuristic_quality_score_empty_text PASSED [ 56%]
tests/training/test_quality_scoring.py::TestHeuristicQualityScore::test_compute_heuristic_quality_score_structured_content FAILED [ 56%]
tests/training/test_quality_scoring.py::TestHeuristicQualityScore::test_compute_heuristic_quality_score_ground_truth_comparison FAILED [ 56%]
tests/training/test_quality_scoring.py::TestHeuristicQualityScore::test_compute_heuristic_quality_score_length_appropriateness PASSED [ 56%]
tests/training/test_quality_scoring.py::TestHeuristicQualityScore::test_compute_heuristic_quality_score_coherence_indicators PASSED [ 56%]
tests/training/test_quality_scoring.py::TestHeuristicQualityScore::test_compute_heuristic_quality_score_prompt_context PASSED [ 56%]
tests/training/test_quality_scoring.py::TestJSONValidityScore::test_compute_json_validity_score_valid_json PASSED [ 56%]
tests/training/test_quality_scoring.py::TestJSONValidityScore::test_compute_json_validity_score_invalid_json PASSED [ 57%]
tests/training/test_quality_scoring.py::TestJSONValidityScore::test_compute_json_validity_score_complex_json PASSED [ 57%]
tests/training/test_quality_scoring.py::TestJSONValidityScore::test_compute_json_validity_score_partial_json PASSED [ 57%]
tests/training/test_quality_scoring.py::TestJSONValidityScore::test_compute_json_validity_score_no_json PASSED [ 57%]
tests/training/test_quality_scoring.py::TestJSONValidityScore::test_compute_json_validity_score_malformed_json FAILED [ 57%]
tests/training/test_quality_scoring.py::TestJSONValidityScore::test_compute_json_validity_score_empty_text PASSED [ 57%]
tests/training/test_quality_scoring.py::TestCodeBlockScore::test_compute_code_block_score_python_code FAILED [ 57%]
tests/training/test_quality_scoring.py::TestCodeBlockScore::test_compute_code_block_score_javascript_code PASSED [ 57%]
tests/training/test_quality_scoring.py::TestCodeBlockScore::test_compute_code_block_score_no_code_blocks PASSED [ 57%]
tests/training/test_quality_scoring.py::TestCodeBlockScore::test_compute_code_block_score_inline_code PASSED [ 57%]
tests/training/test_quality_scoring.py::TestCodeBlockScore::test_compute_code_block_score_mixed_languages FAILED [ 57%]
tests/training/test_quality_scoring.py::TestCodeBlockScore::test_compute_code_block_score_syntax_quality PASSED [ 57%]
tests/training/test_quality_scoring.py::TestCodeBlockScore::test_compute_code_block_score_empty_code_block FAILED [ 57%]
tests/training/test_quality_scoring.py::TestCompositeQualityScore::test_compute_composite_quality_score_all_components FAILED [ 58%]
tests/training/test_quality_scoring.py::TestCompositeQualityScore::test_compute_composite_quality_score_minimal_text FAILED [ 58%]
tests/training/test_quality_scoring.py::TestCompositeQualityScore::test_compute_composite_quality_score_structured_content FAILED [ 58%]
tests/training/test_quality_scoring.py::TestCompositeQualityScore::test_compute_composite_quality_score_weights FAILED [ 58%]
tests/training/test_quality_scoring.py::TestCompositeQualityScore::test_compute_composite_quality_score_empty_text FAILED [ 58%]
tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_empty_list PASSED [ 58%]
tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_single_item FAILED [ 58%]
tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_multiple_items FAILED [ 58%]
tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_with_ground_truth PASSED [ 58%]
tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_mixed_content FAILED [ 58%]
tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_error_handling FAILED [ 58%]
tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_large_batch FAILED [ 58%]
tests/training/test_quality_scoring.py::TestQualityScoringIntegration::test_quality_scoring_consistency PASSED [ 58%]
tests/training/test_quality_scoring.py::TestQualityScoringIntegration::test_quality_scoring_realistic_examples FAILED [ 58%]
tests/training/test_quality_scoring.py::TestQualityScoringIntegration::test_quality_scoring_performance PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_simple_tensors PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_different_tensors_different_hash PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_order_independence PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_cuda_tensors PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_mixed_devices PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_different_dtypes PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_empty_state_dict PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_single_tensor PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_large_tensors PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_detach_handling PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_buffer_operations FAILED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_serialization_failure PASSED [ 59%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_complex_state_dict PASSED [ 60%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_hash_properties PASSED [ 60%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_metadata_included PASSED [ 60%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_dtype_included PASSED [ 60%]
tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_hashlib_called PASSED [ 60%]
tests/training/test_utils.py::TestUtilsIntegration::test_sha256_state_dict_real_model_params PASSED [ 60%]
tests/training/test_utils.py::TestUtilsIntegration::test_sha256_state_dict_memory_efficiency PASSED [ 60%]
tests/training/test_utils.py::TestUtilsIntegration::test_sha256_state_dict_error_recovery PASSED [ 60%]
tests/unit/test_ane_monitor.py::TestANEResidencyMonitor::test_initialization PASSED [ 60%]
tests/unit/test_ane_monitor.py::TestANEResidencyMonitor::test_measure_wall_clock PASSED [ 60%]
tests/unit/test_ane_monitor.py::TestANEResidencyMonitor::test_measure_residency PASSED [ 60%]
tests/unit/test_ane_monitor.py::TestANEResidencyMonitor::test_check_residency_threshold_pass PASSED [ 60%]
tests/unit/test_ane_monitor.py::TestANEResidencyMonitor::test_check_residency_threshold_fail PASSED [ 60%]
tests/unit/test_ane_monitor.py::TestANEResidencyMonitor::test_compare_with_baseline_no_regression PASSED [ 61%]
tests/unit/test_ane_monitor.py::TestANEResidencyMonitor::test_compare_with_baseline_regression PASSED [ 61%]
tests/unit/test_ane_monitor.py::TestANEResidencyMonitor::test_get_model_ops_info_no_model PASSED [ 61%]
tests/unit/test_ane_monitor.py::TestANEResidencyMonitor::test_get_model_ops_info_with_model FAILED [ 61%]
tests/unit/test_ane_monitor.py::TestMeasureModelResidency::test_measure_model_residency PASSED [ 61%]
tests/unit/test_batch_policy.py::TestBatchPolicyConfig::test_default_config PASSED [ 61%]
tests/unit/test_batch_policy.py::TestBatchPolicyConfig::test_custom_config PASSED [ 61%]
tests/unit/test_batch_policy.py::TestBatchPolicy::test_initialization_defaults PASSED [ 61%]
tests/unit/test_batch_policy.py::TestBatchPolicy::test_initialization_with_hardware_profile PASSED [ 61%]
tests/unit/test_batch_policy.py::TestBatchPolicy::test_select_batch_size_interactive PASSED [ 61%]
tests/unit/test_batch_policy.py::TestBatchPolicy::test_select_batch_size_offline PASSED [ 61%]
tests/unit/test_batch_policy.py::TestBatchPolicy::test_select_batch_size_force PASSED [ 61%]
tests/unit/test_batch_policy.py::TestBatchPolicy::test_optimize_from_benchmarks PASSED [ 61%]
tests/unit/test_batch_policy.py::TestBatchPolicy::test_optimize_from_benchmarks_no_improvement PASSED [ 62%]
tests/unit/test_batch_policy.py::TestBatchPolicy::test_should_use_batch_interactive PASSED [ 62%]
tests/unit/test_batch_policy.py::TestBatchPolicy::test_should_use_batch_offline PASSED [ 62%]
tests/unit/test_batch_policy.py::TestBatchPolicy::test_get_policy_summary PASSED [ 62%]
tests/unit/test_batch_policy.py::TestCreateBatchPolicy::test_create_batch_policy PASSED [ 62%]
tests/unit/test_batch_policy.py::TestCreateBatchPolicy::test_create_batch_policy_with_profile PASSED [ 62%]
tests/unit/test_caws_compact.py::TestCAWSCompact::test_compact_format_basic PASSED [ 62%]
tests/unit/test_caws_compact.py::TestCAWSCompact::test_compact_format_dict PASSED [ 62%]
tests/unit/test_caws_compact.py::TestCAWSCompact::test_token_count_limit PASSED [ 62%]
tests/unit/test_caws_compact.py::TestCAWSCompact::test_scope_limiting PASSED [ 62%]
tests/unit/test_caws_compact.py::TestCAWSCompact::test_default_values PASSED [ 62%]
tests/unit/test_caws_compact.py::TestCAWSCompact::test_compact_vs_verbose PASSED [ 62%]
tests/unit/test_caws_structure.py::TestCAWSStructureScore::test_score_with_all_headers PASSED [ 62%]
tests/unit/test_caws_structure.py::TestCAWSStructureScore::test_score_with_code_blocks PASSED [ 62%]
tests/unit/test_caws_structure.py::TestCAWSStructureScore::test_score_empty_text PASSED [ 63%]
tests/unit/test_caws_structure.py::TestCAWSStructureScore::test_score_with_json PASSED [ 63%]
tests/unit/test_caws_structure.py::TestCAWSStructureScore::test_extract_structure_elements PASSED [ 63%]
tests/unit/test_caws_structure.py::TestCAWSStructureScore::test_batch_scoring PASSED [ 63%]
tests/unit/test_caws_structure.py::TestCAWSStructureLoss::test_loss_when_student_lower PASSED [ 63%]
tests/unit/test_caws_structure.py::TestCAWSStructureLoss::test_loss_when_student_higher PASSED [ 63%]
tests/unit/test_caws_structure.py::TestCAWSStructureLoss::test_loss_when_equal PASSED [ 63%]
tests/unit/test_caws_structure.py::TestCAWSStructureLoss::test_loss_requires_grad PASSED [ 63%]
tests/unit/test_code_mode_loss.py::test_code_mode_loss_is_differentiable FAILED [ 63%]
tests/unit/test_code_mode_loss.py::test_code_mode_loss_eligibility_filtering FAILED [ 63%]
tests/unit/test_code_mode_loss.py::test_code_mode_loss_vectorized_eligibility PASSED [ 63%]
tests/unit/test_code_mode_loss_torchscript.py::test_code_mode_loss_torchscript FAILED [ 63%]
tests/unit/test_code_mode_loss_torchscript.py::test_code_mode_loss_forward_method_torchscript PASSED [ 63%]
tests/unit/test_code_mode_weight_scheduler.py::test_code_mode_weight_scheduler PASSED [ 64%]
tests/unit/test_code_mode_weight_scheduler.py::test_code_mode_weight_scheduler_zero_warmup PASSED [ 64%]
tests/unit/test_config_validation.py::TestConfigSchemaValidation::test_training_config_schema_structure PASSED [ 64%]
tests/unit/test_config_validation.py::TestConfigSchemaValidation::test_validate_training_config_valid PASSED [ 64%]
tests/unit/test_config_validation.py::TestConfigSchemaValidation::test_validate_training_config_missing_required FAILED [ 64%]
tests/unit/test_config_validation.py::TestConfigSchemaValidation::test_validate_training_config_invalid_types PASSED [ 64%]
tests/unit/test_config_validation.py::TestConfigSchemaValidation::test_validate_training_config_invalid_enum PASSED [ 64%]
tests/unit/test_config_validation.py::TestConfigSchemaValidation::test_validate_config_file_valid_yaml PASSED [ 64%]
tests/unit/test_config_validation.py::TestConfigSchemaValidation::test_validate_config_file_invalid_yaml PASSED [ 64%]
tests/unit/test_config_validation.py::TestConfigSchemaValidation::test_validate_config_file_nonexistent FAILED [ 64%]
tests/unit/test_config_validation.py::TestDefaultConfigCreation::test_create_default_config_structure PASSED [ 64%]
tests/unit/test_config_validation.py::TestDefaultConfigCreation::test_create_default_config_values PASSED [ 64%]
tests/unit/test_config_validation.py::TestDefaultConfigCreation::test_save_config_template PASSED [ 64%]
tests/unit/test_config_validation.py::TestConfigMerging::test_merge_configs_basic FAILED [ 65%]
tests/unit/test_config_validation.py::TestConfigMerging::test_merge_configs_overrides FAILED [ 65%]
tests/unit/test_config_validation.py::TestConfigMerging::test_merge_configs_empty_list PASSED [ 65%]
tests/unit/test_config_validation.py::TestConfigMerging::test_merge_configs_invalid_file FAILED [ 65%]
tests/unit/test_config_validation.py::TestDeepMerge::test_deep_merge_nested_dicts PASSED [ 65%]
tests/unit/test_config_validation.py::TestDeepMerge::test_deep_merge_non_dict_values PASSED [ 65%]
tests/unit/test_constrained_decode.py::TestJSONFSM::test_start PASSED    [ 65%]
tests/unit/test_constrained_decode.py::TestJSONFSM::test_inside_string_detection PASSED [ 65%]
tests/unit/test_constrained_decode.py::TestJSONFSM::test_step_chars_allowed_outside_string PASSED [ 65%]
tests/unit/test_constrained_decode.py::TestJSONFSM::test_step_chars_allowed_inside_string PASSED [ 65%]
tests/unit/test_constrained_decode.py::TestJSONFSM::test_step_chars_allowed_complete PASSED [ 65%]
tests/unit/test_constrained_decode.py::TestJSONFSM::test_push_text_valid_json PASSED [ 65%]
tests/unit/test_constrained_decode.py::TestJSONFSM::test_push_text_invalid_json PASSED [ 65%]
tests/unit/test_constrained_decode.py::TestSchemaValidator::test_validate_valid_object PASSED [ 65%]
tests/unit/test_constrained_decode.py::TestSchemaValidator::test_validate_missing_required PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestSchemaValidator::test_validate_wrong_type_string PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestSchemaValidator::test_validate_wrong_type_number PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestSchemaValidator::test_validate_wrong_type_boolean PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestSchemaValidator::test_validate_wrong_type_object PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestSchemaValidator::test_validate_wrong_type_array PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestSchemaValidator::test_validate_not_object PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestSchemaValidator::test_validate_extra_fields PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestSchemaValidator::test_set_schema PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestTokenLexicon::test_token_text PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestTokenLexicon::test_tokenizer_vocab_size PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestTokenLexicon::test_candidates_for_chars PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestTokenLexicon::test_by_first_map_built PASSED [ 66%]
tests/unit/test_constrained_decode.py::TestJSONConstrainedDecoder::test_init PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestJSONConstrainedDecoder::test_init_with_registry PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestJSONConstrainedDecoder::test_start PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestJSONConstrainedDecoder::test_allowed_token_mask_empty PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestJSONConstrainedDecoder::test_allowed_token_mask_inside_string PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestJSONConstrainedDecoder::test_allowed_token_mask_eos_token PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestJSONConstrainedDecoder::test_push_token PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestJSONConstrainedDecoder::test_finalize_valid_json PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestJSONConstrainedDecoder::test_finalize_invalid_json PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestJSONConstrainedDecoder::test_finalize_schema_validation_failure PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestJSONConstrainedDecoder::test_end_to_end_simple PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestDecoderIntegration::test_with_real_tokenizer FAILED [ 67%]
tests/unit/test_constrained_decode.py::TestDecoderIntegration::test_dynamic_schema_switching PASSED [ 67%]
tests/unit/test_constrained_decode.py::TestDecoderIntegration::test_start_resets_schema PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_synthesize_prompt_basic PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_synthesize_prompt_control_case PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_synthesize_prompt_with_integration PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_all_scenarios PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_all_complexities PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_all_structures PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_adversarial_range_violation PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_adversarial_malformed_json PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_adversarial_ambiguity PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_multilingual_generation PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_long_context_token_aware PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_long_context_byte_based PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_stratification_enforcement PASSED [ 68%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_tool_result_fields_dict PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_control_case_decline PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_compact_caws_header PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestGenerateContextualPrompts::test_normalize_text PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_extract_process_step_targets_basic PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_extract_process_step_targets_control_case PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_extract_process_step_targets_preserves_tool_result_fields PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_multi_call_extraction PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_token_span_alignment_edge_cases PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_normalization_preservation PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_control_case_filtering PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_error_handling_invalid_tokenizer PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_integration_field_extraction PASSED [ 69%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_preserve_existing_metadata PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_validate_json_args PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_validate_json_args_invalid PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestExtractProcessTargets::test_extract_with_normalized_text PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_verify_item_basic PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_verify_item_control_case PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_compute_integration_f1 PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_grounded_values_in_span PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_is_long_context_item PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_caws_header_validation PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_privacy_scanning FAILED [ 70%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_semantic_validation PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_stratification_validation PASSED [ 70%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_multi_call_parity PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_grounding_validation PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_integration_f1_edge_cases PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_error_handling_malformed_items PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_contains_grounding PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_parse_tool_json_slice PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_check_stratification_backbone PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_verify_token_alignment PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_retry_case_validation PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_adversarial_range_violation_validation PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_adversarial_malformed_json_validation PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_adversarial_ambiguity_validation PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_multi_call_span_validation PASSED [ 71%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_token_alignment_with_tokenizer PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_low_integration_f1_detection PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_unknown_tool_detection PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_retry_insufficient_attempts PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_retry_last_attempt_failed PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_retry_missing_spans_or_attempts PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_missing_json_args_span PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_json_parse_fail PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_adversarial_range_violation_not_handled PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_adversarial_malformed_json_not_repaired PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_adversarial_ambiguity_not_clarified PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_multi_call_span_out_of_bounds PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_integration_not_grounded PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_low_integration_f1_flagging PASSED [ 72%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_is_long_context_with_tokenizer PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_is_long_context_fallback_to_bytes PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_is_long_context_item_with_tokenizer PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_is_long_context_item_tokenizer_exception PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_contains_grounding_with_synonyms PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_contains_grounding_no_spans PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_contains_grounding_no_fields PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_check_stratification_backbone_small_n PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_check_stratification_backbone_large_n PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_basic PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_invalid_json PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_with_tokenizer PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_with_schema_validation PASSED [ 73%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_schema_validation_errors PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_unknown_tool_detection PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_integration_f1_calculation PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_long_context_counting PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_stratification_check PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_with_prompt_spans_target PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_empty_lines_skipped PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_schema_file_not_found PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_tokenizer_exception PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_eligible_results_calculation PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_integration_misses_tracking PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_grounding_misses_tracking PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_multi_call_parity_tracking PASSED [ 74%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_gate_failures PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_adversarial_quota_check PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_main_function_long_context_quota_check PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_grounded_values_in_span_numeric PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_grounded_values_in_span_url PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_grounded_values_in_span_empty_seg PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_grounded_values_in_span_empty_fields PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_verify_token_alignment_failure PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_verify_token_alignment_invalid_span PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_verify_token_alignment_exception PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_semantic_validation_invalid_args PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_multi_call_parity_mismatch PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_multi_call_name_span_mismatch PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_parse_tool_json_slice_no_braces PASSED [ 75%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_parse_tool_json_slice_exception PASSED [ 76%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_check_privacy_url_not_allowlisted PASSED [ 76%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_check_privacy_multiple_emails FAILED [ 76%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_check_privacy_multiple_uuids FAILED [ 76%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_check_caws_header_exception PASSED [ 76%]
tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_check_caws_header_missing_keys PASSED [ 76%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_missing_dependencies PASSED [ 76%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_invalid_inputs PASSED [ 76%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_file_io_errors PASSED [ 76%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_tokenizer_errors PASSED [ 76%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_memory_errors PASSED [ 76%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_json_parse_errors PASSED [ 76%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_empty_inputs PASSED [ 76%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_load_tokenizer_success PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_load_tokenizer_import_error PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_load_tokenizer_general_exception PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_main_function_basic PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_main_function_empty_file PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestErrorHandling::test_main_function_json_error PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestUtilTokenSpans::test_fast_tokenizer_offsets PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestUtilTokenSpans::test_slow_tokenizer_fallback PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestUtilTokenSpans::test_normalize_text_nfc PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestUtilTokenSpans::test_normalize_text_lf PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestUtilTokenSpans::test_normalize_text_both PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestUtilTokenSpans::test_empty_spans PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestUtilTokenSpans::test_out_of_bounds_spans PASSED [ 77%]
tests/unit/test_contextual_generation.py::TestUtilTokenSpans::test_unicode_handling PASSED [ 78%]
tests/unit/test_contextual_generation.py::TestUtilTokenSpans::test_byte_spans_to_token_spans_multiple PASSED [ 78%]
tests/unit/test_contextual_generation.py::TestUtilTokenSpans::test_byte_spans_to_token_spans_slow_fallback PASSED [ 78%]
tests/unit/test_contextual_generation.py::TestUtilTokenSpans::test_tokenizer_exception_fallback PASSED [ 78%]
tests/unit/test_contextual_generation.py::TestNegativeMutationSimulation::test_eligibility_filter_off_by_one PASSED [ 78%]
tests/unit/test_contextual_generation.py::TestNegativeMutationSimulation::test_control_case_not_removed PASSED [ 78%]
tests/unit/test_contextual_generation.py::TestNegativeMutationSimulation::test_empty_list_vs_none PASSED [ 78%]
tests/unit/test_contextual_generation.py::TestNegativeMutationSimulation::test_span_cap_off_by_one PASSED [ 78%]
tests/unit/test_contextual_generation.py::TestNegativeMutationSimulation::test_multi_call_parity_empty PASSED [ 78%]
tests/unit/test_contextual_generation.py::TestNegativeMutationSimulation::test_token_alignment_boundary PASSED [ 78%]
tests/unit/test_dataset.py::TestKDDataset::test_dataset_initialization PASSED [ 78%]
tests/unit/test_dataset.py::TestKDDataset::test_dataset_loads_jsonl PASSED [ 78%]
tests/unit/test_dataset.py::TestKDDataset::test_dataset_getitem PASSED   [ 78%]
tests/unit/test_dataset.py::TestKDDataset::test_dataset_with_teacher_logits PASSED [ 79%]
tests/unit/test_dataset.py::TestKDDataset::test_dataset_truncation PASSED [ 79%]
tests/unit/test_dataset.py::TestKDDataset::test_dataset_labels_shifted PASSED [ 79%]
tests/unit/test_dataset.py::TestCollateKDBatch::test_collate_basic PASSED [ 79%]
tests/unit/test_dataset.py::TestCollateKDBatch::test_collate_with_teacher_logits PASSED [ 79%]
tests/unit/test_dataset.py::TestCollateKDBatch::test_collate_variable_length PASSED [ 79%]
tests/unit/test_dataset.py::TestCollateKDBatch::test_collate_pads_labels_with_ignore_index PASSED [ 79%]
tests/unit/test_dataset_modules.py::TestAnswerGenerationDataset::test_dataset_initialization PASSED [ 79%]
tests/unit/test_dataset_modules.py::TestAnswerGenerationDataset::test_dataset_getitem_basic FAILED [ 79%]
tests/unit/test_dataset_modules.py::TestAnswerGenerationDataset::test_dataset_prompt_formatting FAILED [ 79%]
tests/unit/test_dataset_modules.py::TestAnswerGenerationDataset::test_dataset_empty_tools FAILED [ 79%]
tests/unit/test_dataset_modules.py::TestAnswerGenerationDataset::test_dataset_missing_fields FAILED [ 79%]
tests/unit/test_dataset_modules.py::TestAnswerGenerationDataset::test_dataset_file_not_found PASSED [ 79%]
tests/unit/test_dataset_modules.py::TestAnswerGenerationDataset::test_collate_answer_generation_batch PASSED [ 79%]
tests/unit/test_dataset_modules.py::TestPostToolDataset::test_dataset_initialization PASSED [ 80%]
tests/unit/test_dataset_modules.py::TestPostToolDataset::test_dataset_getitem FAILED [ 80%]
tests/unit/test_dataset_modules.py::TestPostToolDataset::test_dataset_tool_result_formatting FAILED [ 80%]
tests/unit/test_dataset_modules.py::TestPostToolDataset::test_collate_post_tool_batch PASSED [ 80%]
tests/unit/test_dataset_modules.py::TestToolSelectDataset::test_dataset_initialization PASSED [ 80%]
tests/unit/test_dataset_modules.py::TestToolSelectDataset::test_dataset_getitem FAILED [ 80%]
tests/unit/test_dataset_modules.py::TestToolSelectDataset::test_dataset_tool_formatting FAILED [ 80%]
tests/unit/test_dataset_modules.py::TestToolSelectDataset::test_dataset_empty_tools FAILED [ 80%]
tests/unit/test_dataset_modules.py::TestToolSelectDataset::test_collate_tool_select_batch PASSED [ 80%]
tests/unit/test_dataset_modules.py::TestToolSelectDataset::test_dataset_json_formatting FAILED [ 80%]
tests/unit/test_dataset_modules.py::TestDatasetErrorHandling::test_answer_generation_invalid_json FAILED [ 80%]
tests/unit/test_dataset_modules.py::TestDatasetErrorHandling::test_datasets_empty_file PASSED [ 80%]
tests/unit/test_dataset_modules.py::TestDatasetErrorHandling::test_datasets_whitespace_only_lines PASSED [ 80%]
tests/unit/test_entropy_scheduling.py::TestEntropyWeighting::test_high_entropy_high_temperature PASSED [ 81%]
tests/unit/test_entropy_scheduling.py::TestEntropyWeighting::test_low_entropy_low_temperature PASSED [ 81%]
tests/unit/test_entropy_scheduling.py::TestEntropyWeighting::test_entropy_computation PASSED [ 81%]
tests/unit/test_entropy_scheduling.py::TestEntropyWeighting::test_temperature_range PASSED [ 81%]
tests/unit/test_entropy_scheduling.py::TestEntropyWeighting::test_weight_normalization PASSED [ 81%]
tests/unit/test_entropy_scheduling.py::TestEntropyWeighting::test_entropy_clamping PASSED [ 81%]
tests/unit/test_entropy_scheduling.py::TestEntropyWeighting::test_entropy_vs_linear_schedule PASSED [ 81%]
tests/unit/test_enumerated_shapes.py::TestSampleEnumeratedShape::test_default_production_mix_4_shapes PASSED [ 81%]
tests/unit/test_enumerated_shapes.py::TestSampleEnumeratedShape::test_custom_shape_probs FAILED [ 81%]
tests/unit/test_enumerated_shapes.py::TestSampleEnumeratedShape::test_periodic_upweight_rare PASSED [ 81%]
tests/unit/test_enumerated_shapes.py::TestSampleEnumeratedShape::test_no_periodic_upweight PASSED [ 81%]
tests/unit/test_enumerated_shapes.py::TestTruncateBatchToShape::test_truncate_sequence_keys PASSED [ 81%]
tests/unit/test_enumerated_shapes.py::TestTruncateBatchToShape::test_truncate_sequence_vocab_keys PASSED [ 81%]
tests/unit/test_enumerated_shapes.py::TestTruncateBatchToShape::test_no_truncation_when_shorter PASSED [ 82%]
tests/unit/test_enumerated_shapes.py::TestTruncateBatchToShape::test_preserve_metadata_keys PASSED [ 82%]
tests/unit/test_enumerated_shapes.py::TestTruncateBatchToShape::test_all_sequence_keys_truncated PASSED [ 82%]
tests/unit/test_extractors.py::TestToolCallExtraction::test_extract_tool_call_valid_json PASSED [ 82%]
tests/unit/test_extractors.py::TestToolCallExtraction::test_extract_tool_call_embedded PASSED [ 82%]
tests/unit/test_extractors.py::TestToolCallExtraction::test_extract_tool_call_no_match PASSED [ 82%]
tests/unit/test_extractors.py::TestToolCallExtraction::test_extract_tool_call_with_tool_names PASSED [ 82%]
tests/unit/test_extractors.py::TestToolNameSpanExtraction::test_extract_tool_name_span_found PASSED [ 82%]
tests/unit/test_extractors.py::TestToolNameSpanExtraction::test_extract_tool_name_span_not_found PASSED [ 82%]
tests/unit/test_extractors.py::TestToolNameSpanExtraction::test_extract_tool_name_span_json PASSED [ 82%]
tests/unit/test_extractors.py::TestJSONArgumentSpanExtraction::test_extract_json_argument_spans_simple PASSED [ 82%]
tests/unit/test_extractors.py::TestJSONArgumentSpanExtraction::test_extract_json_argument_spans_multiple PASSED [ 82%]
tests/unit/test_extractors.py::TestJSONArgumentSpanExtraction::test_extract_json_argument_spans_nested PASSED [ 82%]
tests/unit/test_extractors.py::TestJSONArgumentSpanExtraction::test_extract_json_argument_spans_no_json PASSED [ 82%]
tests/unit/test_extractors.py::TestIntegrationSpanIdentification::test_identify_integration_spans_with_results PASSED [ 83%]
tests/unit/test_extractors.py::TestIntegrationSpanIdentification::test_identify_integration_spans_no_results PASSED [ 83%]
tests/unit/test_extractors.py::TestIntegrationSpanIdentification::test_identify_integration_spans_patterns PASSED [ 83%]
tests/unit/test_gqa_transformer.py::TestRMSNorm::test_rmsnorm_forward PASSED [ 83%]
tests/unit/test_gqa_transformer.py::TestRMSNorm::test_rmsnorm_normalization PASSED [ 83%]
tests/unit/test_gqa_transformer.py::TestRMSNorm::test_rmsnorm_gradient_flow PASSED [ 83%]
tests/unit/test_gqa_transformer.py::TestSwiGLU::test_swiglu_forward PASSED [ 83%]
tests/unit/test_gqa_transformer.py::TestSwiGLU::test_swiglu_gradient_flow PASSED [ 83%]
tests/unit/test_gqa_transformer.py::TestRotaryEmbedding::test_rope_apply PASSED [ 83%]
tests/unit/test_gqa_transformer.py::TestRotaryEmbedding::test_rope_apply_single PASSED [ 83%]
tests/unit/test_gqa_transformer.py::TestRotaryEmbedding::test_rope_scaling_dynamic PASSED [ 83%]
tests/unit/test_gqa_transformer.py::TestRotaryEmbedding::test_rope_rotation_property PASSED [ 83%]
tests/unit/test_gqa_transformer.py::TestMHAGQA::test_mha_gqa_forward PASSED [ 83%]
tests/unit/test_gqa_transformer.py::TestMHAGQA::test_mha_gqa_with_mask FAILED [ 84%]
tests/unit/test_gqa_transformer.py::TestMHAGQA::test_mha_gqa_forward_decode PASSED [ 84%]
tests/unit/test_gqa_transformer.py::TestMHAGQA::test_mha_gqa_gqa_expansion PASSED [ 84%]
tests/unit/test_gqa_transformer.py::TestBlock::test_block_forward PASSED [ 84%]
tests/unit/test_gqa_transformer.py::TestBlock::test_block_residual_connection PASSED [ 84%]
tests/unit/test_gqa_transformer.py::TestBlock::test_block_forward_decode PASSED [ 84%]
tests/unit/test_gqa_transformer.py::TestStudentLM::test_studentlm_forward PASSED [ 84%]
tests/unit/test_gqa_transformer.py::TestStudentLM::test_studentlm_forward_with_mask FAILED [ 84%]
tests/unit/test_gqa_transformer.py::TestStudentLM::test_studentlm_forward_decode PASSED [ 84%]
tests/unit/test_gqa_transformer.py::TestStudentLM::test_studentlm_gradient_flow PASSED [ 84%]
tests/unit/test_gqa_transformer.py::TestStudentLM::test_studentlm_default_config PASSED [ 84%]
tests/unit/test_gqa_transformer.py::TestStudentLM::test_studentlm_consistency_prefill_decode PASSED [ 84%]
tests/unit/test_input_validation.py::TestValidationError::test_validation_error_creation PASSED [ 84%]
tests/unit/test_input_validation.py::TestInputValidatorInitialization::test_init_default PASSED [ 85%]
tests/unit/test_input_validation.py::TestInputValidatorInitialization::test_init_non_strict PASSED [ 85%]
tests/unit/test_input_validation.py::TestInputValidatorInitialization::test_suspicious_patterns_compiled PASSED [ 85%]
tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_valid PASSED [ 85%]
tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_too_long FAILED [ 85%]
tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_suspicious_script FAILED [ 85%]
tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_javascript_url FAILED [ 85%]
tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_event_handler FAILED [ 85%]
tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_custom_field_name PASSED [ 85%]
tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_none_input FAILED [ 85%]
tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_empty_string PASSED [ 85%]
tests/unit/test_input_validation.py::TestStructuredDataValidation::test_validate_structured_data_valid FAILED [ 85%]
tests/unit/test_input_validation.py::TestStructuredDataValidation::test_validate_structured_data_missing_required FAILED [ 85%]
tests/unit/test_input_validation.py::TestStructuredDataValidation::test_validate_structured_data_invalid_types FAILED [ 86%]
tests/unit/test_input_validation.py::TestStructuredDataValidation::test_validate_structured_data_suspicious_content FAILED [ 86%]
tests/unit/test_input_validation.py::TestStructuredDataValidation::test_validate_structured_data_too_long FAILED [ 86%]
tests/unit/test_input_validation.py::TestToolValidation::test_validate_tools_valid FAILED [ 86%]
tests/unit/test_input_validation.py::TestToolValidation::test_validate_tools_too_many FAILED [ 86%]
tests/unit/test_input_validation.py::TestToolValidation::test_validate_tools_missing_fields FAILED [ 86%]
tests/unit/test_input_validation.py::TestToolValidation::test_validate_tools_suspicious_description FAILED [ 86%]
tests/unit/test_input_validation.py::TestFileValidation::test_validate_file_path_valid FAILED [ 86%]
tests/unit/test_input_validation.py::TestFileValidation::test_validate_file_path_nonexistent FAILED [ 86%]
tests/unit/test_input_validation.py::TestFileValidation::test_validate_file_path_too_large FAILED [ 86%]
tests/unit/test_input_validation.py::TestFileValidation::test_validate_file_path_permission_error FAILED [ 86%]
tests/unit/test_input_validation.py::TestTrainingDataValidation::test_validate_training_data_valid FAILED [ 86%]
tests/unit/test_input_validation.py::TestTrainingDataValidation::test_validate_training_data_invalid_structure FAILED [ 86%]
tests/unit/test_input_validation.py::TestTrainingDataValidation::test_validate_training_data_empty_list PASSED [ 86%]
tests/unit/test_input_validation.py::TestTrainingDataValidation::test_validate_tool_trace_valid FAILED [ 87%]
tests/unit/test_input_validation.py::TestTrainingDataValidation::test_validate_tool_trace_invalid PASSED [ 87%]
tests/unit/test_input_validation.py::TestSecurityPatterns::test_detect_suspicious_patterns_script_tags FAILED [ 87%]
tests/unit/test_input_validation.py::TestSecurityPatterns::test_detect_suspicious_patterns_javascript_url FAILED [ 87%]
tests/unit/test_input_validation.py::TestSecurityPatterns::test_detect_suspicious_patterns_iframe FAILED [ 87%]
tests/unit/test_input_validation.py::TestSecurityPatterns::test_detect_suspicious_patterns_event_handler FAILED [ 87%]
tests/unit/test_input_validation.py::TestSecurityPatterns::test_no_suspicious_patterns_normal_text FAILED [ 87%]
tests/unit/test_input_validation.py::TestSecurityPatterns::test_suspicious_patterns_case_insensitive FAILED [ 87%]
tests/unit/test_json_repair.py::TestJSONValidation::test_validate_json_valid_object PASSED [ 87%]
tests/unit/test_json_repair.py::TestJSONValidation::test_validate_json_valid_array PASSED [ 87%]
tests/unit/test_json_repair.py::TestJSONValidation::test_validate_json_invalid PASSED [ 87%]
tests/unit/test_json_repair.py::TestJSONValidation::test_validate_json_with_text PASSED [ 87%]
tests/unit/test_json_repair.py::TestJSONValidation::test_validate_json_empty PASSED [ 87%]
tests/unit/test_json_repair.py::TestJSONRepair::test_repair_json_valid PASSED [ 88%]
tests/unit/test_json_repair.py::TestJSONRepair::test_repair_json_missing_quote PASSED [ 88%]
tests/unit/test_json_repair.py::TestJSONRepair::test_repair_json_without_jsonrepair PASSED [ 88%]
tests/unit/test_json_repair.py::TestJSONRepairDetection::test_check_repair_needed_valid PASSED [ 88%]
tests/unit/test_json_repair.py::TestJSONRepairDetection::test_check_repair_needed_invalid PASSED [ 88%]
tests/unit/test_json_repair.py::TestJSONRepairDetection::test_check_repair_needed_without_jsonrepair PASSED [ 88%]
tests/unit/test_json_repair.py::TestBatchJSONRepair::test_batch_check_all_valid PASSED [ 88%]
tests/unit/test_json_repair.py::TestBatchJSONRepair::test_batch_check_mixed PASSED [ 88%]
tests/unit/test_json_repair.py::TestBatchJSONRepair::test_batch_check_empty PASSED [ 88%]
tests/unit/test_kv_cache_optimized.py::TestOptimizedKVCache::test_initialization PASSED [ 88%]
tests/unit/test_kv_cache_optimized.py::TestOptimizedKVCache::test_update PASSED [ 88%]
tests/unit/test_kv_cache_optimized.py::TestOptimizedKVCache::test_get_slice PASSED [ 88%]
tests/unit/test_kv_cache_optimized.py::TestOptimizedKVCache::test_get_full PASSED [ 88%]
tests/unit/test_kv_cache_optimized.py::TestOptimizedKVCache::test_clear PASSED [ 89%]
tests/unit/test_kv_cache_optimized.py::TestOptimizedKVCache::test_get_size_mb PASSED [ 89%]
tests/unit/test_kv_cache_optimized.py::TestOptimizedKVCache::test_stats PASSED [ 89%]
tests/unit/test_kv_cache_optimized.py::TestGroupedQueryKVCache::test_initialization PASSED [ 89%]
tests/unit/test_kv_cache_optimized.py::TestGroupedQueryKVCache::test_gqa_reduction PASSED [ 89%]
tests/unit/test_kv_cache_optimized.py::TestGroupedQueryKVCache::test_stats PASSED [ 89%]
tests/unit/test_kv_cache_optimized.py::TestCreateKVCacheForModel::test_create_standard_cache PASSED [ 89%]
tests/unit/test_kv_cache_optimized.py::TestCreateKVCacheForModel::test_create_gqa_cache PASSED [ 89%]
tests/unit/test_losses.py::TestKLDivergence::test_kl_divergence_basic PASSED [ 89%]
tests/unit/test_losses.py::TestKLDivergence::test_kl_divergence_temperature PASSED [ 89%]
tests/unit/test_losses.py::TestKLDivergence::test_kl_divergence_reduction PASSED [ 89%]
tests/unit/test_losses.py::TestKLDivergence::test_kl_divergence_identical_distributions PASSED [ 89%]
tests/unit/test_losses.py::TestCrossEntropyLoss::test_cross_entropy_basic PASSED [ 89%]
tests/unit/test_losses.py::TestCrossEntropyLoss::test_cross_entropy_ignore_index PASSED [ 89%]
tests/unit/test_losses.py::TestCombinedKDLoss::test_combined_kd_loss_basic PASSED [ 90%]
tests/unit/test_losses.py::TestCombinedKDLoss::test_combined_kd_loss_weights PASSED [ 90%]
tests/unit/test_losses.py::TestCombinedKDLoss::test_combined_kd_loss_gradient_flow PASSED [ 90%]
tests/unit/test_losses_speed.py::test_length_kd_completeness_exemption FAILED [ 90%]
tests/unit/test_losses_speed.py::test_length_kd_no_excess FAILED         [ 90%]
tests/unit/test_losses_speed.py::test_length_kd_hinge FAILED             [ 90%]
tests/unit/test_losses_speed.py::test_early_tool_ce_only_when_needed FAILED [ 90%]
tests/unit/test_losses_speed.py::test_early_tool_json_prior_fallback FAILED [ 90%]
tests/unit/test_losses_speed.py::test_early_tool_masked_when_not_needed FAILED [ 90%]
tests/unit/test_losses_speed.py::test_early_tool_ramp FAILED             [ 90%]
tests/unit/test_priority1_cot_free.py::TestCoTFreeValidation::test_dataset_rejects_reasoning_content PASSED [ 90%]
tests/unit/test_priority1_cot_free.py::TestCoTFreeValidation::test_dataset_accepts_valid_sample PASSED [ 90%]
tests/unit/test_priority1_cot_free.py::TestCoTFreeValidation::test_dataset_accepts_process_step_targets PASSED [ 90%]
tests/unit/test_priority1_cot_free.py::TestProcessStepLosses::test_tool_name_loss PASSED [ 91%]
tests/unit/test_priority1_cot_free.py::TestProcessStepLosses::test_json_argument_loss PASSED [ 91%]
tests/unit/test_priority1_cot_free.py::TestProcessStepLosses::test_integration_copy_loss PASSED [ 91%]
tests/unit/test_priority1_cot_free.py::TestProcessStepLosses::test_combined_kd_loss_with_process_step PASSED [ 91%]
tests/unit/test_priority1_cot_free.py::TestProcessStepLosses::test_combined_kd_loss_without_process_step PASSED [ 91%]
tests/unit/test_priority1_cot_free.py::TestExtractors::test_extract_tool_name_span PASSED [ 91%]
tests/unit/test_priority1_cot_free.py::TestExtractors::test_extract_json_argument_spans PASSED [ 91%]
tests/unit/test_priority1_cot_free.py::TestExtractors::test_identify_integration_spans PASSED [ 91%]
tests/unit/test_process_losses.py::TestValidateJSON::test_validate_json_valid_object PASSED [ 91%]
tests/unit/test_process_losses.py::TestValidateJSON::test_validate_json_valid_array PASSED [ 91%]
tests/unit/test_process_losses.py::TestValidateJSON::test_validate_json_invalid_missing_quote PASSED [ 91%]
tests/unit/test_process_losses.py::TestValidateJSON::test_validate_json_invalid_trailing_comma PASSED [ 91%]
tests/unit/test_process_losses.py::TestValidateJSON::test_validate_json_with_text_around PASSED [ 91%]
tests/unit/test_process_losses.py::TestValidateJSON::test_validate_json_empty_string PASSED [ 92%]
tests/unit/test_process_losses.py::TestValidateJSON::test_validate_json_no_json PASSED [ 92%]
tests/unit/test_process_losses.py::TestValidateJSON::test_validate_json_multiple_json_objects PASSED [ 92%]
tests/unit/test_process_losses.py::TestExtractToolCall::test_extract_tool_call_valid PASSED [ 92%]
tests/unit/test_process_losses.py::TestExtractToolCall::test_extract_tool_call_not_in_list PASSED [ 92%]
tests/unit/test_process_losses.py::TestExtractToolCall::test_extract_tool_call_no_name_field PASSED [ 92%]
tests/unit/test_process_losses.py::TestExtractToolCall::test_extract_tool_call_invalid_json PASSED [ 92%]
tests/unit/test_process_losses.py::TestExtractToolCall::test_extract_tool_call_with_text_around PASSED [ 92%]
tests/unit/test_process_losses.py::TestExtractToolCall::test_extract_tool_call_empty_tool_names PASSED [ 92%]
tests/unit/test_process_losses.py::TestJSONValidityLoss::test_json_validity_loss_all_valid PASSED [ 92%]
tests/unit/test_process_losses.py::TestJSONValidityLoss::test_json_validity_loss_all_invalid PASSED [ 92%]
tests/unit/test_process_losses.py::TestJSONValidityLoss::test_json_validity_loss_mixed PASSED [ 92%]
tests/unit/test_process_losses.py::TestJSONValidityLoss::test_json_validity_loss_empty_list PASSED [ 92%]
tests/unit/test_process_losses.py::TestToolSelectionLoss::test_tool_selection_loss_correct PASSED [ 93%]
tests/unit/test_process_losses.py::TestToolSelectionLoss::test_tool_selection_loss_incorrect PASSED [ 93%]
tests/unit/test_process_losses.py::TestToolSelectionLoss::test_tool_selection_loss_no_targets PASSED [ 93%]
tests/unit/test_process_losses.py::TestToolSelectionLoss::test_tool_selection_loss_invalid_json PASSED [ 93%]
tests/unit/test_process_losses.py::TestProcessSupervisionLoss::test_process_supervision_loss_both_components PASSED [ 93%]
tests/unit/test_process_losses.py::TestProcessSupervisionLoss::test_process_supervision_loss_no_targets PASSED [ 93%]
tests/unit/test_process_losses.py::TestProcessSupervisionLoss::test_process_supervision_loss_weight_scaling PASSED [ 93%]
tests/unit/test_prompt_cache.py::TestPromptCache::test_cache_hit PASSED  [ 93%]
tests/unit/test_prompt_cache.py::TestPromptCache::test_cache_miss_different_prompts PASSED [ 93%]
tests/unit/test_prompt_cache.py::TestPromptCache::test_cache_size_limit PASSED [ 93%]
tests/unit/test_prompt_cache.py::TestPromptCache::test_cache_stats PASSED [ 93%]
tests/unit/test_prompt_cache.py::TestPromptCache::test_cache_clear PASSED [ 93%]
tests/unit/test_prompt_cache.py::TestPromptCache::test_precomputed_hash PASSED [ 93%]
tests/unit/test_prompt_cache.py::TestExtractSystemPrompt::test_extract_system_prefix PASSED [ 93%]
tests/unit/test_prompt_cache.py::TestExtractSystemPrompt::test_extract_first_paragraph PASSED [ 94%]
tests/unit/test_prompt_cache.py::TestExtractSystemPrompt::test_extract_before_user_marker PASSED [ 94%]
tests/unit/test_prompt_cache.py::TestExtractSystemPrompt::test_no_system_prompt PASSED [ 94%]
tests/unit/test_prompt_cache.py::TestExtractSystemPrompt::test_extract_prompt_parts PASSED [ 94%]
tests/unit/test_prompt_cache.py::TestPromptCacheIntegration::test_cache_with_real_state PASSED [ 94%]
tests/unit/test_qat_integration.py::TestShouldEnableQAT::test_qat_disabled PASSED [ 94%]
tests/unit/test_qat_integration.py::TestShouldEnableQAT::test_qat_enabled_at_start_fraction PASSED [ 94%]
tests/unit/test_qat_integration.py::TestShouldEnableQAT::test_qat_default_start_fraction PASSED [ 94%]
tests/unit/test_qat_integration.py::TestApplyQATToModel::test_qat_not_available PASSED [ 94%]
tests/unit/test_qat_integration.py::TestApplyQATToModel::test_qat_config_parameters PASSED [ 94%]
tests/unit/test_qat_integration.py::TestCheckQATStability::test_stability_check_no_nan PASSED [ 94%]
tests/unit/test_qat_integration.py::TestCheckQATStability::test_stability_check_with_nan FAILED [ 94%]
tests/unit/test_qat_integration.py::TestCheckQATStability::test_stability_check_error_handling PASSED [ 94%]
tests/unit/test_qat_integration.py::TestCheckQATStability::test_stability_check_model_mode PASSED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_init PASSED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_get_schema_existing PASSED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_get_schema_nonexistent PASSED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_register_schema PASSED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_validate_tool_call_valid FAILED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_validate_tool_call_missing_required PASSED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_validate_tool_call_name_mismatch FAILED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_validate_generic_fallback PASSED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_list_tools PASSED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_get_generic_schema PASSED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_save_schema PASSED [ 95%]
tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_load_schemas_from_directory PASSED [ 95%]
tests/unit/test_schema_registry.py::TestGetRegistry::test_get_registry_singleton PASSED [ 96%]
tests/unit/test_schema_registry.py::TestGetRegistry::test_get_registry_functionality PASSED [ 96%]
tests/unit/test_security_contextual.py::TestInputValidation::test_malicious_input_handling PASSED [ 96%]
tests/unit/test_security_contextual.py::TestInputValidation::test_path_traversal_prevention PASSED [ 96%]
tests/unit/test_security_contextual.py::TestInputValidation::test_command_injection_prevention PASSED [ 96%]
tests/unit/test_security_contextual.py::TestPIIRedaction::test_pii_redaction PASSED [ 96%]
tests/unit/test_security_contextual.py::TestPIIRedaction::test_uuid_detection FAILED [ 96%]
tests/unit/test_security_contextual.py::TestPIIRedaction::test_email_detection FAILED [ 96%]
tests/unit/test_security_contextual.py::TestURLAllowlist::test_url_allowlist PASSED [ 96%]
tests/unit/test_security_contextual.py::TestURLAllowlist::test_url_validation PASSED [ 96%]
tests/unit/test_security_contextual.py::TestSafetyScanning::test_safety_scanning PASSED [ 96%]
tests/unit/test_speculative_decode.py::TestSpeculativeDecoder::test_initialization PASSED [ 96%]
tests/unit/test_speculative_decode.py::TestSpeculativeDecoder::test_generate_basic PASSED [ 96%]
tests/unit/test_speculative_decode.py::TestSpeculativeDecoder::test_draft_k_tokens FAILED [ 96%]
tests/unit/test_speculative_decode.py::TestSpeculativeDecoder::test_verify_tokens_accept FAILED [ 97%]
tests/unit/test_speculative_decode.py::TestSpeculativeDecoder::test_verify_tokens_reject FAILED [ 97%]
tests/unit/test_speculative_decode.py::TestSpeculativeDecoder::test_stats_tracking PASSED [ 97%]
tests/unit/test_speculative_decode.py::TestSpeculativeDecoder::test_reset_stats PASSED [ 97%]
tests/unit/test_speed_gates.py::TestEvaluateSpeedGates::test_no_baseline_skips_gates PASSED [ 97%]
tests/unit/test_speed_gates.py::TestEvaluateSpeedGates::test_hardware_mismatch_skips_gates PASSED [ 97%]
tests/unit/test_speed_gates.py::TestEvaluateSpeedGates::test_ttft_regression_pass PASSED [ 97%]
tests/unit/test_speed_gates.py::TestEvaluateSpeedGates::test_ttft_regression_fail PASSED [ 97%]
tests/unit/test_speed_gates.py::TestEvaluateSpeedGates::test_tps_regression_pass PASSED [ 97%]
tests/unit/test_speed_gates.py::TestEvaluateSpeedGates::test_tps_regression_fail PASSED [ 97%]
tests/unit/test_speed_gates.py::TestEvaluateSpeedGates::test_ttfa_gate_pass PASSED [ 97%]
tests/unit/test_speed_gates.py::TestEvaluateSpeedGates::test_ttfa_gate_fail PASSED [ 97%]
tests/unit/test_speed_gates.py::TestEvaluateSpeedGates::test_all_gates_pass PASSED [ 97%]
tests/unit/test_speed_gates.py::TestLoadBaselineSpeedMetrics::test_load_baseline_success PASSED [ 98%]
tests/unit/test_speed_gates.py::TestLoadBaselineSpeedMetrics::test_load_baseline_not_found PASSED [ 98%]
tests/unit/test_speed_gates.py::TestLoadBaselineSpeedMetrics::test_load_baseline_invalid_json PASSED [ 98%]
tests/unit/test_speed_metrics.py::TestMeasureProxy::test_ttft_measurement PASSED [ 98%]
tests/unit/test_speed_metrics.py::TestMeasureProxy::test_tps_measurement PASSED [ 98%]
tests/unit/test_speed_metrics.py::TestMeasureProxy::test_ttfa_measurement PASSED [ 98%]
tests/unit/test_speed_metrics.py::TestMeasureProxy::test_ttfa_no_tool PASSED [ 98%]
tests/unit/test_speed_metrics.py::TestMeasureProxy::test_no_tokenizer PASSED [ 98%]
tests/unit/test_speed_metrics.py::TestAggregateSpeedMetrics::test_aggregate_empty_list PASSED [ 98%]
tests/unit/test_speed_metrics.py::TestAggregateSpeedMetrics::test_aggregate_single_metric PASSED [ 98%]
tests/unit/test_speed_metrics.py::TestAggregateSpeedMetrics::test_aggregate_multiple_metrics PASSED [ 98%]
tests/unit/test_speed_metrics.py::TestAggregateSpeedMetrics::test_aggregate_filters_inf PASSED [ 98%]
tests/unit/test_speed_metrics.py::TestIsValidToolJSON::test_valid_tool_json PASSED [ 98%]
tests/unit/test_speed_metrics.py::TestIsValidToolJSON::test_invalid_tool_json PASSED [ 99%]
tests/unit/test_speed_metrics.py::TestIsValidToolJSON::test_partial_json PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestRingBuffer::test_initialization PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestRingBuffer::test_write_and_read PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestRingBuffer::test_wrap_around PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestRingBuffer::test_clear PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestOptimizedTokenizer::test_initialization PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestOptimizedTokenizer::test_encode_optimized PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestOptimizedTokenizer::test_encode_batch_optimized PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestOptimizedTokenizer::test_encode_streaming PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestOptimizedTokenizer::test_decode_optimized PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestOptimizedTokenizer::test_decode_batch_optimized PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestOptimizedTokenizer::test_stats PASSED [ 99%]
tests/unit/test_tokenizer_optimized.py::TestWrapTokenizer::test_wrap_tokenizer PASSED [100%]

==================================== ERRORS ====================================
________ ERROR at setup of TestHuggingFaceBackend.test_hf_sample_basic _________
venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:402: in hf_raise_for_status
    response.raise_for_status()
venv/lib/python3.11/site-packages/requests/models.py:1026: in raise_for_status
    raise HTTPError(http_error_msg, response=self)
E   requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/test/model/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:
venv/lib/python3.11/site-packages/transformers/utils/hub.py:479: in cached_files
    hf_hub_download(
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1007: in hf_hub_download
    return _hf_hub_download_to_cache_dir(
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1114: in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1655: in _raise_on_head_call_error
    raise head_call_error
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1543: in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114: in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1460: in get_hf_file_metadata
    r = _request_wrapper(
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:283: in _request_wrapper
    response = _request_wrapper(
venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:307: in _request_wrapper
    hf_raise_for_status(response)
venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:452: in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
E   huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6916795c-3b3a0b1c06b772b76fdc0adc;fb5c8d7c-3d2f-48cf-af11-b5aea7d37151)
E   
E   Repository Not Found for url: https://huggingface.co/test/model/resolve/main/tokenizer_config.json.
E   Please make sure you specified the correct `repo_id` and `repo_type`.
E   If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication

The above exception was the direct cause of the following exception:
tests/models/test_teacher_client.py:348: in hf_client
    return TeacherClient(backend="hf", model_name="test/model")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
models/teacher/teacher_client.py:114: in __init__
    self._load_hf_model(model_name, device)
models/teacher/teacher_client.py:334: in _load_hf_model
    self._tokenizer = AutoTokenizer.from_pretrained(model_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1073: in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:905: in get_tokenizer_config
    resolved_config_file = cached_file(
venv/lib/python3.11/site-packages/transformers/utils/hub.py:322: in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/utils/hub.py:511: in cached_files
    raise OSError(
E   OSError: test/model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
E   If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
---------------------------- Captured stdout setup -----------------------------
[TeacherClient] Loading HuggingFace model: test/model
=================================== FAILURES ===================================
______________ TestLoadContract.test_load_contract_file_not_found ______________
tests/conversion/test_convert_coreml.py:51: in test_load_contract_file_not_found
    with pytest.raises(FileNotFoundError):
E   Failed: DID NOT RAISE <class 'FileNotFoundError'>
______ TestConvertPyTorchToCoreML.test_convert_pytorch_to_coreml_success _______
tests/conversion/test_convert_coreml.py:83: in test_convert_pytorch_to_coreml_success
    with patch('conversion.convert_coreml.ct') as mock_ct, \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'conversion.convert_coreml' from '/Users/darianrosebrook/Desktop/Projects/distill/conversion/convert_coreml.py'> does not have the attribute 'ct'
_ TestConvertPyTorchToCoreML.test_convert_pytorch_to_coreml_with_ane_optimization _
tests/conversion/test_convert_coreml.py:115: in test_convert_pytorch_to_coreml_with_ane_optimization
    with patch('conversion.convert_coreml.ct') as mock_ct, \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'conversion.convert_coreml' from '/Users/darianrosebrook/Desktop/Projects/distill/conversion/convert_coreml.py'> does not have the attribute 'ct'
__ TestConvertPyTorchToCoreML.test_convert_pytorch_to_coreml_ane_incompatible __
tests/conversion/test_convert_coreml.py:156: in test_convert_pytorch_to_coreml_ane_incompatible
    result = convert_pytorch_to_coreml(
E   TypeError: convert_pytorch_to_coreml() got an unexpected keyword argument 'model'
_ TestConvertPyTorchToCoreML.test_convert_pytorch_to_coreml_conversion_failure _
tests/conversion/test_convert_coreml.py:171: in test_convert_pytorch_to_coreml_conversion_failure
    with patch('conversion.convert_coreml.ct') as mock_ct, \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'conversion.convert_coreml' from '/Users/darianrosebrook/Desktop/Projects/distill/conversion/convert_coreml.py'> does not have the attribute 'ct'
_________ TestConvertONNXToCoreML.test_convert_onnx_to_coreml_success __________
tests/conversion/test_convert_coreml.py:193: in test_convert_onnx_to_coreml_success
    with patch('conversion.convert_coreml.ct') as mock_ct, \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'conversion.convert_coreml' from '/Users/darianrosebrook/Desktop/Projects/distill/conversion/convert_coreml.py'> does not have the attribute 'ct'
____ TestConvertONNXToCoreML.test_convert_onnx_to_coreml_with_custom_target ____
tests/conversion/test_convert_coreml.py:220: in test_convert_onnx_to_coreml_with_custom_target
    with patch('conversion.convert_coreml.ct') as mock_ct, \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'conversion.convert_coreml' from '/Users/darianrosebrook/Desktop/Projects/distill/conversion/convert_coreml.py'> does not have the attribute 'ct'
______ TestConvertONNXToCoreML.test_convert_onnx_to_coreml_file_not_found ______
tests/conversion/test_convert_coreml.py:246: in test_convert_onnx_to_coreml_file_not_found
    with patch('conversion.convert_coreml.ct') as mock_ct, \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'conversion.convert_coreml' from '/Users/darianrosebrook/Desktop/Projects/distill/conversion/convert_coreml.py'> does not have the attribute 'ct'
____________ TestCreatePlaceholder.test_create_placeholder_success _____________
tests/conversion/test_convert_coreml.py:270: in test_create_placeholder_success
    assert result == str(output_path)
E   AssertionError: assert None == '/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_create_placeholder_succes0/placeholder.mlpackage'
E    +  where '/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_create_placeholder_succes0/placeholder.mlpackage' = str(PosixPath('/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_create_placeholder_succes0/placeholder.mlpackage'))
----------------------------- Captured stdout call -----------------------------
[convert_coreml] Placeholder created: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_create_placeholder_succes0/placeholder.mlpackage
________________ TestMainFunction.test_main_pytorch_conversion _________________
conversion/convert_coreml.py:620: in main
    pytorch_model = torch.jit.load(args.input_path)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/jit/_serialization.py:161: in load
    cpp_module = torch._C.import_ir_module_from_buffer(
E   TypeError: import_ir_module_from_buffer(): incompatible function arguments. The following argument types are supported:
E       1. (arg0: torch._C.CompilationUnit, arg1: str, arg2: object, arg3: dict, arg4: bool) -> torch._C.ScriptModule
E   
E   Invoked with: <torch.jit.CompilationUnit object at 0x17a337cb0>, <Mock name='ArgumentParser().parse_args().input_path.read()' id='6338223120'>, None, {}, False

During handling of the above exception, another exception occurred:
conversion/convert_coreml.py:627: in main
    with open(args.input_path, "rb") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: expected str, bytes or os.PathLike object, not Mock

During handling of the above exception, another exception occurred:
tests/conversion/test_convert_coreml.py:325: in test_main_pytorch_conversion
    main()
conversion/convert_coreml.py:631: in main
    raise RuntimeError(
E   RuntimeError: Failed to load PyTorch model from <Mock name='ArgumentParser().parse_args().input_path' id='6337646352'>. Expected TorchScript (.pt) or ExportedProgram. Error: expected str, bytes or os.PathLike object, not Mock
________ TestMainFunction.test_main_conversion_failure_with_placeholder ________
conversion/convert_coreml.py:620: in main
    pytorch_model = torch.jit.load(args.input_path)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/jit/_serialization.py:161: in load
    cpp_module = torch._C.import_ir_module_from_buffer(
E   TypeError: import_ir_module_from_buffer(): incompatible function arguments. The following argument types are supported:
E       1. (arg0: torch._C.CompilationUnit, arg1: str, arg2: object, arg3: dict, arg4: bool) -> torch._C.ScriptModule
E   
E   Invoked with: <torch.jit.CompilationUnit object at 0x1789362f0>, <Mock name='ArgumentParser().parse_args().input_path.read()' id='6344245904'>, None, {}, False

During handling of the above exception, another exception occurred:
conversion/convert_coreml.py:627: in main
    with open(args.input_path, "rb") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: expected str, bytes or os.PathLike object, not Mock

During handling of the above exception, another exception occurred:
tests/conversion/test_convert_coreml.py:402: in test_main_conversion_failure_with_placeholder
    main()
conversion/convert_coreml.py:631: in main
    raise RuntimeError(
E   RuntimeError: Failed to load PyTorch model from <Mock name='ArgumentParser().parse_args().input_path' id='6340190672'>. Expected TorchScript (.pt) or ExportedProgram. Error: expected str, bytes or os.PathLike object, not Mock
__________________ TestMainFunction.test_main_invalid_backend __________________
conversion/convert_coreml.py:444: in convert_onnx_to_coreml
    onnx_model = onnx.load(onnx_path)
                 ^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:229: in load_model
    model = _get_serializer(format, f).deserialize_proto(_load_bytes(f), ModelProto())
            ^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:197: in _get_serializer
    if (file_path := _get_file_path(f)) is not None:
                     ^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:185: in _get_file_path
    return os.path.abspath(f.name)
           ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: expected str, bytes or os.PathLike object, not Mock

During handling of the above exception, another exception occurred:
tests/conversion/test_convert_coreml.py:419: in test_main_invalid_backend
    main()
conversion/convert_coreml.py:655: in main
    convert_onnx_to_coreml(
conversion/convert_coreml.py:446: in convert_onnx_to_coreml
    raise RuntimeError(f"Failed to load ONNX model: {e}")
E   RuntimeError: Failed to load ONNX model: expected str, bytes or os.PathLike object, not Mock
----------------------------- Captured stdout call -----------------------------
[convert_coreml]   Skipping version check for toy model (testing mode)
[convert_coreml] Loading ONNX model: <Mock name='ArgumentParser().parse_args().input_path' id='6344030608'>
________________ TestMainFunction.test_main_contract_not_found _________________
conversion/convert_coreml.py:444: in convert_onnx_to_coreml
    onnx_model = onnx.load(onnx_path)
                 ^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:229: in load_model
    model = _get_serializer(format, f).deserialize_proto(_load_bytes(f), ModelProto())
            ^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:197: in _get_serializer
    if (file_path := _get_file_path(f)) is not None:
                     ^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:185: in _get_file_path
    return os.path.abspath(f.name)
           ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: expected str, bytes or os.PathLike object, not Mock

During handling of the above exception, another exception occurred:
tests/conversion/test_convert_coreml.py:435: in test_main_contract_not_found
    main()
conversion/convert_coreml.py:655: in main
    convert_onnx_to_coreml(
conversion/convert_coreml.py:446: in convert_onnx_to_coreml
    raise RuntimeError(f"Failed to load ONNX model: {e}")
E   RuntimeError: Failed to load ONNX model: expected str, bytes or os.PathLike object, not Mock
----------------------------- Captured stdout call -----------------------------
[convert_coreml]   Skipping version check for toy model (testing mode)
[convert_coreml] Loading ONNX model: <Mock name='ArgumentParser().parse_args().input_path' id='6343873104'>
___________ TestANEOptimizations.test_detect_int64_tensors_fallback ____________
tests/conversion/test_convert_coreml.py:447: in test_detect_int64_tensors_fallback
    result = convert_coreml_module.detect_int64_tensors_on_attention_paths(mock_model)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/test_ane_optimizations.py:91: in detect_int64_tensors_on_attention_paths
    for node in graph_module.graph.nodes:
E   TypeError: 'Mock' object is not iterable
________ TestANEOptimizations.test_check_ane_op_compatibility_fallback _________
tests/conversion/test_convert_coreml.py:456: in test_check_ane_op_compatibility_fallback
    assert result["compatible"] == True
E   assert False == True
_______ TestCoreMLConversionIntegration.test_conversion_workflow_pytorch _______
tests/conversion/test_convert_coreml.py:507: in test_conversion_workflow_pytorch
    assert result == str(placeholder_path)
E   AssertionError: assert None == '/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_conversion_workflow_pytor0/placeholder.mlpackage'
E    +  where '/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_conversion_workflow_pytor0/placeholder.mlpackage' = str(PosixPath('/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_conversion_workflow_pytor0/placeholder.mlpackage'))
----------------------------- Captured stdout call -----------------------------
[convert_coreml] Placeholder created: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_conversion_workflow_pytor0/placeholder.mlpackage
__________ TestDecodeWrapper.test_decode_wrapper_forward_single_token __________
tests/conversion/test_export_onnx.py:160: in test_decode_wrapper_forward_single_token
    assert result[0].shape == (1, 1, 32000)  # [B=1, T=1, vocab_size]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert torch.Size([2, 1, 32000]) == (1, 1, 32000)
E     
E     At index 0 diff: 2 != 1
E     
E     Full diff:
E     + torch.Size([2, 1, 32000])
E     - (
E     -     1,...
E     
E     ...Full output truncated (3 lines hidden), use '-vv' to show
________ TestDecodeWrapper.test_decode_wrapper_forward_batch_processing ________
tests/conversion/test_export_onnx.py:180: in test_decode_wrapper_forward_batch_processing
    assert logits.shape[0] == batch_size  # Batch size preserved
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   assert 2 == 4
____________________ TestMainFunction.test_main_both_modes _____________________
tests/conversion/test_export_onnx.py:230: in test_main_both_modes
    main(config="test_config.json", mode="both")
conversion/export_onnx.py:111: in main
    for _ in range(cfg.n_layers):
             ^^^^^^^^^^^^^^^^^^^
E   TypeError: 'Mock' object cannot be interpreted as an integer
----------------------------- Captured stdout call -----------------------------
Exported artifacts/onnx/student_T128.onnx
Exported artifacts/onnx/student_T256.onnx
Exported artifacts/onnx/student_T512.onnx
____________________ TestMainFunction.test_main_decode_only ____________________
tests/conversion/test_export_onnx.py:305: in test_main_decode_only
    main(config="test_config.json", mode="decode")
conversion/export_onnx.py:111: in main
    for _ in range(cfg.n_layers):
             ^^^^^^^^^^^^^^^^^^^
E   TypeError: 'Mock' object cannot be interpreted as an integer
_________________ TestMainFunction.test_main_config_not_found __________________
tests/conversion/test_export_onnx.py:318: in test_main_config_not_found
    main(config="nonexistent.json", mode="both")
conversion/export_onnx.py:72: in main
    model = StudentLM(ModelCfg())
            ^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:287: in __init__
    self.blocks = nn.ModuleList([Block(self.cfg) for _ in range(self.cfg.n_layers)])
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:287: in <listcomp>
    self.blocks = nn.ModuleList([Block(self.cfg) for _ in range(self.cfg.n_layers)])
                                 ^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:247: in __init__
    self.attn = MHA_GQA(
models/student/architectures/gqa_transformer.py:143: in __init__
    assert n_heads % n_kv_heads == 0, "GQA requires head groups"
           ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: GQA requires head groups
__________________ TestMainFunction.test_main_invalid_config ___________________
tests/conversion/test_export_onnx.py:328: in test_main_invalid_config
    main(config="invalid.json", mode="both")
conversion/export_onnx.py:72: in main
    model = StudentLM(ModelCfg())
            ^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:287: in __init__
    self.blocks = nn.ModuleList([Block(self.cfg) for _ in range(self.cfg.n_layers)])
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:287: in <listcomp>
    self.blocks = nn.ModuleList([Block(self.cfg) for _ in range(self.cfg.n_layers)])
                                 ^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:247: in __init__
    self.attn = MHA_GQA(
models/student/architectures/gqa_transformer.py:143: in __init__
    assert n_heads % n_kv_heads == 0, "GQA requires head groups"
           ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: GQA requires head groups
__________________ TestMainFunction.test_main_fallback_config __________________
tests/conversion/test_export_onnx.py:339: in test_main_fallback_config
    main(config="failing_config.json", mode="prefill")
conversion/export_onnx.py:66: in main
    shape_data = json.load(open(config, "r"))
                           ^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1124: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1128: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1183: in _execute_mock_call
    raise effect
E   Exception: File operation failed
_______________ TestMainFunction.test_main_model_loading_failure _______________
tests/conversion/test_export_onnx.py:368: in test_main_model_loading_failure
    with pytest.raises(SystemExit):
E   Failed: DID NOT RAISE <class 'SystemExit'>
----------------------------- Captured stdout call -----------------------------
Exported artifacts/onnx/student_T128.onnx
________________ TestMainFunction.test_main_onnx_export_failure ________________
tests/conversion/test_export_onnx.py:403: in test_main_onnx_export_failure
    main(config="test_config.json", mode="prefill")
conversion/export_onnx.py:85: in main
    torch.onnx.export(
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1124: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1128: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1183: in _execute_mock_call
    raise effect
E   Exception: ONNX export failed
___________ TestONNXExportIntegration.test_config_parsing_edge_cases ___________
tests/conversion/test_export_onnx.py:442: in test_config_parsing_edge_cases
    main(config="empty_config.json", mode="prefill")
conversion/export_onnx.py:72: in main
    model = StudentLM(ModelCfg())
            ^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:287: in __init__
    self.blocks = nn.ModuleList([Block(self.cfg) for _ in range(self.cfg.n_layers)])
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:287: in <listcomp>
    self.blocks = nn.ModuleList([Block(self.cfg) for _ in range(self.cfg.n_layers)])
                                 ^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:247: in __init__
    self.attn = MHA_GQA(
models/student/architectures/gqa_transformer.py:143: in __init__
    assert n_heads % n_kv_heads == 0, "GQA requires head groups"
           ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: GQA requires head groups
______________ TestONNXExportIntegration.test_export_file_naming _______________
tests/conversion/test_export_onnx.py:498: in test_export_file_naming
    assert "256" in str(output_path)
E   AssertionError: assert '256' in '(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32), None)'
E    +  where '(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32), None)' = str((tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32), None))
----------------------------- Captured stdout call -----------------------------
Exported artifacts/onnx/student_T256.onnx
_____ TestONNXExportIntegration.test_checkpoint_loading_and_model_creation _____
tests/conversion/test_export_onnx.py:570: in test_checkpoint_loading_and_model_creation
    mock_load.assert_called()
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:908: in assert_called
    raise AssertionError(msg)
E   AssertionError: Expected 'load' to have been called.
----------------------------- Captured stdout call -----------------------------
Exported artifacts/onnx/student_T128.onnx
_________ TestPrefillWrapper.test_prefill_wrapper_forward_without_halt _________
tests/conversion/test_export_pytorch.py:66: in test_prefill_wrapper_forward_without_halt
    assert result[0].shape == (2, 10, 32000)
E   AssertionError: assert <Mock name='mock().shape' id='6347961872'> == (2, 10, 32000)
E    +  where <Mock name='mock().shape' id='6347961872'> = <Mock name='mock()' id='6347963216'>.shape
__________ TestPrefillWrapper.test_prefill_wrapper_forward_with_halt ___________
tests/conversion/test_export_pytorch.py:80: in test_prefill_wrapper_forward_with_halt
    assert len(result) == 2
E   AssertionError: assert 1 == 2
E    +  where 1 = len((<Mock name='mock()' id='6347337744'>,))
__________ TestDecodeWrapper.test_decode_wrapper_forward_single_token __________
tests/conversion/test_export_pytorch.py:125: in test_decode_wrapper_forward_single_token
    result = wrapper(input_ids)
             ^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
conversion/export_pytorch.py:55: in forward
    for i in range(n_layers):
             ^^^^^^^^^^^^^^^
E   TypeError: 'Mock' object cannot be interpreted as an integer
_________ TestDecodeWrapper.test_decode_wrapper_forward_with_kv_cache __________
tests/conversion/test_export_pytorch.py:142: in test_decode_wrapper_forward_with_kv_cache
    result = wrapper(input_ids, kv_cache_1, kv_cache_2)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
conversion/export_pytorch.py:55: in forward
    for i in range(n_layers):
             ^^^^^^^^^^^^^^^
E   TypeError: 'Mock' object cannot be interpreted as an integer
________________ TestExportPrefill.test_export_prefill_success _________________
tests/conversion/test_export_pytorch.py:169: in test_export_prefill_success
    result = export_prefill(
E   TypeError: export_prefill() got an unexpected keyword argument 'seq_length'
_____________ TestExportPrefill.test_export_prefill_with_halt_head _____________
tests/conversion/test_export_pytorch.py:211: in test_export_prefill_with_halt_head
    result = export_prefill(
E   TypeError: export_prefill() got an unexpected keyword argument 'seq_length'
_________________ TestExportDecode.test_export_decode_success __________________
tests/conversion/test_export_pytorch.py:241: in test_export_decode_success
    result = export_decode(
conversion/export_pytorch.py:204: in export_decode
    json.dump(contract, f, indent=2)
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:179: in dump
    for chunk in iterable:
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:432: in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:406: in _iterencode_dict
    yield from chunks
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:439: in _iterencode
    o = _default(o)
        ^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:180: in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
E   TypeError: Object of type Mock is not JSON serializable
----------------------------- Captured stdout call -----------------------------
[export_pytorch] Saved decode model: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_export_decode_success0/decode.pt
____________ TestExportDecode.test_export_decode_different_configs _____________
tests/conversion/test_export_pytorch.py:290: in test_export_decode_different_configs
    result = export_decode(
conversion/export_pytorch.py:204: in export_decode
    json.dump(contract, f, indent=2)
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:179: in dump
    for chunk in iterable:
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:432: in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:406: in _iterencode_dict
    yield from chunks
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:439: in _iterencode
    o = _default(o)
        ^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:180: in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
E   TypeError: Object of type Mock is not JSON serializable
----------------------------- Captured stdout call -----------------------------
[export_pytorch] Saved decode model: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_export_decode_different_c0/decode_6_6_128.pt
______________________ TestMainFunction.test_main_success ______________________
tests/conversion/test_export_pytorch.py:358: in test_main_success
    mock_export_prefill.assert_called_once()
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:918: in assert_called_once
    raise AssertionError(msg)
E   AssertionError: Expected 'export_prefill' to have been called once. Called 0 times.
----------------------------- Captured stdout call -----------------------------
[export_pytorch]   Skipping version check for toy model (testing mode)
[export_pytorch]  Export complete: <Mock name='Path()' id='6347579728'>
_____________ TestMainFunction.test_main_checkpoint_without_config _____________
tests/conversion/test_export_pytorch.py:400: in test_main_checkpoint_without_config
    main()
conversion/export_pytorch.py:257: in main
    raise ValueError(
E   ValueError: Checkpoint model.pt missing 'config' field. Cannot determine model architecture. Please use a checkpoint saved with config.
----------------------------- Captured stdout call -----------------------------
[export_pytorch]   Skipping version check for toy model (testing mode)
______ TestModelConfigurations.test_prefill_wrapper_different_batch_sizes ______
tests/conversion/test_export_pytorch.py:449: in test_prefill_wrapper_different_batch_sizes
    assert result[0].shape == (batch_size, seq_length, 32000)
E   AssertionError: assert <Mock name='mock().shape' id='6344525008'> == (1, 128, 32000)
E    +  where <Mock name='mock().shape' id='6344525008'> = <Mock name='mock()' id='6344531792'>.shape
_______ TestModelConfigurations.test_decode_wrapper_different_kv_configs _______
tests/conversion/test_export_pytorch.py:475: in test_decode_wrapper_different_kv_configs
    result = wrapper(input_ids, *kv_caches)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
conversion/export_pytorch.py:55: in forward
    for i in range(n_layers):
             ^^^^^^^^^^^^^^^
E   TypeError: 'Mock' object cannot be interpreted as an integer
____________ TestExportEdgeCases.test_export_prefill_minimal_config ____________
tests/conversion/test_export_pytorch.py:492: in test_export_prefill_minimal_config
    result = export_prefill(
E   TypeError: export_prefill() got an unexpected keyword argument 'seq_length'
____________ TestExportEdgeCases.test_export_decode_minimal_config _____________
tests/conversion/test_export_pytorch.py:510: in test_export_decode_minimal_config
    result = export_decode(
conversion/export_pytorch.py:204: in export_decode
    json.dump(contract, f, indent=2)
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:179: in dump
    for chunk in iterable:
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:432: in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:406: in _iterencode_dict
    yield from chunks
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:439: in _iterencode
    o = _default(o)
        ^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:180: in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
E   TypeError: Object of type Mock is not JSON serializable
----------------------------- Captured stdout call -----------------------------
[export_pytorch] Saved decode model: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_export_decode_minimal_con0/minimal_decode.pt
____________ TestExportEdgeCases.test_wrapper_parameter_validation _____________
tests/conversion/test_export_pytorch.py:535: in test_wrapper_parameter_validation
    assert result[0].shape == (1, 10, 1000)
E   AssertionError: assert <Mock name='mock().shape' id='6343202128'> == (1, 10, 1000)
E    +  where <Mock name='mock().shape' id='6343202128'> = <Mock name='mock()' id='6343211664'>.shape
_____________ TestForceInputDtype.test_force_input_dtype_not_found _____________
tests/conversion/test_onnx_surgery.py:63: in test_force_input_dtype_not_found
    graph = onnx.helper.make_graph([node], ["input"], [output_tensor])
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: make_graph() missing 1 required positional argument: 'outputs'
______ TestStripRedundantCasts.test_strip_redundant_casts_redundant_cast _______
tests/conversion/test_onnx_surgery.py:205: in test_strip_redundant_casts_redundant_cast
    result = strip_redundant_casts(model)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
conversion/onnx_surgery.py:61: in strip_redundant_casts
    for input_info in model_with_shapes.graph.input:
E   TypeError: 'Mock' object is not iterable
______ TestStripRedundantCasts.test_strip_redundant_casts_with_cast_nodes ______
tests/conversion/test_onnx_surgery.py:260: in test_strip_redundant_casts_with_cast_nodes
    result = strip_redundant_casts(model)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
conversion/onnx_surgery.py:61: in strip_redundant_casts
    for input_info in model_with_shapes.graph.input:
E   TypeError: 'Mock' object is not iterable
_____ TestCastInt64Initializers.test_cast_int64_initializers_int64_present _____
tests/conversion/test_onnx_surgery.py:299: in test_cast_int64_initializers_int64_present
    np.array([1, 2, 3], dtype=np.int64),
    ^^
E   NameError: name 'np' is not defined
______ TestCastInt64Initializers.test_cast_int64_initializers_mixed_types ______
tests/conversion/test_onnx_surgery.py:326: in test_cast_int64_initializers_mixed_types
    np.array([1, 2, 3], dtype=np.int64),
    ^^
E   NameError: name 'np' is not defined
_______ TestCastInt64Initializers.test_cast_int64_initializers_no_int64 ________
tests/conversion/test_onnx_surgery.py:362: in test_cast_int64_initializers_no_int64
    np.array([1.0, 2.0], dtype=np.float32),
    ^^
E   NameError: name 'np' is not defined
___________ TestRunFunction.test_run_success_without_simplification ____________
tests/conversion/test_onnx_surgery.py:407: in test_run_success_without_simplification
    run("input.onnx", "output.onnx")
conversion/onnx_surgery.py:167: in run
    m = force_input_dtype(m, "input_ids", TensorProto.INT32)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
conversion/onnx_surgery.py:27: in force_input_dtype
    for i in model.graph.input:
E   TypeError: 'Mock' object is not iterable
_____________ TestRunFunction.test_run_success_with_simplification _____________
tests/conversion/test_onnx_surgery.py:437: in test_run_success_with_simplification
    run("input.onnx", "output.onnx")
conversion/onnx_surgery.py:167: in run
    m = force_input_dtype(m, "input_ids", TensorProto.INT32)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
conversion/onnx_surgery.py:27: in force_input_dtype
    for i in model.graph.input:
E   TypeError: 'Mock' object is not iterable
______________________ TestMainFunction.test_main_success ______________________
tests/conversion/test_onnx_surgery.py:483: in test_main_success
    main()
conversion/onnx_surgery.py:181: in main
    model = onnx.load(args.inp)
            ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:229: in load_model
    model = _get_serializer(format, f).deserialize_proto(_load_bytes(f), ModelProto())
                                                         ^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:166: in _load_bytes
    with open(f, "rb") as readable:
         ^^^^^^^^^^^^^
E   FileNotFoundError: [Errno 2] No such file or directory: 'input.onnx'
____________________ TestMainFunction.test_main_run_failure ____________________
tests/conversion/test_onnx_surgery.py:505: in test_main_run_failure
    main()
conversion/onnx_surgery.py:181: in main
    model = onnx.load(args.inp)
            ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:229: in load_model
    model = _get_serializer(format, f).deserialize_proto(_load_bytes(f), ModelProto())
                                                         ^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:166: in _load_bytes
    with open(f, "rb") as readable:
         ^^^^^^^^^^^^^
E   FileNotFoundError: [Errno 2] No such file or directory: 'input.onnx'
_______________ TestMainFunction.test_main_missing_required_args _______________
tests/conversion/test_onnx_surgery.py:520: in test_main_missing_required_args
    main()
conversion/onnx_surgery.py:181: in main
    model = onnx.load(args.inp)
            ^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:229: in load_model
    model = _get_serializer(format, f).deserialize_proto(_load_bytes(f), ModelProto())
                                                         ^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/onnx/__init__.py:166: in _load_bytes
    with open(f, "rb") as readable:
         ^^^^^^^^^^^^^
E   TypeError: expected str, bytes or os.PathLike object, not NoneType
__________ TestONNXSurgeryIntegration.test_complete_surgery_workflow ___________
tests/conversion/test_onnx_surgery.py:540: in test_complete_surgery_workflow
    np.array([1, 2, 3], dtype=np.int64),
    ^^
E   NameError: name 'np' is not defined
___________________________ test_8_ball_pipeline_e2e ___________________________
tests/e2e/test_8_ball_pipeline.py:79: in test_8_ball_pipeline_e2e
    assert dataset_path.exists(), "Dataset file not created"
E   AssertionError: Dataset file not created
E   assert False
E    +  where False = exists()
E    +    where exists = PosixPath('/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/8ball_e2e__4661b5_/8_ball_kd.jsonl').exists
----------------------------- Captured stdout call -----------------------------

 8-BALL E2E PIPELINE TEST 
============================================================

[Step 1] Generating 8-ball KD dataset...
_________________ TestTokenReductionE2E.test_efficiency_curves _________________
tests/e2e/test_token_reduction.py:180: in test_efficiency_curves
    assert len(curves["accuracy_vs_tokens"]["accuracy"]) == len(current_metrics)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'EfficiencyCurve' object is not subscriptable
_______________ TestCESMeasurement.test_ces_combined_milestones ________________
tests/e2e/test_toy_ces_measurement.py:267: in test_ces_combined_milestones
    assert combined_ces < baseline_ces, (
E   AssertionError: Combined CES 12617 should be < baseline 12555
E   assert 12617 < 12555
_____________________ test_toy_training_without_code_mode ______________________
tests/e2e/test_toy_code_mode.py:69: in test_toy_training_without_code_mode
    assert loss_dict["total"].requires_grad
E   assert False
E    +  where False = tensor(2.7730).requires_grad
___________________ test_toy_training_with_code_mode_enabled ___________________
tests/e2e/test_toy_code_mode.py:204: in test_toy_training_with_code_mode_enabled
    assert isinstance(code_mode_loss, torch.Tensor)
E   AssertionError: assert False
E    +  where False = isinstance(None, <class 'torch.Tensor'>)
E    +    where <class 'torch.Tensor'> = torch.Tensor
_____________________ test_toy_code_mode_with_span_targets _____________________
tests/e2e/test_toy_code_mode.py:427: in test_toy_code_mode_with_span_targets
    code_mode_loss = code_mode_loss_module(
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/losses.py:689: in forward
    eligibility_mask = self._compute_eligibility_mask(
training/losses.py:653: in _compute_eligibility_mask
    tool_count = meta.get("tool_count", 0)
                 ^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
_______________ test_toy_code_mode_weight_scheduler_integration ________________
tests/e2e/test_toy_code_mode.py:532: in test_toy_code_mode_weight_scheduler_integration
    code_mode_loss = code_mode_loss_module(
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/losses.py:689: in forward
    eligibility_mask = self._compute_eligibility_mask(
training/losses.py:653: in _compute_eligibility_mask
    tool_count = meta.get("tool_count", 0)
                 ^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
___________ TestCombinedMilestones.test_training_with_both_features ____________
tests/e2e/test_toy_combined_milestones.py:164: in test_training_with_both_features
    code_mode_loss = code_mode_loss_module(
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/losses.py:689: in forward
    eligibility_mask = self._compute_eligibility_mask(
training/losses.py:653: in _compute_eligibility_mask
    tool_count = meta.get("tool_count", 0)
                 ^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
___________ TestCombinedMilestones.test_code_mode_with_latent_spans ____________
tests/e2e/test_toy_combined_milestones.py:252: in test_code_mode_with_latent_spans
    assert "import" in training_text or "from" in training_text
E   AssertionError: assert ('import' in 'Process large dataset:\n\n<bot>\n<bot>\n<eot>\nStep 3: Upload results\n\nDone' or 'from' in 'Process large dataset:\n\n<bot>\n<bot>\n<eot>\nStep 3: Upload results\n\nDone')
_____________ TestCombinedMilestones.test_mixed_batch_eligibility ______________
tests/e2e/test_toy_combined_milestones.py:341: in test_mixed_batch_eligibility
    loss = code_mode_loss_module(
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/losses.py:689: in forward
    eligibility_mask = self._compute_eligibility_mask(
training/losses.py:653: in _compute_eligibility_mask
    tool_count = meta.get("tool_count", 0)
                 ^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
____________ TestCombinedMilestones.test_full_pipeline_integration _____________
tests/e2e/test_toy_combined_milestones.py:430: in test_full_pipeline_integration
    code_mode_loss = code_mode_loss_module(
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/losses.py:689: in forward
    eligibility_mask = self._compute_eligibility_mask(
training/losses.py:653: in _compute_eligibility_mask
    tool_count = meta.get("tool_count", 0)
                 ^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
____________________________ test_toy_pipeline_e2e _____________________________
tests/e2e/test_toy_pipeline.py:45: in test_toy_pipeline_e2e
    assert dataset_path.exists(), "Dataset file not created"
E   AssertionError: Dataset file not created
E   assert False
E    +  where False = exists()
E    +    where exists = PosixPath('/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/toy_e2e_2y_mtli3/toy_kd.jsonl').exists
----------------------------- Captured stdout call -----------------------------

[test_toy_pipeline] Step 1: Generating toy KD dataset...
_______________________ test_toy_pipeline_with_code_mode _______________________
tests/e2e/test_toy_pipeline.py:253: in test_toy_pipeline_with_code_mode
    assert dataset_path.exists(), "Dataset file not created"
E   AssertionError: Dataset file not created
E   assert False
E    +  where False = exists()
E    +    where exists = PosixPath('/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/toy_e2e_05uqvg5d/toy_kd.jsonl').exists
----------------------------- Captured stdout call -----------------------------

[test_toy_pipeline_code_mode] Step 1: Generating toy KD dataset...
______________________ test_toy_pipeline_with_latent_mode ______________________
tests/e2e/test_toy_pipeline.py:357: in test_toy_pipeline_with_latent_mode
    assert dataset_path.exists(), "Dataset file not created"
E   AssertionError: Dataset file not created
E   assert False
E    +  where False = exists()
E    +    where exists = PosixPath('/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/toy_e2e_kvf6j04e/toy_kd.jsonl').exists
----------------------------- Captured stdout call -----------------------------

[test_toy_pipeline_latent] Step 1: Generating toy KD dataset...
_____________________ test_toy_pipeline_with_both_features _____________________
tests/e2e/test_toy_pipeline.py:463: in test_toy_pipeline_with_both_features
    assert dataset_path.exists(), "Dataset file not created"
E   AssertionError: Dataset file not created
E   assert False
E    +  where False = exists()
E    +    where exists = PosixPath('/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/toy_e2e_z_3znl6v/toy_kd.jsonl').exists
----------------------------- Captured stdout call -----------------------------

[test_toy_pipeline_both] Step 1: Generating toy KD dataset...
_______________ TestEightBallConstants.test_id_to_answer_mapping _______________
tests/evaluation/test_8ball_eval.py:62: in test_id_to_answer_mapping
    assert ID_TO_ANSWER[210] == "Signs point to yes"
E   AssertionError: assert 'Reply hazy, try again' == 'Signs point to yes'
E     
E     - Signs point to yes
E     + Reply hazy, try again
_______________ TestDataClasses.test_prediction_result_creation ________________
tests/evaluation/test_8ball_eval.py:76: in test_prediction_result_creation
    result = PredictionResult(
E   TypeError: PredictionResult.__init__() got an unexpected keyword argument 'predicted_token'
_______________ TestDataClasses.test_evaluation_metrics_creation _______________
tests/evaluation/test_8ball_eval.py:92: in test_evaluation_metrics_creation
    metrics = EvaluationMetrics(
E   TypeError: EvaluationMetrics.__init__() got an unexpected keyword argument 'total_predictions'
___________ TestLoadEvalQuestions.test_load_eval_questions_json_file ___________
tests/evaluation/test_8ball_eval.py:123: in test_load_eval_questions_json_file
    result = load_eval_questions(json_file)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/8ball_eval.py:109: in load_eval_questions
    return data.get("questions", [])
           ^^^^^^^^
E   AttributeError: 'list' object has no attribute 'get'
___________ TestLoadEvalQuestions.test_load_eval_questions_text_file ___________
tests/evaluation/test_8ball_eval.py:139: in test_load_eval_questions_text_file
    result = load_eval_questions(text_file)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/8ball_eval.py:108: in load_eval_questions
    data = json.load(f)
           ^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:293: in load
    return loads(fp.read(),
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346: in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337: in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:355: in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
E   json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
_______ TestLoadEvalQuestions.test_load_eval_questions_nonexistent_file ________
tests/evaluation/test_8ball_eval.py:145: in test_load_eval_questions_nonexistent_file
    with pytest.raises(FileNotFoundError):
E   Failed: DID NOT RAISE <class 'FileNotFoundError'>
_________ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_success _________
tests/evaluation/test_8ball_eval.py:187: in test_evaluate_pytorch_model_success
    with patch('evaluation.eightball_eval.AutoTokenizer', return_value=mock_tokenizer), \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
_____ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_empty_questions _____
tests/evaluation/test_8ball_eval.py:206: in test_evaluate_pytorch_model_empty_questions
    with patch('evaluation.eightball_eval.AutoTokenizer'), \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
____ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_tokenizer_failure ____
tests/evaluation/test_8ball_eval.py:217: in test_evaluate_pytorch_model_tokenizer_failure
    with patch('evaluation.eightball_eval.AutoTokenizer', side_effect=Exception("Tokenizer error")):
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
__________ TestEvaluateCoreMLModel.test_evaluate_coreml_model_success __________
tests/evaluation/test_8ball_eval.py:242: in test_evaluate_coreml_model_success
    with patch('evaluation.eightball_eval.ctk.load') as mock_load, \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
______ TestEvaluateCoreMLModel.test_evaluate_coreml_model_file_not_found _______
tests/evaluation/test_8ball_eval.py:262: in test_evaluate_coreml_model_file_not_found
    with patch('evaluation.eightball_eval.ctk.load', side_effect=FileNotFoundError):
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
____ TestEvaluateOllamaModel.test_evaluate_ollama_model_subprocess_failure _____
tests/evaluation/test_8ball_eval.py:307: in test_evaluate_ollama_model_subprocess_failure
    with pytest.raises(Exception):
E   Failed: DID NOT RAISE <class 'Exception'>
----------------------------- Captured stdout call -----------------------------
Error evaluating question 'Test question': argument of type 'Mock' is not iterable
_______ TestEvaluateOllamaModel.test_evaluate_ollama_model_invalid_json ________
tests/evaluation/test_8ball_eval.py:323: in test_evaluate_ollama_model_invalid_json
    with pytest.raises(json.JSONDecodeError):
E   Failed: DID NOT RAISE <class 'json.decoder.JSONDecodeError'>
__________ TestComparePredictions.test_compare_predictions_identical ___________
tests/evaluation/test_8ball_eval.py:333: in test_compare_predictions_identical
    PredictionResult("Q1", 200, "It is certain", 0.9, True),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
__________ TestComparePredictions.test_compare_predictions_different ___________
tests/evaluation/test_8ball_eval.py:351: in test_compare_predictions_different
    PredictionResult("Q1", 200, "It is certain", 0.9, True),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
_________ TestComparePredictions.test_compare_predictions_empty_lists __________
tests/evaluation/test_8ball_eval.py:367: in test_compare_predictions_empty_lists
    metrics = compare_predictions([], [])
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/8ball_eval.py:308: in compare_predictions
    exact_match_rate=exact_matches / len(reference),
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   ZeroDivisionError: division by zero
______ TestComparePredictions.test_compare_predictions_different_lengths _______
tests/evaluation/test_8ball_eval.py:375: in test_compare_predictions_different_lengths
    predictions1 = [PredictionResult("Q1", 200, "It is certain", 0.9, True)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
______ TestComparePredictions.test_compare_predictions_token_distribution ______
tests/evaluation/test_8ball_eval.py:387: in test_compare_predictions_token_distribution
    PredictionResult("Q1", 200, "It is certain", 0.9, True),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
________________ TestMainFunction.test_main_pytorch_evaluation _________________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
_________________ TestMainFunction.test_main_coreml_evaluation _________________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
_________________ TestMainFunction.test_main_ollama_evaluation _________________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
__________________ TestMainFunction.test_main_invalid_backend __________________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
_________________ TestMainFunction.test_main_missing_eval_file _________________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
__________ TestEightBallIntegration.test_prediction_result_validation __________
tests/evaluation/test_8ball_eval.py:554: in test_prediction_result_validation
    valid_result = PredictionResult(
E   TypeError: PredictionResult.__init__() got an unexpected keyword argument 'predicted_token'
_________ TestEightBallIntegration.test_evaluation_metrics_calculation _________
tests/evaluation/test_8ball_eval.py:570: in test_evaluation_metrics_calculation
    PredictionResult("Q1", 200, "It is certain", 0.9, True),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
______________ TestEightBallIntegration.test_evaluation_workflow _______________
tests/evaluation/test_8ball_eval.py:606: in test_evaluation_workflow
    loaded_questions = load_eval_questions(questions_file)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/8ball_eval.py:109: in load_eval_questions
    return data.get("questions", [])
           ^^^^^^^^
E   AttributeError: 'list' object has no attribute 'get'
___ TestValidateBudgetAdherence.test_validate_budget_adherence_within_limits ___
tests/evaluation/test_caws_eval.py:50: in test_validate_budget_adherence_within_limits
    assert result["files_changed_count"] == 1
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
_ TestValidateBudgetAdherence.test_validate_budget_adherence_exceeds_loc_limit _
tests/evaluation/test_caws_eval.py:64: in test_validate_budget_adherence_exceeds_loc_limit
    assert result["lines_removed"] == 1
E   assert 0 == 1
_ TestValidateBudgetAdherence.test_validate_budget_adherence_exceeds_files_limit _
tests/evaluation/test_caws_eval.py:88: in test_validate_budget_adherence_exceeds_files_limit
    assert result["files_changed_count"] == 3
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
____ TestValidateBudgetAdherence.test_validate_budget_adherence_empty_diff _____
tests/evaluation/test_caws_eval.py:99: in test_validate_budget_adherence_empty_diff
    assert result["files_changed_count"] == 1  # Default when no files detected
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
__ TestValidateBudgetAdherence.test_validate_budget_adherence_multiple_files ___
tests/evaluation/test_caws_eval.py:123: in test_validate_budget_adherence_multiple_files
    assert result["files_changed_count"] == 2
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
___ TestValidateBudgetAdherence.test_validate_budget_adherence_binary_files ____
tests/evaluation/test_caws_eval.py:135: in test_validate_budget_adherence_binary_files
    assert result["files_changed_count"] == 1
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
____ TestValidateBudgetAdherence.test_validate_budget_adherence_edge_cases _____
tests/evaluation/test_caws_eval.py:153: in test_validate_budget_adherence_edge_cases
    assert result["lines_removed"] == 1
E   assert 0 == 1
_______ TestValidateGateIntegrity.test_validate_gate_integrity_all_pass ________
tests/evaluation/test_caws_eval.py:184: in test_validate_gate_integrity_all_pass
    assert result["tests_pass"] == True
E   assert False == True
______ TestValidateGateIntegrity.test_validate_gate_integrity_tests_fail _______
tests/evaluation/test_caws_eval.py:198: in test_validate_gate_integrity_tests_fail
    assert result["overall_integrity"] == False
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'overall_integrity'
_______ TestValidateGateIntegrity.test_validate_gate_integrity_lint_fail _______
tests/evaluation/test_caws_eval.py:208: in test_validate_gate_integrity_lint_fail
    assert result["lint_clean"] == False
           ^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'lint_clean'
_____ TestValidateGateIntegrity.test_validate_gate_integrity_coverage_fail _____
tests/evaluation/test_caws_eval.py:219: in test_validate_gate_integrity_coverage_fail
    assert result["coverage_sufficient"] == False
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'coverage_sufficient'
____ TestValidateGateIntegrity.test_validate_gate_integrity_missing_fields _____
tests/evaluation/test_caws_eval.py:231: in test_validate_gate_integrity_missing_fields
    assert "overall_integrity" in result
E   AssertionError: assert 'overall_integrity' in {'all_gates_pass': False, 'coverage_pass': False, 'lint_pass': False, 'tests_pass': False}
___ TestValidateProvenanceClarity.test_validate_provenance_clarity_complete ____
tests/evaluation/test_caws_eval.py:244: in test_validate_provenance_clarity_complete
    result = validate_provenance_clarity(rationale, evidence, diff_present)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
_ TestValidateProvenanceClarity.test_validate_provenance_clarity_missing_rationale _
tests/evaluation/test_caws_eval.py:257: in test_validate_provenance_clarity_missing_rationale
    result = validate_provenance_clarity(rationale, evidence, diff_present)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
_ TestValidateProvenanceClarity.test_validate_provenance_clarity_missing_evidence _
tests/evaluation/test_caws_eval.py:268: in test_validate_provenance_clarity_missing_evidence
    result = validate_provenance_clarity(rationale, evidence, diff_present)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
____ TestValidateProvenanceClarity.test_validate_provenance_clarity_no_diff ____
tests/evaluation/test_caws_eval.py:279: in test_validate_provenance_clarity_no_diff
    result = validate_provenance_clarity(rationale, evidence, diff_present)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
_ TestValidateProvenanceClarity.test_validate_provenance_clarity_whitespace_only _
tests/evaluation/test_caws_eval.py:290: in test_validate_provenance_clarity_whitespace_only
    result = validate_provenance_clarity(rationale, evidence, diff_present)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
______ TestEvaluateCawsCompliance.test_evaluate_caws_compliance_all_pass _______
tests/evaluation/test_caws_eval.py:310: in test_evaluate_caws_compliance_all_pass
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
_____ TestEvaluateCawsCompliance.test_evaluate_caws_compliance_gates_fail ______
tests/evaluation/test_caws_eval.py:336: in test_evaluate_caws_compliance_gates_fail
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
___ TestEvaluateCawsCompliance.test_evaluate_caws_compliance_provenance_fail ___
tests/evaluation/test_caws_eval.py:357: in test_evaluate_caws_compliance_provenance_fail
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
_____ TestEvaluateCawsCompliance.test_evaluate_caws_compliance_budget_fail _____
tests/evaluation/test_caws_eval.py:378: in test_evaluate_caws_compliance_budget_fail
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
__ TestEvaluateCawsCompliance.test_evaluate_caws_compliance_multiple_failures __
tests/evaluation/test_caws_eval.py:401: in test_evaluate_caws_compliance_multiple_failures
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
_____________ TestHelperFunctions.test_load_working_spec_not_found _____________
tests/evaluation/test_caws_eval.py:437: in test_load_working_spec_not_found
    result = _load_working_spec("nonexistent.yaml")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:142: in _load_working_spec
    raise FileNotFoundError(f"Working spec not found: {spec_path}")
E   FileNotFoundError: Working spec not found: nonexistent.yaml
_____________ TestHelperFunctions.test_load_json_file_invalid_json _____________
tests/evaluation/test_caws_eval.py:477: in test_load_json_file_invalid_json
    result = _load_json_file(str(json_file))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:173: in _load_json_file
    return json.load(f)
           ^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:293: in load
    return loads(fp.read(),
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346: in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337: in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:355: in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
E   json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
__________________ TestHelperFunctions.test_run_tests_success __________________
tests/evaluation/test_caws_eval.py:491: in test_run_tests_success
    assert result["passed"] == 150
           ^^^^^^^^^^^^^^^^
E   KeyError: 'passed'
__________________ TestHelperFunctions.test_run_tests_failure __________________
tests/evaluation/test_caws_eval.py:505: in test_run_tests_failure
    assert result["passed"] == 140
           ^^^^^^^^^^^^^^^^
E   KeyError: 'passed'
_________________ TestHelperFunctions.test_run_linter_success __________________
tests/evaluation/test_caws_eval.py:516: in test_run_linter_success
    result = _run_linter()
             ^^^^^^^^^^^^^
evaluation/caws_eval.py:213: in _run_linter
    "output": result.stdout + result.stderr,
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: can only concatenate str (not "Mock") to str
________________ TestHelperFunctions.test_run_coverage_success _________________
tests/evaluation/test_caws_eval.py:531: in test_run_coverage_success
    assert result["line_percent"] == 85.5
           ^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'line_percent'
______________________ TestMainFunction.test_main_success ______________________
tests/evaluation/test_caws_eval.py:570: in test_main_success
    main(
E   TypeError: main() got an unexpected keyword argument 'evidence'
___________________ TestMainFunction.test_main_missing_spec ____________________
tests/evaluation/test_caws_eval.py:588: in test_main_missing_spec
    main(
E   TypeError: main() got an unexpected keyword argument 'evidence'
___________________ TestMainFunction.test_main_test_failure ____________________
tests/evaluation/test_caws_eval.py:603: in test_main_test_failure
    main(
E   TypeError: main() got an unexpected keyword argument 'evidence'
________ TestCawsEvalIntegration.test_complete_caws_evaluation_workflow ________
tests/evaluation/test_caws_eval.py:628: in test_complete_caws_evaluation_workflow
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
_________ TestCawsEvalIntegration.test_caws_evaluation_with_violations _________
tests/evaluation/test_caws_eval.py:655: in test_caws_evaluation_with_violations
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
___________ TestCawsEvalIntegration.test_budget_adherence_edge_cases ___________
tests/evaluation/test_caws_eval.py:684: in test_budget_adherence_edge_cases
    assert result["files_changed_count"] == 1
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
____________ TestCawsEvalIntegration.test_gate_integrity_thresholds ____________
tests/evaluation/test_caws_eval.py:703: in test_gate_integrity_thresholds
    assert result["overall_integrity"] == True
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'overall_integrity'
__________ TestCawsEvalIntegration.test_provenance_clarity_validation __________
tests/evaluation/test_caws_eval.py:713: in test_provenance_clarity_validation
    result = validate_provenance_clarity(
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
______________ TestClaimExtractionEvaluator.test_evaluate_success ______________
tests/evaluation/test_claim_extraction_metrics.py:155: in test_evaluate_success
    assert result.claim_ratio == 1.33
E   assert 1.3333333333333333 == 1.33
E    +  where 1.3333333333333333 = ClaimExtractionEvalResult(student_claim_count=4, teacher_claim_count=3, student_success_rate=0.75, teacher_success_rate=0.85, claim_ratio=1.3333333333333333, success_rate_ratio=0.8823529411764706, claim_extraction_loss=0.0).claim_ratio
________ TestClaimExtractionEvaluator.test_evaluate_mismatched_lengths _________
tests/evaluation/test_claim_extraction_metrics.py:221: in test_evaluate_mismatched_lengths
    result = evaluator.evaluate(student_outputs, teacher_outputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/claim_extraction_metrics.py:68: in evaluate
    raise ValueError(
E   ValueError: Student and teacher outputs must have same length: 3 != 2
___ TestClaimExtractionMetricsIntegration.test_complete_evaluation_workflow ____
tests/evaluation/test_claim_extraction_metrics.py:305: in test_complete_evaluation_workflow
    assert result.claim_ratio == 1.33
E   assert 1.3333333333333333 == 1.33
E    +  where 1.3333333333333333 = ClaimExtractionEvalResult(student_claim_count=8, teacher_claim_count=6, student_success_rate=0.75, teacher_success_rate=0.83, claim_ratio=1.3333333333333333, success_rate_ratio=0.9036144578313253, claim_extraction_loss=0.0).claim_ratio
__ TestClaimExtractionMetricsIntegration.test_metrics_calculation_edge_cases ___
tests/evaluation/test_claim_extraction_metrics.py:396: in test_metrics_calculation_edge_cases
    assert abs(result.claim_extraction_loss - expected_metrics['claim_extraction_loss']) < 0.001
E   assert 1.0 < 0.001
E    +  where 1.0 = abs((1.0 - 0.0))
E    +    where 1.0 = ClaimExtractionEvalResult(student_claim_count=0, teacher_claim_count=0, student_success_rate=0.0, teacher_success_rate=0.0, claim_ratio=0.0, success_rate_ratio=0.0, claim_extraction_loss=1.0).claim_extraction_loss
____________ TestEvaluationMetrics.test_evaluation_metrics_creation ____________
tests/evaluation/test_classification_eval.py:108: in test_evaluation_metrics_creation
    metrics = EvaluationMetrics(
E   TypeError: EvaluationMetrics.__init__() got an unexpected keyword argument 'class_distribution'
____________ TestEvaluationMetrics.test_evaluation_metrics_minimal _____________
tests/evaluation/test_classification_eval.py:132: in test_evaluation_metrics_minimal
    assert metrics.class_distribution is None
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'EvaluationMetrics' object has no attribute 'class_distribution'
_____ TestLoadClassificationConfig.test_load_classification_config_success _____
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_load_classification_confi0/config

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:151: in test_load_classification_config_success
    result = load_classification_config(str(config_file))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_load_classification_confi0/config.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_load_classification_confi0/config
_ TestLoadClassificationConfig.test_load_classification_config_file_not_found __
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: nonexistent

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:165: in test_load_classification_config_file_not_found
    load_classification_config("nonexistent.json")
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from nonexistent.json: Cannot find module: nonexistent
__ TestLoadClassificationConfig.test_load_classification_config_invalid_json ___
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_load_classification_confi1/invalid

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:174: in test_load_classification_config_invalid_json
    load_classification_config(str(config_file))
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_load_classification_confi1/invalid.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_load_classification_confi1/invalid
_ TestLoadClassificationConfig.test_load_classification_config_missing_fields __
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_load_classification_confi2/incomplete

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:185: in test_load_classification_config_missing_fields
    load_classification_config(str(config_file))
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_load_classification_confi2/incomplete.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_load_classification_confi2/incomplete
_________ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_success _________
tests/evaluation/test_classification_eval.py:225: in test_evaluate_pytorch_model_success
    with patch('evaluation.classification_eval.AutoTokenizer', return_value=mock_tokenizer), \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.classification_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/classification_eval.py'> does not have the attribute 'AutoTokenizer'
_____ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_empty_questions _____
tests/evaluation/test_classification_eval.py:243: in test_evaluate_pytorch_model_empty_questions
    with patch('evaluation.classification_eval.AutoTokenizer'), \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.classification_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/classification_eval.py'> does not have the attribute 'AutoTokenizer'
____ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_tokenizer_failure ____
tests/evaluation/test_classification_eval.py:254: in test_evaluate_pytorch_model_tokenizer_failure
    with patch('evaluation.classification_eval.AutoTokenizer', side_effect=Exception("Tokenizer error")):
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.classification_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/classification_eval.py'> does not have the attribute 'AutoTokenizer'
__________ TestEvaluateCoreMLModel.test_evaluate_coreml_model_success __________
tests/evaluation/test_classification_eval.py:283: in test_evaluate_coreml_model_success
    with patch('evaluation.classification_eval.ctk.load') as mock_load, \
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'ctk'
______ TestEvaluateCoreMLModel.test_evaluate_coreml_model_file_not_found _______
tests/evaluation/test_classification_eval.py:307: in test_evaluate_coreml_model_file_not_found
    with patch('evaluation.classification_eval.ctk.load', side_effect=FileNotFoundError):
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'ctk'
__________ TestEvaluateOllamaModel.test_evaluate_ollama_model_success __________
tests/evaluation/test_classification_eval.py:353: in test_evaluate_ollama_model_success
    assert results[1].predicted_class_name == "No"
E   AssertionError: assert 'Yes' == 'No'
E     
E     - No
E     + Yes
____ TestEvaluateOllamaModel.test_evaluate_ollama_model_subprocess_failure _____
tests/evaluation/test_classification_eval.py:365: in test_evaluate_ollama_model_subprocess_failure
    with pytest.raises(Exception):
E   Failed: DID NOT RAISE <class 'Exception'>
----------------------------- Captured stdout call -----------------------------
Error evaluating question 'Test question': argument of type 'Mock' is not iterable
_______ TestEvaluateOllamaModel.test_evaluate_ollama_model_invalid_json ________
tests/evaluation/test_classification_eval.py:381: in test_evaluate_ollama_model_invalid_json
    with pytest.raises(json.JSONDecodeError):
E   Failed: DID NOT RAISE <class 'json.decoder.JSONDecodeError'>
__________ TestComparePredictions.test_compare_predictions_identical ___________
tests/evaluation/test_classification_eval.py:410: in test_compare_predictions_identical
    metrics = compare_predictions(predictions1, predictions2, mock_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
__________ TestComparePredictions.test_compare_predictions_different ___________
tests/evaluation/test_classification_eval.py:428: in test_compare_predictions_different
    metrics = compare_predictions(predictions1, predictions2, mock_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
_________ TestComparePredictions.test_compare_predictions_empty_lists __________
tests/evaluation/test_classification_eval.py:436: in test_compare_predictions_empty_lists
    metrics = compare_predictions([], [], mock_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
______ TestComparePredictions.test_compare_predictions_different_lengths _______
tests/evaluation/test_classification_eval.py:450: in test_compare_predictions_different_lengths
    compare_predictions(predictions1, predictions2, mock_config)
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
______ TestComparePredictions.test_compare_predictions_with_probabilities ______
tests/evaluation/test_classification_eval.py:463: in test_compare_predictions_with_probabilities
    metrics = compare_predictions(predictions1, predictions2, mock_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
____ TestComparePredictions.test_compare_predictions_without_probabilities _____
tests/evaluation/test_classification_eval.py:480: in test_compare_predictions_without_probabilities
    metrics = compare_predictions(predictions1, predictions2, mock_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
________________ TestMainFunction.test_main_pytorch_evaluation _________________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
_________________ TestMainFunction.test_main_coreml_evaluation _________________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
__________________ TestMainFunction.test_main_invalid_backend __________________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
_________________ TestMainFunction.test_main_config_not_found __________________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
_____ TestClassificationEvalIntegration.test_complete_evaluation_workflow ______
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_complete_evaluation_workf0/test_config

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:604: in test_complete_evaluation_workflow
    config = load_classification_config(str(config_file))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_complete_evaluation_workf0/test_config.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_complete_evaluation_workf0/test_config
____ TestClassificationEvalIntegration.test_evaluation_metrics_calculation _____
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_evaluation_metrics_calcul0/metrics_config

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:634: in test_evaluation_metrics_calculation
    config = load_classification_config(str(config_file))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_evaluation_metrics_calcul0/metrics_config.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_evaluation_metrics_calcul0/metrics_config
__________________ TestStepAdapter.test_step_adapter_creation __________________
tests/evaluation/test_perf_mem_eval.py:65: in test_step_adapter_creation
    adapter = StepAdapter(
E   TypeError: StepAdapter() takes no arguments
__________________ TestStepAdapter.test_step_adapter_defaults __________________
tests/evaluation/test_perf_mem_eval.py:81: in test_step_adapter_defaults
    adapter = StepAdapter(
E   TypeError: StepAdapter() takes no arguments
________________ TestDetectHardware.test_detect_hardware_macos _________________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.perf_mem_eval' has no attribute 'platform'
______________ TestDetectHardware.test_detect_hardware_non_macos _______________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.perf_mem_eval' has no attribute 'platform'
____________ TestDetectHardware.test_detect_hardware_no_coremltools ____________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.perf_mem_eval' has no attribute 'platform'
_______________ TestGreedyArgmax.test_greedy_argmax_empty_array ________________
tests/evaluation/test_perf_mem_eval.py:176: in test_greedy_argmax_empty_array
    result = greedy_argmax(logits)
             ^^^^^^^^^^^^^^^^^^^^^
evaluation/perf_mem_eval.py:129: in greedy_argmax
    return int(np.argmax(logits))
               ^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:1229: in argmax
    return _wrapfunc(a, 'argmax', axis=axis, out=out, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: in _wrapfunc
    return bound(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^
E   ValueError: attempt to get argmax of an empty sequence
_____________ TestGreedyArgmax.test_greedy_argmax_negative_values ______________
tests/evaluation/test_perf_mem_eval.py:185: in test_greedy_argmax_negative_values
    assert result == 1  # Index of least negative (highest) value
    ^^^^^^^^^^^^^^^^^^
E   assert 3 == 1
__________ TestIsValidToolJSON.test_is_valid_tool_json_invalid_syntax __________
tests/evaluation/test_perf_mem_eval.py:228: in test_is_valid_tool_json_invalid_syntax
    assert result == False, f"Should reject invalid JSON: {invalid_json}"
E   AssertionError: Should reject invalid JSON: {"name": "calculator", "arguments": }
E   assert True == False
__________ TestIsValidToolJSON.test_is_valid_tool_json_missing_fields __________
tests/evaluation/test_perf_mem_eval.py:234: in test_is_valid_tool_json_missing_fields
    assert result == False
E   assert True == False
_______________ TestRunCoreMLSpeed.test_run_coreml_speed_success _______________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.perf_mem_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/perf_mem_eval.py'> does not have the attribute 'coremltools'
_________ TestRunCoreMLSpeed.test_run_coreml_speed_model_load_failure __________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.perf_mem_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/perf_mem_eval.py'> does not have the attribute 'coremltools'
___________ TestRunCoreMLSpeed.test_run_coreml_speed_no_coremltools ____________
tests/evaluation/test_perf_mem_eval.py:306: in test_run_coreml_speed_no_coremltools
    run_coreml_speed("model.mlpackage", tokenized_prompts)
E   TypeError: run_coreml_speed() missing 1 required positional argument: 'adapter'
_________ TestLoadTokenizedPrompts.test_load_tokenized_prompts_success _________
tests/evaluation/test_perf_mem_eval.py:328: in test_load_tokenized_prompts_success
    result = load_tokenized_prompts(str(json_file))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
_____ TestLoadTokenizedPrompts.test_load_tokenized_prompts_file_not_found ______
tests/evaluation/test_perf_mem_eval.py:341: in test_load_tokenized_prompts_file_not_found
    load_tokenized_prompts("nonexistent.json")
E   TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
______ TestLoadTokenizedPrompts.test_load_tokenized_prompts_invalid_json _______
tests/evaluation/test_perf_mem_eval.py:350: in test_load_tokenized_prompts_invalid_json
    load_tokenized_prompts(str(json_file))
E   TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
__ TestLoadTokenizedPrompts.test_load_tokenized_prompts_missing_tokens_field ___
tests/evaluation/test_perf_mem_eval.py:365: in test_load_tokenized_prompts_missing_tokens_field
    load_tokenized_prompts(str(json_file))
E   TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
______________________ TestMainFunction.test_main_success ______________________
tests/evaluation/test_perf_mem_eval.py:409: in test_main_success
    main()
evaluation/perf_mem_eval.py:953: in main
    with args.out.open("w") as f:
E   TypeError: 'Mock' object does not support the context manager protocol
_______________ TestMainFunction.test_main_missing_required_args _______________
tests/evaluation/test_perf_mem_eval.py:431: in test_main_missing_required_args
    main()
evaluation/perf_mem_eval.py:656: in main
    prompts = load_tokenized_prompts(
evaluation/perf_mem_eval.py:468: in load_tokenized_prompts
    with open(dataset_path, "r", encoding="utf-8") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: expected str, bytes or os.PathLike object, not Mock
----------------------------- Captured stdout call -----------------------------
[perf_mem_eval] WARN: Failed to initialize prompt cache: unsupported operand type(s) for *: 'Mock' and 'int'
____________ TestPerfMemEvalIntegration.test_tokenization_workflow _____________
tests/evaluation/test_perf_mem_eval.py:463: in test_tokenization_workflow
    prompts = load_tokenized_prompts(str(json_file))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
________ TestPerfMemEvalIntegration.test_json_validation_comprehensive _________
tests/evaluation/test_perf_mem_eval.py:513: in test_json_validation_comprehensive
    assert is_valid_tool_json(pattern) == False
E   assert True == False
E    +  where True = is_valid_tool_json('{"name": "calculator"}')
________ TestPerfMemEvalIntegration.test_end_to_end_workflow_simulation ________
tests/evaluation/test_perf_mem_eval.py:579: in test_end_to_end_workflow_simulation
    main()
evaluation/perf_mem_eval.py:953: in main
    with args.out.open("w") as f:
E   TypeError: 'Mock' object does not support the context manager protocol
----------------------------- Captured stdout call -----------------------------
[perf_mem_eval] WARN: Failed to initialize prompt cache: unsupported operand type(s) for *: 'Mock' and 'int'
[perf_mem_eval] WARN: Failed to initialize optimized KV cache: '>' not supported between instances of 'Mock' and 'int'
[perf_mem_eval] WARN: Batch size <Mock name='ArgumentParser().parse_args().batch_size' id='6346737552'> not allowed: Batch size <Mock name='ArgumentParser().parse_args().batch_size' id='6346737552'> not in allowed list [2, 4]
[perf_mem_eval] Batch policy: workload_type=<Mock name='ArgumentParser().parse_args().workload_type' id='6346749392'>, batch_size=<Mock name='ArgumentParser().parse_args().batch_size' id='6346737552'>
[perf_mem_eval] WARN: Failed to initialize speculative decoder: [Errno 2] No such file or directory: "/Users/darianrosebrook/Desktop/Projects/distill/<Mock name='ArgumentParser().parse_args().drafter_model' id='6346737936'>"
[perf_mem_eval] WARN: Failed to measure ANE residency: [Errno 2] No such file or directory: "/Users/darianrosebrook/Desktop/Projects/distill/<Mock name='ArgumentParser().parse_args().model' id='6343272016'>"
__________________ TestLoadModel.test_load_model_with_config ___________________
tests/evaluation/test_tool_use_eval.py:74: in test_load_model_with_config
    mock_model.load_state_dict.assert_called_once_with(mock_checkpoint['model_state_dict'])
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:951: in assert_called_once_with
    return self.assert_called_with(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:939: in assert_called_with
    raise AssertionError(_error_message()) from cause
E   AssertionError: expected call not found.
E   Expected: load_state_dict({'layer.weight': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])})
E     Actual: load_state_dict({'layer.weight': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])}, strict=False)
_________________ TestLoadModel.test_load_model_without_config _________________
tests/evaluation/test_tool_use_eval.py:102: in test_load_model_without_config
    assert result == mock_model
E   AssertionError: assert <Mock name='StudentLM().to()' id='6344402320'> == <Mock name='StudentLM()' id='6344738064'>
__________________ TestGenerateText.test_generate_text_basic ___________________
tests/evaluation/test_tool_use_eval.py:138: in test_generate_text_basic
    result = generate_text(mock_model, mock_tokenizer, prompt, max_length)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/tool_use_eval.py:65: in generate_text
    device = next(model.parameters()).device
             ^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'Mock' object is not an iterator
_________________ TestGenerateText.test_generate_text_with_eos _________________
tests/evaluation/test_tool_use_eval.py:155: in test_generate_text_with_eos
    result = generate_text(mock_model, mock_tokenizer, prompt, max_length=10)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: generate_text() got an unexpected keyword argument 'max_length'
_______________ TestGenerateText.test_generate_text_temperature ________________
tests/evaluation/test_tool_use_eval.py:164: in test_generate_text_temperature
    result = generate_text(mock_model, mock_tokenizer, prompt, temperature=temperature)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: generate_text() got an unexpected keyword argument 'temperature'
________________ TestGenerateText.test_generate_text_max_length ________________
tests/evaluation/test_tool_use_eval.py:175: in test_generate_text_max_length
    result = generate_text(mock_model, mock_tokenizer, prompt, max_length=max_length)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: generate_text() got an unexpected keyword argument 'max_length'
______________ TestEvaluateToolUse.test_evaluate_tool_use_success ______________
tests/evaluation/test_tool_use_eval.py:393: in test_evaluate_tool_use_success
    results = evaluate_tool_use('dummy_checkpoint.pt', test_cases, device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
___________ TestEvaluateToolUse.test_evaluate_tool_use_invalid_json ____________
tests/evaluation/test_tool_use_eval.py:427: in test_evaluate_tool_use_invalid_json
    results = evaluate_tool_use('dummy_checkpoint.pt', test_cases, device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
____________ TestEvaluateToolUse.test_evaluate_tool_use_wrong_tool _____________
tests/evaluation/test_tool_use_eval.py:451: in test_evaluate_tool_use_wrong_tool
    results = evaluate_tool_use('dummy_checkpoint.pt', test_cases, device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
_________ TestEvaluateToolUse.test_evaluate_tool_use_empty_test_cases __________
tests/evaluation/test_tool_use_eval.py:474: in test_evaluate_tool_use_empty_test_cases
    results = evaluate_tool_use('dummy.pt', [], device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
______________________ TestMainFunction.test_main_success ______________________
tests/evaluation/test_tool_use_eval.py:504: in test_main_success
    main()
evaluation/tool_use_eval.py:344: in main
    model = load_model(args.checkpoint, device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/tool_use_eval.py:26: in load_model
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/serialization.py:998: in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/serialization.py:445: in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/serialization.py:426: in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
E   FileNotFoundError: [Errno 2] No such file or directory: 'model.pt'
_______________ TestMainFunction.test_main_checkpoint_not_found ________________
tests/evaluation/test_tool_use_eval.py:524: in test_main_checkpoint_not_found
    main()
evaluation/tool_use_eval.py:344: in main
    model = load_model(args.checkpoint, device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/tool_use_eval.py:26: in load_model
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/serialization.py:998: in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/serialization.py:445: in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/serialization.py:426: in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
E   FileNotFoundError: [Errno 2] No such file or directory: 'nonexistent.pt'
----------------------------- Captured stdout call -----------------------------
[tool_use_eval] Loading model from: nonexistent.pt
_________________ TestMainFunction.test_main_config_not_found __________________
tests/evaluation/test_tool_use_eval.py:537: in test_main_config_not_found
    main()
evaluation/tool_use_eval.py:344: in main
    model = load_model(args.checkpoint, device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/tool_use_eval.py:26: in load_model
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/serialization.py:1040: in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/serialization.py:1237: in _legacy_load
    f_should_read_directly = _should_read_directly(f)
                             ^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/serialization.py:522: in _should_read_directly
    return f.fileno() >= 0
           ^^^^^^^^^^^^^^^
E   TypeError: '>=' not supported between instances of 'Mock' and 'int'
----------------------------- Captured stdout call -----------------------------
[tool_use_eval] Loading model from: <Mock name='ArgumentParser().parse_args().checkpoint' id='6351767312'>
_________ TestToolUseEvalIntegration.test_complete_evaluation_workflow _________
tests/evaluation/test_tool_use_eval.py:632: in test_complete_evaluation_workflow
    results = evaluate_tool_use(str(config_file), test_config, device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
___________________ test_batch_contains_process_step_targets ___________________
tests/integration/test_process_step_integration.py:213: in test_batch_contains_process_step_targets
    batch = next(iter(dataloader))
            ^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631: in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675: in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54: in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
training/dataset.py:479: in collate_kd_batch
    result["tool_name_ids"] = torch.stack(tool_name_ids_list)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: stack expects each tensor to be equal size, but got [17] at entry 0 and [9] at entry 1
----------------------------- Captured stdout call -----------------------------
[KDDataset] Loaded 2 samples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_batch_contains_process_st0/test_process_step.jsonl
_________________ test_process_supervision_loss_with_token_ids _________________
tests/integration/test_process_step_integration.py:232: in test_process_supervision_loss_with_token_ids
    tool_name_ids = torch.tensor(
E   ValueError: expected sequence of length 8 at dim 1 (got 9)
_________________ test_training_step_with_process_step_targets _________________
tests/integration/test_process_step_integration.py:324: in test_training_step_with_process_step_targets
    batch = next(iter(dataloader))
            ^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631: in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675: in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54: in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
training/dataset.py:479: in collate_kd_batch
    result["tool_name_ids"] = torch.stack(tool_name_ids_list)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: stack expects each tensor to be equal size, but got [17] at entry 0 and [9] at entry 1
----------------------------- Captured stdout call -----------------------------
[KDDataset] Loaded 2 samples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_training_step_with_proces0/test_process_step.jsonl
__ TestLatencyAwareLossesIntegration.test_length_aware_loss_in_training_step ___
tests/integration/test_speed_optimization_integration.py:155: in test_length_aware_loss_in_training_step
    loss, diags = length_aware_kd_loss(
E   TypeError: length_aware_kd_loss() got an unexpected keyword argument 'student_attn_mask'
___ TestLatencyAwareLossesIntegration.test_early_tool_loss_in_training_step ____
tests/integration/test_speed_optimization_integration.py:185: in test_early_tool_loss_in_training_step
    loss, diags = early_tool_call_loss(
E   TypeError: early_tool_call_loss() got an unexpected keyword argument 'logits'
____ TestLatencyAwareLossesIntegration.test_latency_losses_with_combined_kd ____
tests/integration/test_speed_optimization_integration.py:231: in test_latency_losses_with_combined_kd
    length_loss, _ = length_aware_kd_loss(
E   TypeError: length_aware_kd_loss() got an unexpected keyword argument 'student_attn_mask'
__________ TestSpeedMetricsIntegration.test_speed_metrics_aggregation __________
tests/integration/test_speed_optimization_integration.py:304: in test_speed_metrics_aggregation
    assert aggregated["ttft_ms"]["p95"] == 200.0
E   assert 195.0 == 200.0
__ TestTrainingStepWithSpeedOptimizations.test_training_step_with_length_loss __
tests/integration/test_speed_optimization_integration.py:355: in test_training_step_with_length_loss
    assert "length_kd" in loss_dict
E   AssertionError: assert 'length_kd' in {'ce_ground_truth': 7.136858940124512, 'ce_teacher': 7.1016340255737305, 'grad_norm': 0.5479307770729065, 'total': 2.84769868850708}
----------------------------- Captured stdout call -----------------------------
[distill_kd] Step 0 loss components: ce_ground_truth=7.1369, ce_teacher=7.1016, total=2.8477
_ TestTrainingStepWithSpeedOptimizations.test_training_step_with_early_tool_loss _
tests/integration/test_speed_optimization_integration.py:407: in test_training_step_with_early_tool_loss
    assert "early_tool" in loss_dict
E   AssertionError: assert 'early_tool' in {'ce_ground_truth': 7.100335597991943, 'ce_teacher': 7.044817924499512, 'grad_norm': 0.5460230708122253, 'total': 2.829030752182007}
----------------------------- Captured stdout call -----------------------------
[distill_kd] Step 0 loss components: ce_ground_truth=7.1003, ce_teacher=7.0448, total=2.8290
____________________ TestAPITier.test_tier_limits_structure ____________________
tests/models/test_teacher_client.py:44: in test_tier_limits_structure
    assert hasattr(limits, 'rpm_limit')
E   AssertionError: assert False
E    +  where False = hasattr(TierLimits(rpm=3, tpm=500000, tpd=1500000, concurrency=1, delay=20.0), 'rpm_limit')
__________ TestTeacherClientInitialization.test_init_api_key_from_env __________
tests/models/test_teacher_client.py:77: in test_init_api_key_from_env
    assert client.api_key == "env_api_key"
E   AssertionError: assert 'sk-weBO8bPAP...Ev0I7oAqpjsgG' == 'env_api_key'
E     
E     - env_api_key
E     + sk-weBO8bPAP4wg4Gjwg02auszjDQMLMU74qr9Ev0I7oAqpjsgG
_____________ TestTeacherClientInitialization.test_init_hf_backend _____________
tests/models/test_teacher_client.py:90: in test_init_hf_backend
    mock_pipeline.assert_called_once()
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:918: in assert_called_once
    raise AssertionError(msg)
E   AssertionError: Expected 'pipeline' to have been called once. Called 0 times.
----------------------------- Captured stdout call -----------------------------
[TeacherClient] Loading HuggingFace model: test/model
[TeacherClient] Model loaded on cpu
___________ TestTierDetection.test_update_tier_from_response_headers ___________
tests/models/test_teacher_client.py:168: in test_update_tier_from_response_headers
    assert client._tier == APITier.TIER2
E   AssertionError: assert <APITier.FREE: 'free'> == <APITier.TIER2: 'tier2'>
E     
E     - tier2
E     + free
______________ TestTierDetection.test_update_tier_unknown_header _______________
tests/models/test_teacher_client.py:180: in test_update_tier_unknown_header
    assert client._tier == APITier.UNKNOWN
E   AssertionError: assert <APITier.FREE: 'free'> == <APITier.UNKNOWN: 'unknown'>
E     
E     - unknown
E     + free
__________________ TestRetrySession.test_setup_retry_session ___________________
tests/models/test_teacher_client.py:191: in test_setup_retry_session
    assert hasattr(client, '_session')
E   AssertionError: assert False
E    +  where False = hasattr(<models.teacher.teacher_client.TeacherClient object at 0x17ae88750>, '_session')
__________ TestHTTPBackendSampling.test_sample_single_prompt_success ___________
tests/models/test_teacher_client.py:244: in test_sample_single_prompt_success
    assert "finish_reason" in results[0]
E   AssertionError: assert 'finish_reason' in {'logits': None, 'prompt': 'What is 2+2?', 'text': 'Test response'}
----------------------------- Captured stdout call -----------------------------
[TeacherClient] Starting request: endpoint=http://test.com/v1/chat/completions, model=kimi-k2-thinking, prompt_len=12, max_tokens=16384, max_attempts=6
[TeacherClient] Current tier: free, limits: 3 RPM, 500,000 TPM, delay=20.0s
[TeacherClient] Initial attempt 1/6
[TeacherClient] Response: status=200, duration=0.00s
[TeacherClient] No rate limit headers found in response
[TeacherClient] Success: response_len=13, tokens=unknown (input: unknown, output: unknown)
_______________ TestHTTPBackendSampling.test_sample_with_logits ________________
tests/models/test_teacher_client.py:262: in test_sample_with_logits
    assert results[0]["logits"] is not None
E   assert None is not None
----------------------------- Captured stdout call -----------------------------
[TeacherClient] Starting request: endpoint=http://test.com/v1/chat/completions, model=kimi-k2-thinking, prompt_len=4, max_tokens=16384, max_attempts=6
[TeacherClient] Current tier: free, limits: 3 RPM, 500,000 TPM, delay=20.0s
[TeacherClient] Initial attempt 1/6
[TeacherClient] Response: status=200, duration=0.00s
[TeacherClient] No rate limit headers found in response
[TeacherClient] Success: response_len=4, tokens=unknown (input: unknown, output: unknown)
_____________ TestHTTPBackendSampling.test_sample_retry_on_failure _____________
tests/models/test_teacher_client.py:284: in test_sample_retry_on_failure
    assert results[0]["text"] == "Success"
E   AssertionError: assert '' == 'Success'
E     
E     - Success
----------------------------- Captured stdout call -----------------------------
[TeacherClient] Starting request: endpoint=http://test.com/v1/chat/completions, model=kimi-k2-thinking, prompt_len=4, max_tokens=16384, max_attempts=6
[TeacherClient] Current tier: free, limits: 3 RPM, 500,000 TPM, delay=20.0s
[TeacherClient] Initial attempt 1/6
[TeacherClient] Response: status=429, duration=0.00s
[TeacherClient] Unexpected error (attempt 1/6): TypeError: 'Mock' object is not iterable
[TeacherClient] Traceback: Traceback (most recent call last):
  File "/Users/darianrosebrook/Desktop/Projects/distill/models/teacher/teacher_client.py", line 547, in _sample_single_with_retry
    all_rate_headers = {
                       ^
TypeError: 'Mock' object is not iterable

[TeacherClient] ERROR: All retries exhausted. Final error: 'Mock' object is not iterable
___________ TestHTTPBackendSampling.test_sample_rate_limit_handling ____________
tests/models/test_teacher_client.py:298: in test_sample_rate_limit_handling
    with pytest.raises(requests.exceptions.HTTPError):
E   Failed: DID NOT RAISE <class 'requests.exceptions.HTTPError'>
----------------------------- Captured stdout call -----------------------------
[TeacherClient] Starting request: endpoint=http://test.com/v1/chat/completions, model=kimi-k2-thinking, prompt_len=4, max_tokens=16384, max_attempts=6
[TeacherClient] Current tier: free, limits: 3 RPM, 500,000 TPM, delay=20.0s
[TeacherClient] Initial attempt 1/6
[TeacherClient] Response: status=429, duration=0.00s
[TeacherClient] Rate limit related headers: Retry-After=2
[TeacherClient] Rate limited (429), waiting 20.0s before retry 1/6 (Retry-After: 2.0s, tier min: 20.0s, tier backoff: 20.0s, using: 20.0s)
[TeacherClient] Retry attempt 2/6
[TeacherClient] Response: status=429, duration=0.00s
[TeacherClient] Rate limit related headers: Retry-After=2
[TeacherClient] Rate limited (429), waiting 40.0s before retry 2/6 (Retry-After: 2.0s, tier min: 20.0s, tier backoff: 40.0s, using: 40.0s)
[TeacherClient] Retry attempt 3/6
[TeacherClient] Response: status=429, duration=0.00s
[TeacherClient] Rate limit related headers: Retry-After=2
[TeacherClient] Rate limited (429), waiting 80.0s before retry 3/6 (Retry-After: 2.0s, tier min: 20.0s, tier backoff: 80.0s, using: 80.0s)
[TeacherClient] Retry attempt 4/6
[TeacherClient] Response: status=429, duration=0.00s
[TeacherClient] Rate limit related headers: Retry-After=2
[TeacherClient] Rate limited (429), waiting 160.0s before retry 4/6 (Retry-After: 2.0s, tier min: 20.0s, tier backoff: 160.0s, using: 160.0s)
[TeacherClient] Retry attempt 5/6
[TeacherClient] Response: status=429, duration=0.00s
[TeacherClient] Rate limit related headers: Retry-After=2
[TeacherClient] Rate limited (429), waiting 320.0s before retry 5/6 (Retry-After: 2.0s, tier min: 20.0s, tier backoff: 320.0s, using: 320.0s)
[TeacherClient] Retry attempt 6/6
[TeacherClient] Response: status=429, duration=0.00s
[TeacherClient] Rate limit related headers: Retry-After=2
[TeacherClient] Rate limited (429), waiting 640.0s before retry 6/6 (Retry-After: 2.0s, tier min: 20.0s, tier backoff: 640.0s, using: 640.0s)
[TeacherClient] ERROR: Max retries (6) exceeded. Last error: Rate limit (429)
[TeacherClient] ERROR: All retries exhausted. Final error: Max retries exceeded
___________ TestHTTPBackendSampling.test_sample_max_retries_exceeded ___________
tests/models/test_teacher_client.py:313: in test_sample_max_retries_exceeded
    with pytest.raises(requests.exceptions.HTTPError):
E   Failed: DID NOT RAISE <class 'requests.exceptions.HTTPError'>
----------------------------- Captured stdout call -----------------------------
[TeacherClient] Starting request: endpoint=http://test.com/v1/chat/completions, model=kimi-k2-thinking, prompt_len=4, max_tokens=16384, max_attempts=6
[TeacherClient] Current tier: free, limits: 3 RPM, 500,000 TPM, delay=20.0s
[TeacherClient] Initial attempt 1/6
[TeacherClient] Response: status=500, duration=0.00s
[TeacherClient] Unexpected error (attempt 1/6): TypeError: 'Mock' object is not iterable
[TeacherClient] Traceback: Traceback (most recent call last):
  File "/Users/darianrosebrook/Desktop/Projects/distill/models/teacher/teacher_client.py", line 547, in _sample_single_with_retry
    all_rate_headers = {
                       ^
TypeError: 'Mock' object is not iterable

[TeacherClient] ERROR: All retries exhausted. Final error: 'Mock' object is not iterable
____________ TestHTTPBackendSampling.test_sample_multi_prompt_batch ____________
tests/models/test_teacher_client.py:336: in test_sample_multi_prompt_batch
    assert results[1]["text"] == "Response 2"
E   AssertionError: assert 'Response 1' == 'Response 2'
E     
E     - Response 2
E     ?          ^
E     + Response 1
E     ?          ^
----------------------------- Captured stdout call -----------------------------
[TeacherClient] Starting request: endpoint=http://test.com/v1/chat/completions, model=kimi-k2-thinking, prompt_len=8, max_tokens=16384, max_attempts=6
[TeacherClient] Current tier: free, limits: 3 RPM, 500,000 TPM, delay=20.0s
[TeacherClient] Initial attempt 1/6
[TeacherClient] Response: status=200, duration=0.00s
[TeacherClient] No rate limit headers found in response
[TeacherClient] Success: response_len=10, tokens=unknown (input: unknown, output: unknown)
[TeacherClient] Starting request: endpoint=http://test.com/v1/chat/completions, model=kimi-k2-thinking, prompt_len=8, max_tokens=16384, max_attempts=6
[TeacherClient] Current tier: free, limits: 3 RPM, 500,000 TPM, delay=20.0s
[TeacherClient] Initial attempt 1/6
[TeacherClient] Response: status=200, duration=0.00s
[TeacherClient] No rate limit headers found in response
[TeacherClient] Success: response_len=10, tokens=unknown (input: unknown, output: unknown)
_______________ TestCircuitBreaker.test_try_fallback_api_success _______________
tests/models/test_teacher_client.py:396: in test_try_fallback_api_success
    assert results[0]["text"] == "Fallback response"
E   AssertionError: assert '' == 'Fallback response'
E     
E     - Fallback response
----------------------------- Captured stdout call -----------------------------
[TeacherClient] Starting request: endpoint=http://test.com/v1/chat/completions, model=kimi-k2-thinking, prompt_len=4, max_tokens=16384, max_attempts=6
[TeacherClient] Current tier: free, limits: 3 RPM, 500,000 TPM, delay=20.0s
[TeacherClient] Initial attempt 1/6
[TeacherClient] Response: status=500, duration=0.00s
[TeacherClient] Unexpected error (attempt 1/6): TypeError: 'Mock' object is not iterable
[TeacherClient] Traceback: Traceback (most recent call last):
  File "/Users/darianrosebrook/Desktop/Projects/distill/models/teacher/teacher_client.py", line 547, in _sample_single_with_retry
    all_rate_headers = {
                       ^
TypeError: 'Mock' object is not iterable

[TeacherClient] ERROR: All retries exhausted. Final error: 'Mock' object is not iterable
__________________ TestHealthCheck.test_health_check_failure ___________________
tests/models/test_teacher_client.py:422: in test_health_check_failure
    assert http_client.health_check() is False
E   assert True is False
E    +  where True = health_check()
E    +    where health_check = <models.teacher.teacher_client.TeacherClient object at 0x17a6e5e10>.health_check
______________ TestMultiStepSampling.test_sample_multi_step_basic ______________
tests/models/test_teacher_client.py:450: in test_sample_multi_step_basic
    assert len(results) == 1
E   AssertionError: assert 3 == 1
E    +  where 3 = len({'logits': None, 'message': {'content': 'Final answer: 42'}, 'text': 'Final answer: 42'})
----------------------------- Captured stdout call -----------------------------
[TeacherClient] Starting request: endpoint=http://test.com/v1/chat/completions, model=kimi-k2-thinking, prompt_len=0, max_tokens=16384, max_attempts=6
[TeacherClient] Current tier: free, limits: 3 RPM, 500,000 TPM, delay=20.0s
[TeacherClient] Initial attempt 1/6
[TeacherClient] Response: status=200, duration=0.00s
[TeacherClient] No rate limit headers found in response
[TeacherClient] Success: response_len=16, tokens=unknown (input: unknown, output: unknown)
_________________ TestErrorHandling.test_sample_empty_prompts __________________
tests/models/test_teacher_client.py:466: in test_sample_empty_prompts
    with pytest.raises(ValueError):
E   Failed: DID NOT RAISE <class 'ValueError'>
_______________ TestErrorHandling.test_sample_malformed_response _______________
tests/models/test_teacher_client.py:477: in test_sample_malformed_response
    with pytest.raises(ValueError):
E   Failed: DID NOT RAISE <class 'ValueError'>
----------------------------- Captured stdout call -----------------------------
[TeacherClient] Starting request: endpoint=http://test.com/v1/chat/completions, model=kimi-k2-thinking, prompt_len=4, max_tokens=16384, max_attempts=6
[TeacherClient] Current tier: free, limits: 3 RPM, 500,000 TPM, delay=20.0s
[TeacherClient] Initial attempt 1/6
[TeacherClient] Response: status=200, duration=0.00s
[TeacherClient] Unexpected error (attempt 1/6): TypeError: 'Mock' object is not iterable
[TeacherClient] Traceback: Traceback (most recent call last):
  File "/Users/darianrosebrook/Desktop/Projects/distill/models/teacher/teacher_client.py", line 547, in _sample_single_with_retry
    all_rate_headers = {
                       ^
TypeError: 'Mock' object is not iterable

[TeacherClient] ERROR: All retries exhausted. Final error: 'Mock' object is not iterable
____________________ TestErrorHandling.test_sample_timeout _____________________
tests/models/test_teacher_client.py:485: in test_sample_timeout
    with pytest.raises(requests.exceptions.Timeout):
E   Failed: DID NOT RAISE <class 'requests.exceptions.Timeout'>
----------------------------- Captured stdout call -----------------------------
[TeacherClient] Starting request: endpoint=http://test.com/v1/chat/completions, model=kimi-k2-thinking, prompt_len=4, max_tokens=16384, max_attempts=6
[TeacherClient] Current tier: free, limits: 3 RPM, 500,000 TPM, delay=20.0s
[TeacherClient] Initial attempt 1/6
[TeacherClient] Timeout (attempt 1/6, timeout=600s): 
[TeacherClient] Retrying in 1.0s
[TeacherClient] Retry attempt 2/6
[TeacherClient] Timeout (attempt 2/6, timeout=600s): 
[TeacherClient] Retrying in 2.0s
[TeacherClient] Retry attempt 3/6
[TeacherClient] Timeout (attempt 3/6, timeout=600s): 
[TeacherClient] Retrying in 4.0s
[TeacherClient] Retry attempt 4/6
[TeacherClient] Timeout (attempt 4/6, timeout=600s): 
[TeacherClient] Retrying in 8.0s
[TeacherClient] Retry attempt 5/6
[TeacherClient] Timeout (attempt 5/6, timeout=600s): 
[TeacherClient] Retrying in 16.0s
[TeacherClient] Retry attempt 6/6
[TeacherClient] Timeout (attempt 6/6, timeout=600s): 
[TeacherClient] Retrying in 32.0s
[TeacherClient] ERROR: Max retries (6) exceeded. Last error: Timeout
[TeacherClient] ERROR: All retries exhausted. Final error: 
_________________ TestCAWSBudgetEnforcement.test_tier_1_limits _________________
tests/runtime/test_caws_budget_enforcement.py:38: in test_tier_1_limits
    controller = RefinementController(
E   TypeError: RefinementController.__init__() got an unexpected keyword argument 'model'
_________________ TestCAWSBudgetEnforcement.test_tier_2_limits _________________
tests/runtime/test_caws_budget_enforcement.py:58: in test_tier_2_limits
    controller = RefinementController(
E   TypeError: RefinementController.__init__() got an unexpected keyword argument 'model'
_________________ TestCAWSBudgetEnforcement.test_tier_3_limits _________________
tests/runtime/test_caws_budget_enforcement.py:78: in test_tier_3_limits
    controller = RefinementController(
E   TypeError: RefinementController.__init__() got an unexpected keyword argument 'model'
___________ TestCAWSBudgetEnforcement.test_budget_breach_forces_halt ___________
tests/runtime/test_caws_budget_enforcement.py:98: in test_budget_breach_forces_halt
    controller = RefinementController(
E   TypeError: RefinementController.__init__() got an unexpected keyword argument 'model'
_______________ TestLatentModeEngine.test_max_latent_spans_limit _______________
tests/runtime/test_latent_mode.py:141: in test_max_latent_spans_limit
    assert (
E   AssertionError: assert ('unmatched_bot' == 'max_latent_spans'
E     
E     - max_latent_spans
E     + unmatched_bot or 'latent' == 'language'
E     
E     - language
E     + latent)
___________ TestSafetyEdgeCases.test_generation_ends_in_latent_mode ____________
tests/runtime/test_safety_edge_cases.py:195: in test_generation_ends_in_latent_mode
    engine.generate_with_latent_mode(
runtime/engine/loop.py:275: in generate_with_latent_mode
    logits, kv_caches = self.model.forward_decode(
    ^^^^^^^^^^^^^^^^^
E   TypeError: cannot unpack non-iterable Mock object
__________________________ test_pipeline_determinism ___________________________
tests/test_claims_pipeline_toy.py:274: in test_pipeline_determinism
    assert json.dumps(result1, sort_keys=True) == json.dumps(result2, sort_keys=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:238: in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:200: in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:258: in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:180: in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
E   TypeError: Object of type VerifiableContentResult is not JSON serializable
______________________ test_pipeline_outcome_distribution ______________________
tests/test_claims_pipeline_toy.py:376: in test_pipeline_outcome_distribution
    assert len(set(outcomes)) > 0
E   assert 0 > 0
E    +  where 0 = len(set())
E    +    where set() = set([])
_______________________ test_kv_cache_index_advancement ________________________
tests/test_prefill_decode_consistency.py:274: in test_kv_cache_index_advancement
    assert k_cache.shape[2] == expected_seq_len, (
E   AssertionError: Layer 0: Expected cache length 11, got 1
E   assert 1 == 11
____________________ test_special_token_ids_match_constants ____________________
tests/test_tokenizer_contract.py:37: in test_special_token_ids_match_constants
    bos_id = tokenizer.bos_token_id
             ^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'MockTokenizer' object has no attribute 'bos_token_id'
____________________ test_special_tokens_are_single_tokens _____________________
tests/test_tokenizer_contract.py:69: in test_special_tokens_are_single_tokens
    assert len(bot_tokens) == 1, (
E   AssertionError: BOT token '<bot>' should be single token, got 3: [621, 541, 150]
E   assert 3 == 1
E    +  where 3 = len([621, 541, 150])
__________________________ test_round_trip_stability ___________________________
tests/test_tokenizer_contract.py:110: in test_round_trip_stability
    assert BOT_TOKEN in decoded or BOT_TOKEN.strip() in decoded, (
E   AssertionError: BOT token lost in round-trip for prompt: '<bot> tool call <eot>' -> 'read_file'
E   assert ('<bot>' in 'read_file' or '<bot>' in 'read_file')
E    +  where '<bot>' = <built-in method strip of str object at 0x172460830>()
E    +    where <built-in method strip of str object at 0x172460830> = '<bot>'.strip
_____________________________ test_masking_safety ______________________________
tests/test_tokenizer_contract.py:135: in test_masking_safety
    bos_id = tokenizer.bos_token_id
             ^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'MockTokenizer' object has no attribute 'bos_token_id'
____________ TestModelCreation.test_create_model_with_quantization _____________
tests/training/test_distill_kd.py:194: in test_create_model_with_quantization
    mock_quantize.assert_called_once()
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:918: in assert_called_once
    raise AssertionError(msg)
E   AssertionError: Expected 'quantize_model' to have been called once. Called 0 times.
______________ TestModelCreation.test_create_model_invalid_config ______________
tests/training/test_distill_kd.py:200: in test_create_model_invalid_config
    with pytest.raises(KeyError):
E   Failed: DID NOT RAISE <class 'KeyError'>
______________ TestOptimizerCreation.test_create_optimizer_adamw _______________
tests/training/test_distill_kd.py:227: in test_create_optimizer_adamw
    assert optimizer.defaults['lr'] == 1e-4
E   assert 0.0002 == 0.0001
__________ TestOptimizerCreation.test_create_optimizer_default_config __________
tests/training/test_distill_kd.py:239: in test_create_optimizer_default_config
    assert optimizer.defaults['lr'] == 1e-3  # Default LR
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   assert 0.0002 == 0.001
___________ TestOptimizerCreation.test_create_optimizer_invalid_type ___________
tests/training/test_distill_kd.py:245: in test_create_optimizer_invalid_type
    with pytest.raises(ValueError):
E   Failed: DID NOT RAISE <class 'ValueError'>
__________________ TestQATOperations.test_apply_qat_to_model ___________________
tests/training/test_distill_kd.py:316: in test_apply_qat_to_model
    apply_qat_to_model(simple_model, qat_cfg, device)
training/distill_kd.py:581: in apply_qat_to_model
    quantized_model = quantize_model(
training/quant_qat_int8.py:300: in quantize_model
    for block in model.blocks:
                 ^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1688: in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
E   AttributeError: 'Linear' object has no attribute 'blocks'
----------------------------- Captured stdout call -----------------------------
[distill_kd] Applying QAT: weight_bits=8, act_bits=8
_______________ TestQATOperations.test_check_qat_stability_valid _______________
tests/training/test_distill_kd.py:332: in test_check_qat_stability_valid
    assert "has_nan" in result
E   AssertionError: assert 'has_nan' in {'qat_stability.cosine_sim': 0.0, 'qat_stability.error': 'Linear.forward() takes 2 positional arguments but 3 were given', 'qat_stability.has_nan': 1.0}
____________ TestQATOperations.test_check_qat_stability_nan_weights ____________
tests/training/test_distill_kd.py:352: in test_check_qat_stability_nan_weights
    assert result["has_nan"]  # Should detect NaN
           ^^^^^^^^^^^^^^^^^
E   KeyError: 'has_nan'
_______________ TestTrainingStep.test_train_step_vocab_clamping ________________
tests/training/test_distill_kd.py:432: in test_train_step_vocab_clamping
    result = train_step(
training/distill_kd.py:1022: in train_step
    student_logits = model(input_ids, attention_mask)  # [B, T, V]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:364: in forward
    x = blk(x, attn_mask)
        ^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:259: in forward
    x = x + self.attn(self.norm1(x), attn_mask)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:182: in forward
    attn_scores = attn_scores + additive_mask
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 3
____________ TestTrainingStep.test_train_step_with_self_evaluation _____________
tests/training/test_distill_kd.py:483: in test_train_step_with_self_evaluation
    result = train_step(
training/distill_kd.py:1344: in train_step
    loss_dict = combined_kd_loss(
training/losses.py:490: in combined_kd_loss
    kl_loss = kl_divergence(
training/losses.py:57: in kl_divergence
    kl = F.kl_div(student_probs, teacher_probs,
venv/lib/python3.11/site-packages/torch/nn/functional.py:2961: in kl_div
    reduced = torch.kl_div(input, target, reduction_enum, log_target=log_target)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: The size of tensor a (20) must match the size of tensor b (10) at non-singleton dimension 0
_____________ TestCheckpointOperations.test_save_checkpoint_basic ______________
tests/training/test_distill_kd.py:568: in test_save_checkpoint_basic
    assert len(checkpoint_files) == 1
E   assert 0 == 1
E    +  where 0 = len([])
----------------------------- Captured stdout call -----------------------------
[utils] WARN: Could not serialize tensor blocks.0.attn.wk.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.attn.wo.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.attn.wq.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.attn.wv.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.mlp.w1.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.mlp.w2.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.mlp.w3.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.norm1.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.norm2.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.attn.wk.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.attn.wo.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.attn.wq.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.attn.wv.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.mlp.w1.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.mlp.w2.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.mlp.w3.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.norm1.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.norm2.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor embed.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor lm_head.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor norm_f.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[distill_kd] Saved checkpoint: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_save_checkpoint_basic0/checkpoints/checkpoint_step_100.pt
_________ TestCheckpointOperations.test_save_checkpoint_with_metadata __________
tests/training/test_distill_kd.py:599: in test_save_checkpoint_with_metadata
    assert len(checkpoint_files) == 1
E   assert 0 == 1
E    +  where 0 = len([])
----------------------------- Captured stdout call -----------------------------
[utils] WARN: Could not serialize tensor blocks.0.attn.wk.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.attn.wo.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.attn.wq.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.attn.wv.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.mlp.w1.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.mlp.w2.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.mlp.w3.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.norm1.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.0.norm2.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.attn.wk.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.attn.wo.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.attn.wq.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.attn.wv.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.mlp.w1.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.mlp.w2.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.mlp.w3.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.norm1.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor blocks.1.norm2.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor embed.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor lm_head.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[utils] WARN: Could not serialize tensor norm_f.weight: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[distill_kd] Saved checkpoint: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_save_checkpoint_with_meta0/checkpoints/checkpoint_step_200.pt
_____________ TestConfigOperations.test_merge_configs_nested_merge _____________
tests/training/test_distill_process.py:122: in test_merge_configs_nested_merge
    assert result["model"]["arch"]["layers"] == 6
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'layers'
___________________ TestModelLoading.test_load_model_success ___________________
tests/training/test_distill_process.py:172: in test_load_model_success
    mock_model_cfg.assert_called_once_with(
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:951: in assert_called_once_with
    return self.assert_called_with(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:939: in assert_called_with
    raise AssertionError(_error_message()) from cause
E   AssertionError: expected call not found.
E   Expected: ModelCfg(d_model=512, n_layers=8, n_heads=8, n_kv_heads=4, d_head=64, vocab_size=32000)
E     Actual: ModelCfg(d_model=512, n_layers=8, n_heads=8, n_kv_heads=4, d_head=64, vocab_size=32000, rope_theta=10000.0, rope_scaling='dynamic', dropout=0.0)
_______________ TestModelLoading.test_load_model_without_config ________________
tests/training/test_distill_process.py:205: in test_load_model_without_config
    assert result == mock_model
E   AssertionError: assert <Mock name='StudentLM().to()' id='6349641744'> == <Mock name='StudentLM()' id='6349637840'>
________ TestTextGeneration.test_generate_text_from_logits_temperature _________
tests/training/test_distill_process.py:251: in test_generate_text_from_logits_temperature
    result = generate_text_from_logits(logits, mock_tokenizer, temperature=temperature)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: generate_text_from_logits() got an unexpected keyword argument 'temperature'
___________ TestTextGeneration.test_generate_text_from_logits_top_k ____________
tests/training/test_distill_process.py:262: in test_generate_text_from_logits_top_k
    result = generate_text_from_logits(logits, mock_tokenizer, top_k=top_k)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: generate_text_from_logits() got an unexpected keyword argument 'top_k'
___________ TestTextGeneration.test_generate_text_from_logits_top_p ____________
tests/training/test_distill_process.py:272: in test_generate_text_from_logits_top_p
    result = generate_text_from_logits(logits, mock_tokenizer, top_p=top_p)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: generate_text_from_logits() got an unexpected keyword argument 'top_p'
___________ TestTextGeneration.test_generate_text_from_logits_greedy ___________
tests/training/test_distill_process.py:284: in test_generate_text_from_logits_greedy
    result = generate_text_from_logits(logits, mock_tokenizer, temperature=0.0)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: generate_text_from_logits() got an unexpected keyword argument 'temperature'
________________ TestTrainingStep.test_train_step_process_basic ________________
tests/training/test_distill_process.py:351: in test_train_step_process_basic
    losses = train_step_process(
E   TypeError: train_step_process() got an unexpected keyword argument 'kd_weight'
____________ TestTrainingStep.test_train_step_process_zero_weights _____________
tests/training/test_distill_process.py:384: in test_train_step_process_zero_weights
    losses = train_step_process(
E   TypeError: train_step_process() got an unexpected keyword argument 'kd_weight'
_______________ TestTrainingStep.test_train_step_process_no_grad _______________
tests/training/test_distill_process.py:408: in test_train_step_process_no_grad
    losses = train_step_process(
E   TypeError: train_step_process() got an unexpected keyword argument 'kd_weight'
_______ TestTrainingStep.test_train_step_process_missing_process_labels ________
tests/training/test_distill_process.py:440: in test_train_step_process_missing_process_labels
    losses = train_step_process(
E   TypeError: train_step_process() got an unexpected keyword argument 'kd_weight'
______________________ TestMainFunction.test_main_success ______________________
tests/training/test_distill_process.py:503: in test_main_success
    main()
training/distill_process.py:306: in main
    optimizer = torch.optim.AdamW(
venv/lib/python3.11/site-packages/torch/optim/adamw.py:52: in __init__
    super().__init__(params, defaults)
venv/lib/python3.11/site-packages/torch/optim/optimizer.py:271: in __init__
    param_groups = list(params)
                   ^^^^^^^^^^^^
E   TypeError: 'Mock' object is not iterable
----------------------------- Captured stdout call -----------------------------
[distill_process] Loading model from: model.pt
________________ TestMainFunction.test_main_config_load_failure ________________
tests/training/test_distill_process.py:520: in test_main_config_load_failure
    main()
training/distill_process.py:297: in main
    cfg = merge_configs(args.config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
training/distill_process.py:45: in merge_configs
    config = load_config(config_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1124: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1128: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1183: in _execute_mock_call
    raise effect
E   FileNotFoundError: Config not found
________________ TestMainFunction.test_main_model_load_failure _________________
tests/training/test_distill_process.py:541: in test_main_model_load_failure
    main()
training/distill_process.py:302: in main
    model = load_model(args.checkpoint, device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1124: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1128: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1183: in _execute_mock_call
    raise effect
E   Exception: Model load failed
----------------------------- Captured stdout call -----------------------------
[distill_process] Loading model from: bad_model.pt
____________ TestTorchScriptExport.test_export_torchscript_success _____________
tests/training/test_export_student.py:63: in test_export_torchscript_success
    mock_save.assert_called_once_with(mock_traced, str(output_path))
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:950: in assert_called_once_with
    raise AssertionError(msg)
E   AssertionError: Expected 'save' to be called once. Called 0 times.
----------------------------- Captured stdout call -----------------------------
[export_student] Saved TorchScript: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_export_torchscript_succes0/model.pt
______________ TestContractCreation.test_create_contract_success _______________
tests/training/test_export_student.py:183: in test_create_contract_success
    result = create_contract(mock_config, enumerated_T, output_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/export_student.py:63: in create_contract
    with open(contract_path, "w") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^
E   FileNotFoundError: [Errno 2] No such file or directory: '/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_create_contract_success0/contract.json/contract.json'
_________ TestContractCreation.test_create_contract_directory_creation _________
tests/training/test_export_student.py:221: in test_create_contract_directory_creation
    result = create_contract(mock_config, enumerated_T, output_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/export_student.py:63: in create_contract
    with open(contract_path, "w") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^
E   FileNotFoundError: [Errno 2] No such file or directory: '/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_create_contract_directory0/subdir/contract.json/contract.json'
____________ TestContractCreation.test_create_contract_file_writing ____________
tests/training/test_export_student.py:231: in test_create_contract_file_writing
    result = create_contract(mock_config, enumerated_T, output_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/export_student.py:63: in create_contract
    with open(contract_path, "w") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^
E   FileNotFoundError: [Errno 2] No such file or directory: '/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_create_contract_file_writ0/contract.json/contract.json'
________________ TestMainFunction.test_main_torchscript_export _________________
tests/training/test_export_student.py:290: in test_main_torchscript_export
    main()
training/export_student.py:142: in main
    example_input = torch.zeros((1, args.seq), dtype=torch.int32)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: zeros(): argument 'size' failed to unpack the object at pos 2 with error "type must be tuple of ints,but got Mock"
----------------------------- Captured stdout call -----------------------------
[export_student] Loaded config from checkpoint
______________ TestMainFunction.test_main_exported_program_export ______________
tests/training/test_export_student.py:341: in test_main_exported_program_export
    main()
training/export_student.py:142: in main
    example_input = torch.zeros((1, args.seq), dtype=torch.int32)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: zeros(): argument 'size' failed to unpack the object at pos 2 with error "type must be tuple of ints,but got Mock"
----------------------------- Captured stdout call -----------------------------
[export_student] Loaded config from checkpoint
________________ TestMainFunction.test_main_invalid_export_type ________________
tests/training/test_export_student.py:401: in test_main_invalid_export_type
    main()
training/export_student.py:142: in main
    example_input = torch.zeros((1, args.seq), dtype=torch.int32)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: zeros(): argument 'size' failed to unpack the object at pos 2 with error "type must be tuple of ints,but got Mock"
----------------------------- Captured stdout call -----------------------------
[export_student] Loaded config from checkpoint
__________________ TestExportIntegration.test_export_workflow __________________
tests/training/test_export_student.py:437: in test_export_workflow
    contract_result = create_contract(config, enumerated_T, contract_path)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/export_student.py:63: in create_contract
    with open(contract_path, "w") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^
E   FileNotFoundError: [Errno 2] No such file or directory: '/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_export_workflow0/contract.json/contract.json'
----------------------------- Captured stdout call -----------------------------
[export_student] Saved TorchScript: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_export_workflow0/model_ts.pt
___ TestConfigurationHandling.test_contract_creation_with_different_configs ____
tests/training/test_export_student.py:481: in test_contract_creation_with_different_configs
    result = create_contract(config, enumerated_T, contract_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/export_student.py:63: in create_contract
    with open(contract_path, "w") as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^
E   FileNotFoundError: [Errno 2] No such file or directory: '/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_contract_creation_with_di0/contract_0.json/contract.json'
______ TestConfigurationHandling.test_export_with_different_input_shapes _______
tests/training/test_export_student.py:513: in test_export_with_different_input_shapes
    result = export_torchscript(model, example_input, output_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/export_student.py:24: in export_torchscript
    traced = torch.jit.trace(model, example_input)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/jit/_trace.py:806: in trace
    return trace_module(
venv/lib/python3.11/site-packages/torch/jit/_trace.py:1074: in trace_module
    module._c._create_method_from_trace(
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501: in _slow_forward
    result = self.forward(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/training/test_export_student.py:500: in forward
    return self.linear(x)
           ^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501: in _slow_forward
    result = self.forward(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:116: in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x20 and 10x5)
----------------------------- Captured stdout call -----------------------------
[export_student] Saved TorchScript: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_export_with_different_inp0/model_0.pt
[export_student] Saved TorchScript: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_export_with_different_inp0/model_1.pt
_______ TestFeatureManager.test_feature_manager_enable_with_dependencies _______
tests/training/test_feature_flags.py:141: in test_feature_manager_enable_with_dependencies
    manager.enable_feature(flag_with_deps)
training/feature_flags.py:176: in enable_feature
    raise ValueError(
E   ValueError: Cannot enable self_evaluation: dependency caws_compliance not enabled
__________ TestFeatureManager.test_feature_manager_get_feature_config __________
tests/training/test_feature_flags.py:179: in test_feature_manager_get_feature_config
    config = manager.get_feature_config(flag)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'FeatureManager' object has no attribute 'get_feature_config'
____________ TestFeatureManager.test_feature_manager_list_features _____________
tests/training/test_feature_flags.py:187: in test_feature_manager_list_features
    features = manager.list_features()
               ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'FeatureManager' object has no attribute 'list_features'
__________ TestFeatureManager.test_feature_manager_reset_to_defaults ___________
tests/training/test_feature_flags.py:205: in test_feature_manager_reset_to_defaults
    manager.reset_to_defaults()
    ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'FeatureManager' object has no attribute 'reset_to_defaults'
_________ TestFeatureManager.test_feature_manager_save_to_environment __________
tests/training/test_feature_flags.py:233: in test_feature_manager_save_to_environment
    env_vars = manager.save_to_environment()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'FeatureManager' object has no attribute 'save_to_environment'
__________ TestFeatureManager.test_feature_manager_get_feature_stats ___________
tests/training/test_feature_flags.py:240: in test_feature_manager_get_feature_stats
    stats = manager.get_feature_stats()
            ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'FeatureManager' object has no attribute 'get_feature_stats'
__________ TestFeatureInteractions.test_feature_manager_thread_safety __________
tests/training/test_feature_flags.py:370: in test_feature_manager_thread_safety
    assert enabled == True, f"Thread {thread_id} failed to enable feature"
E   AssertionError: Thread 0 failed to enable feature
E   assert False == True
__________ TestLatentCurriculum.test_curriculum_applies_latent_slots ___________
tests/training/test_latent_curriculum.py:51: in test_curriculum_applies_latent_slots
    result = curriculum.apply(sample_example, mock_tokenizer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
data/wrappers/curriculum.py:104: in apply
    loss_mask = self._create_loss_mask(training_text, tokenizer, latent_slots, replaced)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
data/wrappers/curriculum.py:204: in _create_loss_mask
    mask = torch.ones(len(tokens), dtype=torch.bool)
                      ^^^^^^^^^^^
E   TypeError: object of type 'Mock' has no len()
____________ TestLatentCurriculum.test_curriculum_creates_loss_mask ____________
tests/training/test_latent_curriculum.py:60: in test_curriculum_creates_loss_mask
    result = curriculum.apply(sample_example, mock_tokenizer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
data/wrappers/curriculum.py:104: in apply
    loss_mask = self._create_loss_mask(training_text, tokenizer, latent_slots, replaced)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
data/wrappers/curriculum.py:204: in _create_loss_mask
    mask = torch.ones(len(tokens), dtype=torch.bool)
                      ^^^^^^^^^^^
E   TypeError: object of type 'Mock' has no len()
____________ TestLatentCurriculum.test_loss_mask_masks_latent_spans ____________
tests/training/test_latent_curriculum.py:96: in test_loss_mask_masks_latent_spans
    result = curriculum.apply(example, mock_tokenizer)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
data/wrappers/curriculum.py:104: in apply
    loss_mask = self._create_loss_mask(training_text, tokenizer, latent_slots, replaced)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
data/wrappers/curriculum.py:204: in _create_loss_mask
    mask = torch.ones(len(tokens), dtype=torch.bool)
                      ^^^^^^^^^^^
E   TypeError: object of type 'Mock' has no len()
________ TestStructuredFormatter.test_structured_formatter_format_basic ________
tests/training/test_logging_utils.py:116: in test_structured_formatter_format_basic
    assert parsed["name"] == "test"
           ^^^^^^^^^^^^^^
E   KeyError: 'name'
___ TestStructuredFormatter.test_structured_formatter_format_with_exception ____
tests/training/test_logging_utils.py:172: in test_structured_formatter_format_with_exception
    raise ValueError("Test exception")
E   ValueError: Test exception

During handling of the above exception, another exception occurred:
tests/training/test_logging_utils.py:174: in test_structured_formatter_format_with_exception
    exc_info = sys.exc_info()
               ^^^
E   NameError: name 'sys' is not defined
_______ TestStructuredFormatter.test_structured_formatter_json_validity ________
tests/training/test_logging_utils.py:254: in test_structured_formatter_json_validity
    assert "name" in parsed
E   AssertionError: assert 'name' in {'extra_field': 'extra_value', 'level': 'DEBUG', 'logger': 'test', 'message': 'Debug', ...}
__________ TestSetupTrainingLogging.test_setup_training_logging_basic __________
tests/training/test_logging_utils.py:272: in test_setup_training_logging_basic
    result = setup_training_logging("test_run", log_level=logging.INFO)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: setup_training_logging() got an unexpected keyword argument 'log_level'
________ TestSetupTrainingLogging.test_setup_training_logging_with_file ________
tests/training/test_logging_utils.py:289: in test_setup_training_logging_with_file
    result = setup_training_logging("test_run", log_file=log_file)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: setup_training_logging() got an unexpected keyword argument 'log_file'
____ TestSetupTrainingLogging.test_setup_training_logging_different_levels _____
tests/training/test_logging_utils.py:307: in test_setup_training_logging_different_levels
    result = setup_training_logging("test_run", log_level=level)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: setup_training_logging() got an unexpected keyword argument 'log_level'
_______________ TestLogTrainingStep.test_log_training_step_basic _______________
tests/training/test_logging_utils.py:323: in test_log_training_step_basic
    mock_logger.info.assert_called_once()
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:918: in assert_called_once
    raise AssertionError(msg)
E   AssertionError: Expected 'info' to have been called once. Called 0 times.
----------------------------- Captured stdout call -----------------------------
{"timestamp": "2025-11-13 16:37:30,074", "level": "INFO", "logger": "training", "message": "Training step completed", "step": 100, "loss": 1.5, "learning_rate": 0.8, "tokens_per_sec": 2.3, "gpu_memory_mb": null}
______ TestLogTrainingStep.test_log_training_step_with_additional_metrics ______
tests/training/test_logging_utils.py:344: in test_log_training_step_with_additional_metrics
    log_training_step(50, 0.5, 1e-4, 1.2, additional_metrics=additional)
E   TypeError: log_training_step() got an unexpected keyword argument 'additional_metrics'
____________ TestLogTrainingStep.test_log_training_step_zero_values ____________
tests/training/test_logging_utils.py:361: in test_log_training_step_zero_values
    extra = call_args[1]['extra']
            ^^^^^^^^^^^^
E   TypeError: 'NoneType' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
{"timestamp": "2025-11-13 16:37:30,139", "level": "INFO", "logger": "training", "message": "Training step completed", "step": 0, "loss": 0.0, "learning_rate": 0.0, "tokens_per_sec": 0.0, "gpu_memory_mb": null}
__________ TestLogValidationMetrics.test_log_validation_metrics_basic __________
tests/training/test_logging_utils.py:386: in test_log_validation_metrics_basic
    mock_logger.info.assert_called_once()
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:918: in assert_called_once
    raise AssertionError(msg)
E   AssertionError: Expected 'info' to have been called once. Called 0 times.
----------------------------- Captured stdout call -----------------------------
{"timestamp": "2025-11-13 16:37:30,150", "level": "INFO", "logger": "training", "message": "Validation completed", "step": 100, "loss": 1.2, "accuracy": 0.85, "perplexity": 8.5}
__________ TestLogValidationMetrics.test_log_validation_metrics_empty __________
tests/training/test_logging_utils.py:408: in test_log_validation_metrics_empty
    extra = call_args[1]['extra']
            ^^^^^^^^^^^^
E   TypeError: 'NoneType' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
{"timestamp": "2025-11-13 16:37:30,203", "level": "INFO", "logger": "training", "message": "Validation completed", "step": 50}
_________ TestLogValidationMetrics.test_log_validation_metrics_complex _________
tests/training/test_logging_utils.py:430: in test_log_validation_metrics_complex
    extra = call_args[1]['extra']
            ^^^^^^^^^^^^
E   TypeError: 'NoneType' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
{"timestamp": "2025-11-13 16:37:30,214", "level": "INFO", "logger": "training", "message": "Validation completed", "step": 200, "loss": 0.8, "accuracy": 0.92, "f1_score": 0.89, "precision": 0.91, "recall": 0.87}
____________ TestLogCheckpointSaved.test_log_checkpoint_saved_basic ____________
tests/training/test_logging_utils.py:449: in test_log_checkpoint_saved_basic
    mock_logger.info.assert_called_once()
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:918: in assert_called_once
    raise AssertionError(msg)
E   AssertionError: Expected 'info' to have been called once. Called 0 times.
----------------------------- Captured stdout call -----------------------------
{"timestamp": "2025-11-13 16:37:30,225", "level": "INFO", "logger": "training", "message": "Checkpoint saved", "step": 500, "checkpoint_path": "/path/to/checkpoint.pt", "loss": 0.7}
_______ TestLogCheckpointSaved.test_log_checkpoint_saved_different_paths _______
tests/training/test_logging_utils.py:479: in test_log_checkpoint_saved_different_paths
    extra = call_args[1]['extra']
            ^^^^^^^^^^^^
E   TypeError: 'NoneType' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
{"timestamp": "2025-11-13 16:37:30,278", "level": "INFO", "logger": "training", "message": "Checkpoint saved", "step": 100, "checkpoint_path": "relative/path/model.pt", "loss": 1.0}
______________________ TestLogError.test_log_error_basic _______________________
tests/training/test_logging_utils.py:497: in test_log_error_basic
    mock_logger.error.assert_called_once()
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:918: in assert_called_once
    raise AssertionError(msg)
E   AssertionError: Expected 'error' to have been called once. Called 0 times.
----------------------------- Captured stdout call -----------------------------
{"timestamp": "2025-11-13 16:37:30,364", "level": "ERROR", "logger": "training", "message": "Test error", "error_type": "dict", "error_message": "{'step': 100, 'operation': 'training'}", "step": null}
____________________ TestLogError.test_log_error_no_context ____________________
tests/training/test_logging_utils.py:522: in test_log_error_no_context
    extra = call_args[1]['extra']
            ^^^^^^^^^^^^
E   TypeError: 'NoneType' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
{"timestamp": "2025-11-13 16:37:30,418", "level": "ERROR", "logger": "training", "message": "Runtime error", "error_type": null, "error_message": null, "step": null}
_______________ TestLogError.test_log_error_different_exceptions _______________
tests/training/test_logging_utils.py:545: in test_log_error_different_exceptions
    extra = call_args[1]['extra']
            ^^^^^^^^^^^^
E   TypeError: 'NoneType' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
{"timestamp": "2025-11-13 16:37:30,429", "level": "ERROR", "logger": "training", "message": "Value error", "error_type": null, "error_message": null, "step": null}
___________ TestLoggingIntegration.test_structured_logging_workflow ____________
tests/training/test_logging_utils.py:560: in test_structured_logging_workflow
    logger = setup_training_logging("test_run", log_file=log_file)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: setup_training_logging() got an unexpected keyword argument 'log_file'
__________ TestMetricsCollector.test_metrics_collector_initialization __________
tests/training/test_monitoring.py:102: in test_metrics_collector_initialization
    assert isinstance(collector.lock, threading.Lock)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union
_____________________ TestMetricsCollector.test_add_metric _____________________
tests/training/test_monitoring.py:113: in test_add_metric
    collector.add_metric(point)
    ^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'MetricsCollector' object has no attribute 'add_metric'
________________ TestMetricsCollector.test_add_metric_eviction _________________
tests/training/test_monitoring.py:130: in test_add_metric_eviction
    collector.add_metric(point)
    ^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'MetricsCollector' object has no attribute 'add_metric'
________________ TestMetricsCollector.test_get_metrics_by_name _________________
tests/training/test_monitoring.py:155: in test_get_metrics_by_name
    collector.add_metric(point)
    ^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'MetricsCollector' object has no attribute 'add_metric'
________________ TestMetricsCollector.test_get_metrics_by_tags _________________
tests/training/test_monitoring.py:184: in test_get_metrics_by_tags
    collector.add_metric(point)
    ^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'MetricsCollector' object has no attribute 'add_metric'
_________________ TestMetricsCollector.test_get_latest_metric __________________
tests/training/test_monitoring.py:208: in test_get_latest_metric
    collector.add_metric(point)
    ^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'MetricsCollector' object has no attribute 'add_metric'
_______________ TestMetricsCollector.test_get_metric_statistics ________________
tests/training/test_monitoring.py:224: in test_get_metric_statistics
    collector.add_metric(point)
    ^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'MetricsCollector' object has no attribute 'add_metric'
___________________ TestMetricsCollector.test_clear_metrics ____________________
tests/training/test_monitoring.py:243: in test_clear_metrics
    collector.add_metric(point)
    ^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'MetricsCollector' object has no attribute 'add_metric'
___________________ TestMetricsCollector.test_thread_safety ____________________
tests/training/test_monitoring.py:271: in test_thread_safety
    assert len(collector.metrics) == 30
E   assert 0 == 30
E    +  where 0 = len([])
E    +    where [] = <training.monitoring.MetricsCollector object at 0x17a410190>.metrics
_____________ TestHealthChecker.test_health_checker_initialization _____________
tests/training/test_monitoring.py:290: in test_health_checker_initialization
    assert len(health_checker.status_history) == 0
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'HealthChecker' object has no attribute 'status_history'
_______________________ TestHealthChecker.test_add_check _______________________
tests/training/test_monitoring.py:302: in test_add_check
    health_checker.add_check("test_check", dummy_check)
    ^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'HealthChecker' object has no attribute 'add_check'
_______________________ TestHealthChecker.test_run_check _______________________
tests/training/test_monitoring.py:317: in test_run_check
    health_checker.add_check("test_check", dummy_check)
    ^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'HealthChecker' object has no attribute 'add_check'
__________________ TestHealthChecker.test_run_check_not_found __________________
tests/training/test_monitoring.py:328: in test_run_check_not_found
    health_checker.run_check("nonexistent_check")
    ^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'HealthChecker' object has no attribute 'run_check'
____________________ TestHealthChecker.test_run_all_checks _____________________
tests/training/test_monitoring.py:342: in test_run_all_checks
    health_checker.add_check("check1", check1)
    ^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'HealthChecker' object has no attribute 'add_check'
_________________ TestHealthChecker.test_get_component_status __________________
tests/training/test_monitoring.py:362: in test_get_component_status
    health_checker.add_check("gpu_check", gpu_check)
    ^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'HealthChecker' object has no attribute 'add_check'
__________________ TestHealthChecker.test_get_overall_health ___________________
tests/training/test_monitoring.py:396: in test_get_overall_health
    health_checker.add_check("healthy", healthy_check)
    ^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'HealthChecker' object has no attribute 'add_check'
_______ TestSystemHealthChecks.test_system_health_checks_initialization ________
tests/training/test_monitoring.py:427: in test_system_health_checks_initialization
    assert len(system_checks.health_checker.checks) > 0
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'SystemHealthChecks' object has no attribute 'health_checker'
_________________ TestSystemHealthChecks.test_cpu_usage_check __________________
tests/training/test_monitoring.py:441: in test_cpu_usage_check
    status = system_checks._check_cpu_usage()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'SystemHealthChecks' object has no attribute '_check_cpu_usage'
________________ TestSystemHealthChecks.test_memory_usage_check ________________
tests/training/test_monitoring.py:454: in test_memory_usage_check
    status = system_checks._check_memory_usage()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'SystemHealthChecks' object has no attribute '_check_memory_usage'
_________________ TestSystemHealthChecks.test_gpu_check_no_gpu _________________
tests/training/test_monitoring.py:465: in test_gpu_check_no_gpu
    status = system_checks._check_gpu_usage()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'SystemHealthChecks' object has no attribute '_check_gpu_usage'
________________ TestSystemHealthChecks.test_gpu_check_with_gpu ________________
tests/training/test_monitoring.py:480: in test_gpu_check_with_gpu
    status = system_checks._check_gpu_usage()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'SystemHealthChecks' object has no attribute '_check_gpu_usage'
_________________ TestSystemHealthChecks.test_disk_usage_check _________________
tests/training/test_monitoring.py:488: in test_disk_usage_check
    status = system_checks._check_disk_usage()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'SystemHealthChecks' object has no attribute '_check_disk_usage'
____________ TestSystemHealthChecks.test_network_connectivity_check ____________
tests/training/test_monitoring.py:496: in test_network_connectivity_check
    status = system_checks._check_network_connectivity()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'SystemHealthChecks' object has no attribute '_check_network_connectivity'
________________ TestSystemHealthChecks.test_run_system_checks _________________
tests/training/test_monitoring.py:503: in test_run_system_checks
    statuses = system_checks.run_system_checks()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'SystemHealthChecks' object has no attribute 'run_system_checks'
___________ TestTrainingMonitor.test_training_monitor_initialization ___________
tests/training/test_monitoring.py:523: in test_training_monitor_initialization
    assert isinstance(training_monitor.metrics_collector, MetricsCollector)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'TrainingMonitor' object has no attribute 'metrics_collector'
__________________ TestTrainingMonitor.test_start_monitoring ___________________
tests/training/test_monitoring.py:530: in test_start_monitoring
    training_monitor.start_monitoring(interval=1.0)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'TrainingMonitor' object has no attribute 'start_monitoring'
___________________ TestTrainingMonitor.test_stop_monitoring ___________________
tests/training/test_monitoring.py:542: in test_stop_monitoring
    training_monitor.start_monitoring(interval=1.0)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'TrainingMonitor' object has no attribute 'start_monitoring'
_____________________ TestTrainingMonitor.test_log_metric ______________________
tests/training/test_monitoring.py:554: in test_log_metric
    training_monitor.log_metric("loss", 0.5, {"epoch": "1"})
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'TrainingMonitor' object has no attribute 'log_metric'
__________________ TestTrainingMonitor.test_log_training_step __________________
tests/training/test_monitoring.py:563: in test_log_training_step
    training_monitor.log_training_step(
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'TrainingMonitor' object has no attribute 'log_training_step'
_______________ TestTrainingMonitor.test_log_validation_metrics ________________
tests/training/test_monitoring.py:582: in test_log_validation_metrics
    training_monitor.log_validation_metrics(
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'TrainingMonitor' object has no attribute 'log_validation_metrics'
________________ TestTrainingMonitor.test_get_monitoring_stats _________________
tests/training/test_monitoring.py:609: in test_get_monitoring_stats
    training_monitor.log_metric("test_metric", 42.0)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'TrainingMonitor' object has no attribute 'log_metric'
__________________ TestTrainingMonitor.test_save_load_metrics __________________
tests/training/test_monitoring.py:621: in test_save_load_metrics
    training_monitor.log_metric("test_loss", 0.5)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'TrainingMonitor' object has no attribute 'log_metric'
_________________ TestTrainingMonitor.test_alert_on_condition __________________
tests/training/test_monitoring.py:646: in test_alert_on_condition
    training_monitor.add_alert_condition(
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'TrainingMonitor' object has no attribute 'add_alert_condition'
_________ TestConcurrencyAndPerformance.test_concurrent_metric_logging _________
tests/training/test_monitoring.py:720: in test_concurrent_metric_logging
    total_metrics = len(training_monitor.metrics_collector.metrics)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'TrainingMonitor' object has no attribute 'metrics_collector'
__________ TestConcurrencyAndPerformance.test_monitoring_performance ___________
tests/training/test_monitoring.py:731: in test_monitoring_performance
    training_monitor.log_metric(f"perf_metric_{i}", float(i % 100))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'TrainingMonitor' object has no attribute 'log_metric'
_ TestHeuristicQualityScore.test_compute_heuristic_quality_score_structured_content _
tests/training/test_quality_scoring.py:39: in test_compute_heuristic_quality_score_structured_content
    assert score > 0.5  # Should be higher due to code blocks
    ^^^^^^^^^^^^^^^^^^
E   assert 0.39999999999999997 > 0.5
_ TestHeuristicQualityScore.test_compute_heuristic_quality_score_ground_truth_comparison _
tests/training/test_quality_scoring.py:57: in test_compute_heuristic_quality_score_ground_truth_comparison
    assert score > 0.5  # Should be higher due to exact match
    ^^^^^^^^^^^^^^^^^^
E   assert 0.39999999999999997 > 0.5
____ TestJSONValidityScore.test_compute_json_validity_score_malformed_json _____
tests/training/test_quality_scoring.py:157: in test_compute_json_validity_score_malformed_json
    assert score < 1.0
E   assert 1.0 < 1.0
_________ TestCodeBlockScore.test_compute_code_block_score_python_code _________
tests/training/test_quality_scoring.py:181: in test_compute_code_block_score_python_code
    assert score > 0.8  # Should be high for well-structured Python code
    ^^^^^^^^^^^^^^^^^^
E   assert 0.7999999999999999 > 0.8
_______ TestCodeBlockScore.test_compute_code_block_score_mixed_languages _______
tests/training/test_quality_scoring.py:220: in test_compute_code_block_score_mixed_languages
    assert score > 0.8  # Should be high for multiple code blocks
    ^^^^^^^^^^^^^^^^^^
E   assert 0.35 > 0.8
______ TestCodeBlockScore.test_compute_code_block_score_empty_code_block _______
tests/training/test_quality_scoring.py:250: in test_compute_code_block_score_empty_code_block
    assert score < 0.3  # Should be low for empty blocks
    ^^^^^^^^^^^^^^^^^^
E   assert 0.5 < 0.3
_ TestCompositeQualityScore.test_compute_composite_quality_score_all_components _
tests/training/test_quality_scoring.py:281: in test_compute_composite_quality_score_all_components
    assert "heuristic" in scores
           ^^^^^^^^^^^^^^^^^^^^^
E   TypeError: argument of type 'float' is not iterable
_ TestCompositeQualityScore.test_compute_composite_quality_score_minimal_text __
tests/training/test_quality_scoring.py:294: in test_compute_composite_quality_score_minimal_text
    assert isinstance(scores, dict)
E   assert False
E    +  where False = isinstance(0.12, dict)
_ TestCompositeQualityScore.test_compute_composite_quality_score_structured_content _
tests/training/test_quality_scoring.py:323: in test_compute_composite_quality_score_structured_content
    assert scores["composite"] > 0.8
           ^^^^^^^^^^^^^^^^^^^
E   TypeError: 'float' object is not subscriptable
____ TestCompositeQualityScore.test_compute_composite_quality_score_weights ____
tests/training/test_quality_scoring.py:332: in test_compute_composite_quality_score_weights
    assert scores["json_validity"] > 0.8
           ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'float' object is not subscriptable
__ TestCompositeQualityScore.test_compute_composite_quality_score_empty_text ___
tests/training/test_quality_scoring.py:339: in test_compute_composite_quality_score_empty_text
    assert scores["composite"] == 0.0
           ^^^^^^^^^^^^^^^^^^^
E   TypeError: 'float' object is not subscriptable
____ TestBatchQualityScoring.test_batch_compute_quality_scores_single_item _____
tests/training/test_quality_scoring.py:359: in test_batch_compute_quality_scores_single_item
    assert isinstance(results[0], dict)
E   assert False
E    +  where False = isinstance(0.12, dict)
___ TestBatchQualityScoring.test_batch_compute_quality_scores_multiple_items ___
tests/training/test_quality_scoring.py:390: in test_batch_compute_quality_scores_multiple_items
    assert isinstance(result, dict)
E   assert False
E    +  where False = isinstance(0.12, dict)
___ TestBatchQualityScoring.test_batch_compute_quality_scores_mixed_content ____
tests/training/test_quality_scoring.py:422: in test_batch_compute_quality_scores_mixed_content
    scores = [r["composite"] for r in results]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/training/test_quality_scoring.py:422: in <listcomp>
    scores = [r["composite"] for r in results]
              ^^^^^^^^^^^^^^
E   TypeError: 'float' object is not subscriptable
___ TestBatchQualityScoring.test_batch_compute_quality_scores_error_handling ___
tests/training/test_quality_scoring.py:438: in test_batch_compute_quality_scores_error_handling
    results = batch_compute_quality_scores(texts)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/quality_scoring.py:263: in batch_compute_quality_scores
    score = compute_composite_quality_score(output, ground_truth, prompt)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/quality_scoring.py:221: in compute_composite_quality_score
    scores["json_validity"] = compute_json_validity_score(teacher_output)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/quality_scoring.py:134: in compute_json_validity_score
    matches = re.findall(json_pattern, text)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/re/__init__.py:216: in findall
    return _compile(pattern, flags).findall(string)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: expected string or bytes-like object, got 'NoneType'
____ TestBatchQualityScoring.test_batch_compute_quality_scores_large_batch _____
tests/training/test_quality_scoring.py:459: in test_batch_compute_quality_scores_large_batch
    assert "composite" in result
           ^^^^^^^^^^^^^^^^^^^^^
E   TypeError: argument of type 'float' is not iterable
____ TestQualityScoringIntegration.test_quality_scoring_realistic_examples _____
tests/training/test_quality_scoring.py:501: in test_quality_scoring_realistic_examples
    scores = [r["composite"] for r in results]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/training/test_quality_scoring.py:501: in <listcomp>
    scores = [r["composite"] for r in results]
              ^^^^^^^^^^^^^^
E   TypeError: 'float' object is not subscriptable
_________ TestSHA256StateDict.test_sha256_state_dict_buffer_operations _________
tests/training/test_utils.py:168: in test_sha256_state_dict_buffer_operations
    result = sha256_state_dict(state_dict)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/utils.py:64: in sha256_state_dict
    return hashlib.sha256(content).hexdigest()
           ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: object supporting the buffer API required
__________ TestANEResidencyMonitor.test_get_model_ops_info_with_model __________
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'coreml.runtime.ane_monitor' from '/Users/darianrosebrook/Desktop/Projects/distill/coreml/runtime/ane_monitor.py'> does not have the attribute 'ct'
____________________ test_code_mode_loss_is_differentiable _____________________
tests/unit/test_code_mode_loss.py:90: in test_code_mode_loss_is_differentiable
    assert isinstance(loss, torch.Tensor), "Loss should be a tensor"
E   AssertionError: Loss should be a tensor
E   assert False
E    +  where False = isinstance(None, <class 'torch.Tensor'>)
E    +    where <class 'torch.Tensor'> = torch.Tensor
__________________ test_code_mode_loss_eligibility_filtering ___________________
tests/unit/test_code_mode_loss.py:137: in test_code_mode_loss_eligibility_filtering
    assert isinstance(loss, torch.Tensor)
E   AssertionError: assert False
E    +  where False = isinstance(None, <class 'torch.Tensor'>)
E    +    where <class 'torch.Tensor'> = torch.Tensor
_______________________ test_code_mode_loss_torchscript ________________________
tests/unit/test_code_mode_loss_torchscript.py:54: in test_code_mode_loss_torchscript
    scripted_module = torch.jit.script(loss_module)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/jit/_script.py:1338: in script
    return torch.jit._recursive.create_script_module(
venv/lib/python3.11/site-packages/torch/jit/_recursive.py:558: in create_script_module
    return create_script_module_impl(nn_module, concrete_type, stubs_fn)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/jit/_recursive.py:635: in create_script_module_impl
    create_methods_and_properties_from_stubs(
venv/lib/python3.11/site-packages/torch/jit/_recursive.py:467: in create_methods_and_properties_from_stubs
    concrete_type._create_methods_and_properties(
venv/lib/python3.11/site-packages/torch/jit/_recursive.py:1036: in compile_unbound_method
    create_methods_and_properties_from_stubs(concrete_type, (stub,), ())
venv/lib/python3.11/site-packages/torch/jit/_recursive.py:467: in create_methods_and_properties_from_stubs
    concrete_type._create_methods_and_properties(
E   RuntimeError: 
E   Arguments for call are not valid.
E   The following variants are available:
E     
E     aten::ge.Tensor(Tensor self, Tensor other) -> Tensor:
E     Expected a value of type 'Tensor' for argument 'self' but instead found type 'Any'.
E     
E     aten::ge.Scalar(Tensor self, Scalar other) -> Tensor:
E     Expected a value of type 'Tensor' for argument 'self' but instead found type 'Any'.
E     
E     aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!):
E     Expected a value of type 'Tensor' for argument 'self' but instead found type 'Any'.
E     
E     aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!):
E     Expected a value of type 'Tensor' for argument 'self' but instead found type 'Any'.
E     
E     aten::ge.int(int a, int b) -> bool:
E     Expected a value of type 'int' for argument 'a' but instead found type 'Any'.
E     
E     aten::ge.float(float a, float b) -> bool:
E     Expected a value of type 'float' for argument 'a' but instead found type 'Any'.
E     
E     aten::ge.int_float(int a, float b) -> bool:
E     Expected a value of type 'int' for argument 'a' but instead found type 'Any'.
E     
E     aten::ge.float_int(float a, int b) -> bool:
E     Expected a value of type 'float' for argument 'a' but instead found type 'Any'.
E     
E     aten::ge(Scalar a, Scalar b) -> bool:
E     Expected a value of type 'number' for argument 'a' but instead found type 'Any'.
E     
E     aten::ge.str(str a, str b) -> bool:
E     Expected a value of type 'str' for argument 'a' but instead found type 'Any'.
E     
E     ge(float a, Tensor b) -> Tensor:
E     Expected a value of type 'float' for argument 'a' but instead found type 'Any'.
E     
E     ge(int a, Tensor b) -> Tensor:
E     Expected a value of type 'int' for argument 'a' but instead found type 'Any'.
E   
E   The original call is:
E     File "/Users/darianrosebrook/Desktop/Projects/distill/training/losses.py", line 658
E       
E               eligible = (
E                   tool_count >= self.min_tools
E                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
E                   or (intermediate_sizes and max(intermediate_sizes) >= self.min_intermediate_chars)
E                   or pii_tags_present
E   'CodeModePreferenceLoss._compute_eligibility_mask' is being compiled since it was called from 'CodeModePreferenceLoss.forward'
E     File "/Users/darianrosebrook/Desktop/Projects/distill/training/losses.py", line 689
E           # Check eligibility for the batch
E           batch_size = len(batch_meta)
E           eligibility_mask = self._compute_eligibility_mask(
E           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
E               batch_meta, batch_size)
E               ~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
E       
E           # If no items in batch are eligible, return zero loss

During handling of the above exception, another exception occurred:
tests/unit/test_code_mode_loss_torchscript.py:57: in test_code_mode_loss_torchscript
    pytest.fail(f"TorchScript compilation failed: {e}")
E   Failed: TorchScript compilation failed: 
E   Arguments for call are not valid.
E   The following variants are available:
E     
E     aten::ge.Tensor(Tensor self, Tensor other) -> Tensor:
E     Expected a value of type 'Tensor' for argument 'self' but instead found type 'Any'.
E     
E     aten::ge.Scalar(Tensor self, Scalar other) -> Tensor:
E     Expected a value of type 'Tensor' for argument 'self' but instead found type 'Any'.
E     
E     aten::ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!):
E     Expected a value of type 'Tensor' for argument 'self' but instead found type 'Any'.
E     
E     aten::ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!):
E     Expected a value of type 'Tensor' for argument 'self' but instead found type 'Any'.
E     
E     aten::ge.int(int a, int b) -> bool:
E     Expected a value of type 'int' for argument 'a' but instead found type 'Any'.
E     
E     aten::ge.float(float a, float b) -> bool:
E     Expected a value of type 'float' for argument 'a' but instead found type 'Any'.
E     
E     aten::ge.int_float(int a, float b) -> bool:
E     Expected a value of type 'int' for argument 'a' but instead found type 'Any'.
E     
E     aten::ge.float_int(float a, int b) -> bool:
E     Expected a value of type 'float' for argument 'a' but instead found type 'Any'.
E     
E     aten::ge(Scalar a, Scalar b) -> bool:
E     Expected a value of type 'number' for argument 'a' but instead found type 'Any'.
E     
E     aten::ge.str(str a, str b) -> bool:
E     Expected a value of type 'str' for argument 'a' but instead found type 'Any'.
E     
E     ge(float a, Tensor b) -> Tensor:
E     Expected a value of type 'float' for argument 'a' but instead found type 'Any'.
E     
E     ge(int a, Tensor b) -> Tensor:
E     Expected a value of type 'int' for argument 'a' but instead found type 'Any'.
E   
E   The original call is:
E     File "/Users/darianrosebrook/Desktop/Projects/distill/training/losses.py", line 658
E       
E               eligible = (
E                   tool_count >= self.min_tools
E                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
E                   or (intermediate_sizes and max(intermediate_sizes) >= self.min_intermediate_chars)
E                   or pii_tags_present
E   'CodeModePreferenceLoss._compute_eligibility_mask' is being compiled since it was called from 'CodeModePreferenceLoss.forward'
E     File "/Users/darianrosebrook/Desktop/Projects/distill/training/losses.py", line 689
E           # Check eligibility for the batch
E           batch_size = len(batch_meta)
E           eligibility_mask = self._compute_eligibility_mask(
E           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
E               batch_meta, batch_size)
E               ~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
E       
E           # If no items in batch are eligible, return zero loss
__ TestConfigSchemaValidation.test_validate_training_config_missing_required ___
tests/unit/test_config_validation.py:94: in test_validate_training_config_missing_required
    assert any("d_model" in error.lower() or "n_layers" in error.lower() for error in errors)
E   assert False
E    +  where False = any(<generator object TestConfigSchemaValidation.test_validate_training_config_missing_required.<locals>.<genexpr> at 0x17a892180>)
_______ TestConfigSchemaValidation.test_validate_config_file_nonexistent _______
tests/unit/test_config_validation.py:180: in test_validate_config_file_nonexistent
    with pytest.raises(FileNotFoundError):
E   Failed: DID NOT RAISE <class 'FileNotFoundError'>
__________________ TestConfigMerging.test_merge_configs_basic __________________
tests/unit/test_config_validation.py:271: in test_merge_configs_basic
    merged = merge_configs(files)
             ^^^^^^^^^^^^^^^^^^^^
training/config_validation.py:244: in merge_configs
    raise ValueError(f"Invalid configuration in {config_path}: {'; '.join(errors)}")
E   ValueError: Invalid configuration in /var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/tmp8jp03bdz.yaml: Configuration validation error: 'training' is a required property
----------------------------- Captured stdout call -----------------------------
 Configuration validation failed: /var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/tmp8jp03bdz.yaml
   Configuration validation error: 'training' is a required property
________________ TestConfigMerging.test_merge_configs_overrides ________________
tests/unit/test_config_validation.py:299: in test_merge_configs_overrides
    merged = merge_configs(files)
             ^^^^^^^^^^^^^^^^^^^^
training/config_validation.py:244: in merge_configs
    raise ValueError(f"Invalid configuration in {config_path}: {'; '.join(errors)}")
E   ValueError: Invalid configuration in /var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/tmpiq_fwoei.yaml: Configuration validation error: 'training' is a required property
----------------------------- Captured stdout call -----------------------------
 Configuration validation failed: /var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/tmpiq_fwoei.yaml
   Configuration validation error: 'training' is a required property
______________ TestConfigMerging.test_merge_configs_invalid_file _______________
tests/unit/test_config_validation.py:321: in test_merge_configs_invalid_file
    merge_configs([invalid_file])
training/config_validation.py:244: in merge_configs
    raise ValueError(f"Invalid configuration in {config_path}: {'; '.join(errors)}")
E   ValueError: Invalid configuration in /var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/tmphr0wwehp.yaml: Failed to parse configuration file: mapping values are not allowed here
E     in "/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/tmphr0wwehp.yaml", line 1, column 14
_______________ TestDecoderIntegration.test_with_real_tokenizer ________________
tests/unit/test_constrained_decode.py:641: in test_with_real_tokenizer
    obj = decoder.finalize(state)
          ^^^^^^^^^^^^^^^^^^^^^^^
coreml/runtime/constrained_decode.py:405: in finalize
    obj = json.loads(st.buffer)
          ^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346: in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337: in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:355: in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
E   json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
________________ TestVerifyContextualSet.test_privacy_scanning _________________
tests/unit/test_contextual_generation.py:726: in test_privacy_scanning
    assert result["privacy_ok"] is False
E   assert True is False
__________ TestVerifyContextualSet.test_check_privacy_multiple_emails __________
tests/unit/test_contextual_generation.py:2604: in test_check_privacy_multiple_emails
    assert result["privacy_ok"] is False
E   assert True is False
__________ TestVerifyContextualSet.test_check_privacy_multiple_uuids ___________
tests/unit/test_contextual_generation.py:2613: in test_check_privacy_multiple_uuids
    assert result["privacy_ok"] is False
E   assert True is False
____________ TestAnswerGenerationDataset.test_dataset_getitem_basic ____________
tests/unit/test_dataset_modules.py:116: in test_dataset_getitem_basic
    item = dataset[0]
           ^^^^^^^^^^
training/dataset_answer_generation.py:133: in __getitem__
    prompt_len = prompt_tokens["input_ids"].shape[1]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'function' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
[AnswerGenerationDataset] Loaded 1 examples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_dataset_getitem_basic0/answer_gen.jsonl
__________ TestAnswerGenerationDataset.test_dataset_prompt_formatting __________
tests/unit/test_dataset_modules.py:152: in test_dataset_prompt_formatting
    item = dataset[0]
           ^^^^^^^^^^
training/dataset_answer_generation.py:133: in __getitem__
    prompt_len = prompt_tokens["input_ids"].shape[1]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'function' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
[AnswerGenerationDataset] Loaded 1 examples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_dataset_prompt_formatting0/test.jsonl
_____________ TestAnswerGenerationDataset.test_dataset_empty_tools _____________
tests/unit/test_dataset_modules.py:175: in test_dataset_empty_tools
    item = dataset[0]
           ^^^^^^^^^^
training/dataset_answer_generation.py:133: in __getitem__
    prompt_len = prompt_tokens["input_ids"].shape[1]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'function' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
[AnswerGenerationDataset] Loaded 1 examples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_dataset_empty_tools0/test.jsonl
___________ TestAnswerGenerationDataset.test_dataset_missing_fields ____________
tests/unit/test_dataset_modules.py:194: in test_dataset_missing_fields
    item = dataset[0]
           ^^^^^^^^^^
training/dataset_answer_generation.py:133: in __getitem__
    prompt_len = prompt_tokens["input_ids"].shape[1]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'function' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
[AnswerGenerationDataset] Loaded 1 examples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_dataset_missing_fields0/test.jsonl
___________________ TestPostToolDataset.test_dataset_getitem ___________________
tests/unit/test_dataset_modules.py:293: in test_dataset_getitem
    item = dataset[0]
           ^^^^^^^^^^
training/dataset_post_tool.py:129: in __getitem__
    prompt_len = prompt_tokens["input_ids"].shape[1]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'function' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
[PostToolDataset] Loaded 1 examples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_dataset_getitem0/post_tool.jsonl
___________ TestPostToolDataset.test_dataset_tool_result_formatting ____________
tests/unit/test_dataset_modules.py:321: in test_dataset_tool_result_formatting
    item = dataset[0]
           ^^^^^^^^^^
training/dataset_post_tool.py:129: in __getitem__
    prompt_len = prompt_tokens["input_ids"].shape[1]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'function' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
[PostToolDataset] Loaded 1 examples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_dataset_tool_result_forma0/test.jsonl
__________________ TestToolSelectDataset.test_dataset_getitem __________________
tests/unit/test_dataset_modules.py:436: in test_dataset_getitem
    item = dataset[0]
           ^^^^^^^^^^
training/dataset_tool_select.py:137: in __getitem__
    labels = target_encoding["input_ids"].squeeze(0).clone()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'function' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
[ToolSelectDataset] Loaded 1 examples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_dataset_getitem1/tool_select.jsonl
______________ TestToolSelectDataset.test_dataset_tool_formatting ______________
tests/unit/test_dataset_modules.py:474: in test_dataset_tool_formatting
    item = dataset[0]
           ^^^^^^^^^^
training/dataset_tool_select.py:137: in __getitem__
    labels = target_encoding["input_ids"].squeeze(0).clone()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'function' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
[ToolSelectDataset] Loaded 1 examples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_dataset_tool_formatting0/test.jsonl
________________ TestToolSelectDataset.test_dataset_empty_tools ________________
tests/unit/test_dataset_modules.py:501: in test_dataset_empty_tools
    item = dataset[0]
           ^^^^^^^^^^
training/dataset_tool_select.py:137: in __getitem__
    labels = target_encoding["input_ids"].squeeze(0).clone()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'function' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
[ToolSelectDataset] Loaded 1 examples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_dataset_empty_tools1/test.jsonl
______________ TestToolSelectDataset.test_dataset_json_formatting ______________
tests/unit/test_dataset_modules.py:558: in test_dataset_json_formatting
    item = dataset[0]
           ^^^^^^^^^^
training/dataset_tool_select.py:137: in __getitem__
    labels = target_encoding["input_ids"].squeeze(0).clone()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'function' object is not subscriptable
----------------------------- Captured stdout call -----------------------------
[ToolSelectDataset] Loaded 1 examples from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_dataset_json_formatting0/test.jsonl
_________ TestDatasetErrorHandling.test_answer_generation_invalid_json _________
tests/unit/test_dataset_modules.py:578: in test_answer_generation_invalid_json
    dataset = AnswerGenerationDataset(str(data_file), "dummy_path")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/dataset_answer_generation.py:60: in __init__
    example = json.loads(line)
              ^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346: in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337: in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:355: in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
E   json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
______________ TestSampleEnumeratedShape.test_custom_shape_probs _______________
tests/unit/test_enumerated_shapes.py:45: in test_custom_shape_probs
    assert count_512 > 60  # Should be ~70% of 100 samples
    ^^^^^^^^^^^^^^^^^^^^^
E   assert 57 > 60
______________________ TestMHAGQA.test_mha_gqa_with_mask _______________________
tests/unit/test_gqa_transformer.py:189: in test_mha_gqa_with_mask
    output = mha(x, attn_mask)
             ^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:186: in forward
    y = y.transpose(1, 2).contiguous().view(b, t, self.n_heads * self.d_head)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: shape '[2, 10, 128]' is invalid for input of size 5120
________________ TestStudentLM.test_studentlm_forward_with_mask ________________
tests/unit/test_gqa_transformer.py:315: in test_studentlm_forward_with_mask
    logits = small_model(input_ids, attn_mask)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:364: in forward
    x = blk(x, attn_mask)
        ^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:259: in forward
    x = x + self.attn(self.norm1(x), attn_mask)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
models/student/architectures/gqa_transformer.py:186: in forward
    y = y.transpose(1, 2).contiguous().view(b, t, self.n_heads * self.d_head)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   RuntimeError: shape '[2, 10, 128]' is invalid for input of size 5120
__________ TestTextInputValidation.test_validate_text_input_too_long ___________
tests/unit/test_input_validation.py:80: in test_validate_text_input_too_long
    validator.validate_text_input(long_text)
training/input_validation.py:75: in validate_text_input
    raise ValidationError(error_msg)
E   training.input_validation.ValidationError: text too long: 50001 characters

During handling of the above exception, another exception occurred:
tests/unit/test_input_validation.py:79: in test_validate_text_input_too_long
    with pytest.raises(ValidationError, match="exceeds maximum length"):
E   AssertionError: Regex pattern did not match.
E     Expected regex: 'exceeds maximum length'
E     Actual message: 'text too long: 50001 characters'
______ TestTextInputValidation.test_validate_text_input_suspicious_script ______
tests/unit/test_input_validation.py:92: in test_validate_text_input_suspicious_script
    validator.validate_text_input(suspicious_text)
training/input_validation.py:87: in validate_text_input
    raise ValidationError(error_msg)
E   training.input_validation.ValidationError: Suspicious content detected in text

During handling of the above exception, another exception occurred:
tests/unit/test_input_validation.py:91: in test_validate_text_input_suspicious_script
    with pytest.raises(ValidationError, match="suspicious content"):
E   AssertionError: Regex pattern did not match.
E     Expected regex: 'suspicious content'
E     Actual message: 'Suspicious content detected in text'
_______ TestTextInputValidation.test_validate_text_input_javascript_url ________
tests/unit/test_input_validation.py:103: in test_validate_text_input_javascript_url
    validator.validate_text_input(suspicious_text)
training/input_validation.py:87: in validate_text_input
    raise ValidationError(error_msg)
E   training.input_validation.ValidationError: Suspicious content detected in text

During handling of the above exception, another exception occurred:
tests/unit/test_input_validation.py:102: in test_validate_text_input_javascript_url
    with pytest.raises(ValidationError, match="suspicious content"):
E   AssertionError: Regex pattern did not match.
E     Expected regex: 'suspicious content'
E     Actual message: 'Suspicious content detected in text'
________ TestTextInputValidation.test_validate_text_input_event_handler ________
tests/unit/test_input_validation.py:110: in test_validate_text_input_event_handler
    validator.validate_text_input(suspicious_text)
training/input_validation.py:87: in validate_text_input
    raise ValidationError(error_msg)
E   training.input_validation.ValidationError: Suspicious content detected in text

During handling of the above exception, another exception occurred:
tests/unit/test_input_validation.py:109: in test_validate_text_input_event_handler
    with pytest.raises(ValidationError, match="suspicious content"):
E   AssertionError: Regex pattern did not match.
E     Expected regex: 'suspicious content'
E     Actual message: 'Suspicious content detected in text'
_________ TestTextInputValidation.test_validate_text_input_none_input __________
tests/unit/test_input_validation.py:124: in test_validate_text_input_none_input
    validator.validate_text_input(None)
training/input_validation.py:66: in validate_text_input
    raise ValidationError(error_msg)
E   training.input_validation.ValidationError: text must be a string, got <class 'NoneType'>

During handling of the above exception, another exception occurred:
tests/unit/test_input_validation.py:123: in test_validate_text_input_none_input
    with pytest.raises(ValidationError, match="cannot be None"):
E   AssertionError: Regex pattern did not match.
E     Expected regex: 'cannot be None'
E     Actual message: "text must be a string, got <class 'NoneType'>"
_______ TestStructuredDataValidation.test_validate_structured_data_valid _______
tests/unit/test_input_validation.py:148: in test_validate_structured_data_valid
    result = validator.validate_structured_data(data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute 'validate_structured_data'
_ TestStructuredDataValidation.test_validate_structured_data_missing_required __
tests/unit/test_input_validation.py:160: in test_validate_structured_data_missing_required
    validator.validate_structured_data(data)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute 'validate_structured_data'
___ TestStructuredDataValidation.test_validate_structured_data_invalid_types ___
tests/unit/test_input_validation.py:171: in test_validate_structured_data_invalid_types
    validator.validate_structured_data(data)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute 'validate_structured_data'
_ TestStructuredDataValidation.test_validate_structured_data_suspicious_content _
tests/unit/test_input_validation.py:182: in test_validate_structured_data_suspicious_content
    validator.validate_structured_data(data)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute 'validate_structured_data'
_____ TestStructuredDataValidation.test_validate_structured_data_too_long ______
tests/unit/test_input_validation.py:193: in test_validate_structured_data_too_long
    validator.validate_structured_data(data)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute 'validate_structured_data'
_________________ TestToolValidation.test_validate_tools_valid _________________
tests/unit/test_input_validation.py:211: in test_validate_tools_valid
    result = validator.validate_tools(tools)
             ^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute 'validate_tools'
_______________ TestToolValidation.test_validate_tools_too_many ________________
tests/unit/test_input_validation.py:220: in test_validate_tools_too_many
    validator.validate_tools(tools)
    ^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute 'validate_tools'
____________ TestToolValidation.test_validate_tools_missing_fields _____________
tests/unit/test_input_validation.py:230: in test_validate_tools_missing_fields
    validator.validate_tools(tools)
    ^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute 'validate_tools'
________ TestToolValidation.test_validate_tools_suspicious_description _________
tests/unit/test_input_validation.py:239: in test_validate_tools_suspicious_description
    validator.validate_tools(tools)
    ^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute 'validate_tools'
_______________ TestFileValidation.test_validate_file_path_valid _______________
tests/unit/test_input_validation.py:256: in test_validate_file_path_valid
    assert result == str(test_file)
E   AssertionError: assert PosixPath('/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_validate_file_path_valid0/test.txt') == '/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_validate_file_path_valid0/test.txt'
E    +  where '/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_validate_file_path_valid0/test.txt' = str(PosixPath('/private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-17/test_validate_file_path_valid0/test.txt'))
____________ TestFileValidation.test_validate_file_path_nonexistent ____________
tests/unit/test_input_validation.py:263: in test_validate_file_path_nonexistent
    validator.validate_file_path(nonexistent_path)
training/input_validation.py:329: in validate_file_path
    raise ValidationError(f"File does not exist: {path}")
E   training.input_validation.ValidationError: File does not exist: /nonexistent/file.txt

During handling of the above exception, another exception occurred:
tests/unit/test_input_validation.py:262: in test_validate_file_path_nonexistent
    with pytest.raises(ValidationError, match="file does not exist"):
E   AssertionError: Regex pattern did not match.
E     Expected regex: 'file does not exist'
E     Actual message: 'File does not exist: /nonexistent/file.txt'
_____________ TestFileValidation.test_validate_file_path_too_large _____________
tests/unit/test_input_validation.py:273: in test_validate_file_path_too_large
    validator.validate_file_path(str(large_file))
training/input_validation.py:335: in validate_file_path
    raise ValidationError(
E   training.input_validation.ValidationError: File too large: 100.0MB > 100MB

During handling of the above exception, another exception occurred:
tests/unit/test_input_validation.py:272: in test_validate_file_path_too_large
    with pytest.raises(ValidationError, match="file too large"):
E   AssertionError: Regex pattern did not match.
E     Expected regex: 'file too large'
E     Actual message: 'File too large: 100.0MB > 100MB'
_________ TestFileValidation.test_validate_file_path_permission_error __________
tests/unit/test_input_validation.py:281: in test_validate_file_path_permission_error
    validator.validate_file_path("/some/file.txt")
training/input_validation.py:328: in validate_file_path
    if must_exist and not path.exists():
                          ^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pathlib.py:1235: in exists
    self.stat()
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1124: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1128: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1183: in _execute_mock_call
    raise effect
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pathlib.py:1003: in resolve
    p.stat()
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1124: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1128: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/Cellar/python@3.11/3.11.14_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1183: in _execute_mock_call
    raise effect
E   PermissionError: Permission denied
_________ TestTrainingDataValidation.test_validate_training_data_valid _________
tests/unit/test_input_validation.py:303: in test_validate_training_data_valid
    assert result == data
E   AssertionError: assert [{'metadata':...pt': 'Hello'}] == [{'metadata':... 'Hi there!'}]
E     
E     At index 0 diff: {'prompt': 'What is 2+2?', 'metadata': {}} != {'prompt': 'What is 2+2?', 'response': '4', 'metadata': {'source': 'test'}}
E     
E     Full diff:
E       [
E           {
E     -         'metadata': {...
E     
E     ...Full output truncated (17 lines hidden), use '-vv' to show
___ TestTrainingDataValidation.test_validate_training_data_invalid_structure ___
tests/unit/test_input_validation.py:315: in test_validate_training_data_invalid_structure
    with pytest.raises(ValidationError):
E   Failed: DID NOT RAISE <class 'training.input_validation.ValidationError'>
__________ TestTrainingDataValidation.test_validate_tool_trace_valid ___________
training/input_validation.py:389: in validate_tool_trace
    validated_call = validator.validate_tool_call(tool_call)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/input_validation.py:145: in validate_tool_call
    raise ValidationError(f"Tool call missing required field: {field}")
E   training.input_validation.ValidationError: Tool call missing required field: name

During handling of the above exception, another exception occurred:
tests/unit/test_input_validation.py:334: in test_validate_tool_trace_valid
    result = validate_tool_trace(trace)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
training/input_validation.py:392: in validate_tool_trace
    raise ValidationError(f"Tool call {i}: {e}")
E   training.input_validation.ValidationError: Tool call 0: Tool call missing required field: name
_______ TestSecurityPatterns.test_detect_suspicious_patterns_script_tags _______
tests/unit/test_input_validation.py:362: in test_detect_suspicious_patterns_script_tags
    assert validator._contains_suspicious_patterns(text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute '_contains_suspicious_patterns'
_____ TestSecurityPatterns.test_detect_suspicious_patterns_javascript_url ______
tests/unit/test_input_validation.py:367: in test_detect_suspicious_patterns_javascript_url
    assert validator._contains_suspicious_patterns(text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute '_contains_suspicious_patterns'
_________ TestSecurityPatterns.test_detect_suspicious_patterns_iframe __________
tests/unit/test_input_validation.py:372: in test_detect_suspicious_patterns_iframe
    assert validator._contains_suspicious_patterns(text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute '_contains_suspicious_patterns'
______ TestSecurityPatterns.test_detect_suspicious_patterns_event_handler ______
tests/unit/test_input_validation.py:377: in test_detect_suspicious_patterns_event_handler
    assert validator._contains_suspicious_patterns(text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute '_contains_suspicious_patterns'
_________ TestSecurityPatterns.test_no_suspicious_patterns_normal_text _________
tests/unit/test_input_validation.py:382: in test_no_suspicious_patterns_normal_text
    assert not validator._contains_suspicious_patterns(text)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute '_contains_suspicious_patterns'
________ TestSecurityPatterns.test_suspicious_patterns_case_insensitive ________
tests/unit/test_input_validation.py:387: in test_suspicious_patterns_case_insensitive
    assert validator._contains_suspicious_patterns(text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'InputValidator' object has no attribute '_contains_suspicious_patterns'
____________________ test_length_kd_completeness_exemption _____________________
tests/unit/test_losses_speed.py:21: in test_length_kd_completeness_exemption
    loss, d = length_aware_kd_loss(s_mask, t_mask, required, hinge=0.15, slope=1.0)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: length_aware_kd_loss() got an unexpected keyword argument 'hinge'
___________________________ test_length_kd_no_excess ___________________________
tests/unit/test_losses_speed.py:36: in test_length_kd_no_excess
    loss, d = length_aware_kd_loss(s_mask, t_mask, required, hinge=0.15, slope=1.0)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: length_aware_kd_loss() got an unexpected keyword argument 'hinge'
_____________________________ test_length_kd_hinge _____________________________
tests/unit/test_losses_speed.py:51: in test_length_kd_hinge
    loss, d = length_aware_kd_loss(s_mask, t_mask, required, hinge=0.15, slope=1.0)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: length_aware_kd_loss() got an unexpected keyword argument 'hinge'
_____________________ test_early_tool_ce_only_when_needed ______________________
tests/unit/test_losses_speed.py:73: in test_early_tool_ce_only_when_needed
    loss, d = early_tool_call_loss(
E   TypeError: early_tool_call_loss() got an unexpected keyword argument 'tokenizer'
_____________________ test_early_tool_json_prior_fallback ______________________
tests/unit/test_losses_speed.py:103: in test_early_tool_json_prior_fallback
    loss, d = early_tool_call_loss(
E   TypeError: early_tool_call_loss() got an unexpected keyword argument 'tokenizer'
____________________ test_early_tool_masked_when_not_needed ____________________
tests/unit/test_losses_speed.py:135: in test_early_tool_masked_when_not_needed
    loss, d = early_tool_call_loss(
E   TypeError: early_tool_call_loss() got an unexpected keyword argument 'tokenizer'
_____________________________ test_early_tool_ramp _____________________________
tests/unit/test_losses_speed.py:167: in test_early_tool_ramp
    loss_zero, _ = early_tool_call_loss(
E   TypeError: early_tool_call_loss() got an unexpected keyword argument 'tokenizer'
_____________ TestCheckQATStability.test_stability_check_with_nan ______________
tests/unit/test_qat_integration.py:136: in test_stability_check_with_nan
    assert metrics["qat_stability.has_nan"] == 1.0
E   assert 0.0 == 1.0
_____________ TestToolSchemaRegistry.test_validate_tool_call_valid _____________
tests/unit/test_schema_registry.py:69: in test_validate_tool_call_valid
    assert is_valid is True
E   assert False is True
_________ TestToolSchemaRegistry.test_validate_tool_call_name_mismatch _________
tests/unit/test_schema_registry.py:96: in test_validate_tool_call_name_mismatch
    assert "mismatch" in error.lower() or "name" in error.lower()
E   AssertionError: assert ('mismatch' in 'missing required key: q' or 'name' in 'missing required key: q')
E    +  where 'missing required key: q' = <built-in method lower of str object at 0x18b577fa0>()
E    +    where <built-in method lower of str object at 0x18b577fa0> = 'Missing required key: q'.lower
E    +  and   'missing required key: q' = <built-in method lower of str object at 0x18b577fa0>()
E    +    where <built-in method lower of str object at 0x18b577fa0> = 'Missing required key: q'.lower
_____________________ TestPIIRedaction.test_uuid_detection _____________________
tests/unit/test_security_contextual.py:94: in test_uuid_detection
    assert result["privacy_ok"] is False
E   assert True is False
____________________ TestPIIRedaction.test_email_detection _____________________
tests/unit/test_security_contextual.py:104: in test_email_detection
    assert result["privacy_ok"] is False
E   assert True is False
__________________ TestSpeculativeDecoder.test_draft_k_tokens __________________
tests/unit/test_speculative_decode.py:146: in test_draft_k_tokens
    assert len(draft_tokens) == 3
E   assert 2 == 3
E    +  where 2 = len(([100, 100, 100], [array([0., 0., 0., ..., 0., 0., 0.]), array([0., 0., 0., ..., 0., 0., 0.]), array([0., 0., 0., ..., 0., 0., 0.])]))
_______________ TestSpeculativeDecoder.test_verify_tokens_accept _______________
tests/unit/test_speculative_decode.py:174: in test_verify_tokens_accept
    accepted = decoder._verify_tokens(draft_tokens, input_ids, worker_state)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: SpeculativeDecoder._verify_tokens() missing 1 required positional argument: 'worker_state'
_______________ TestSpeculativeDecoder.test_verify_tokens_reject _______________
tests/unit/test_speculative_decode.py:205: in test_verify_tokens_reject
    accepted = decoder._verify_tokens(draft_tokens, input_ids, worker_state)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: SpeculativeDecoder._verify_tokens() missing 1 required positional argument: 'worker_state'
=============================== warnings summary ===============================
tests/e2e/test_token_reduction.py::TestTokenReductionE2E::test_baseline_direct_cot
  /Users/darianrosebrook/Desktop/Projects/distill/venv/lib/python3.11/site-packages/_pytest/python.py:170: PytestReturnNotNoneWarning: Test functions should return None, but tests/e2e/test_token_reduction.py::TestTokenReductionE2E::test_baseline_direct_cot returned <class 'eval.scoring.efficiency.EfficiencyMetrics'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/e2e/test_token_reduction.py::TestTokenReductionE2E::test_latent_mode_token_reduction
  /Users/darianrosebrook/Desktop/Projects/distill/venv/lib/python3.11/site-packages/_pytest/python.py:170: PytestReturnNotNoneWarning: Test functions should return None, but tests/e2e/test_token_reduction.py::TestTokenReductionE2E::test_latent_mode_token_reduction returned <class 'eval.scoring.efficiency.EfficiencyMetrics'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_ane_optimizations.py::test_int64_detection_on_attention_paths
tests/test_ane_optimizations.py::test_embedding_layer_int32_verification
tests/test_export_contracts.py::test_export_prefill_model_no_unsupported_ops
tests/test_export_contracts.py::test_export_model_int32_input_ids
  /Users/darianrosebrook/Desktop/Projects/distill/models/student/architectures/gqa_transformer.py:76: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
    scale = max(1.0, t / 2048.0)

tests/test_claims_pipeline_toy.py: 6 warnings
tests/test_claims_policy.py: 5 warnings
tests/test_claims_toy_model_integration.py: 5 warnings
  /Users/darianrosebrook/Desktop/Projects/distill/arbiter/claims/pipeline.py:1449: UserWarning: PlaceholderEntailmentJudge is a placeholder implementation using lexical overlap heuristics. For production use, replace with a trained NLI model. This judge has limited accuracy compared to trained models.
    self.entailment = entailment or PlaceholderEntailmentJudge(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.11.14-final-0 _______________

Name                                              Stmts   Miss  Cover   Missing
-------------------------------------------------------------------------------
conversion/convert_coreml.py                        315    243    23%   32-41, 83-397, 435-436, 440, 448-501, 596-608, 621, 628-629, 637-646, 665
conversion/export_onnx.py                            70      1    99%   151
conversion/export_pytorch.py                        152     67    56%   34, 56-91, 98-137, 205-207, 237-248, 266, 300-308, 327, 334-335, 348-351, 355-356, 362
conversion/judge_export_coreml.py                    36     36     0%   5-86
conversion/judge_export_onnx.py                      43     43     0%   5-100
conversion/make_toy_block.py                         74     74     0%   11-122
conversion/make_toy_onnx.py                          35     35     0%   10-81
conversion/make_toy_torch.py                         55     55     0%   10-92
conversion/onnx_surgery.py                          116     48    59%   57-58, 80-104, 112-132, 137, 157, 168-170, 182-200, 204
conversion/shape_validator.py                        60     26    57%   43-68, 77-78, 114, 127, 171, 211-228
conversion/validators.py                              0      0   100%
evaluation/8ball_eval.py                            181    118    35%   81-105, 116-159, 166-210, 242-244, 279, 287-304, 315-416, 420
evaluation/__init__.py                                0      0   100%
evaluation/caws_eval.py                             169     89    47%   15-16, 34, 79-84, 105-123, 138, 151, 165, 169-170, 191-194, 216, 218-221, 250-255, 291-388, 392
evaluation/claim_extraction_metrics.py               67     26    61%   150-192, 201
evaluation/classification_eval.py                   223    154    31%   58, 64, 76-84, 93-136, 143-187, 219-221, 228-230, 264-292, 301-444, 448
evaluation/compare_8ball_pipelines.py                78     78     0%   12-210
evaluation/long_ctx_eval.py                           4      4     0%   1-6
evaluation/perf_mem_eval.py                         429    274    36%   15-17, 37-38, 55-57, 86, 104, 122, 156-412, 449-457, 462-466, 469-524, 639, 648, 665-667, 689, 715, 720-723, 733-737, 746-782, 791-831, 837, 839, 845-859, 869-870, 895-914, 928-929, 947, 954-955
evaluation/performance_benchmarks.py                 89     89     0%   10-350
evaluation/pipeline_preservation_eval.py            100    100     0%   15-260
evaluation/reasoning_eval.py                        106    106     0%   13-276
evaluation/tool_use_eval.py                         197    122    38%   53, 68-96, 121, 148, 170-309, 339-340, 347-397, 401
evaluation/toy/__init__.py                            1      1     0%   8
evaluation/toy/binary_classifier.py                  24     24     0%   11-79
evaluation/toy/eight_ball.py                         30     30     0%   9-114
evaluation/toy/eight_ball_config.py                  22     22     0%   11-105
evaluation/toy/ternary_classifier.py                 24     24     0%   13-92
evaluation/toy_contracts.py                         151    151     0%   14-301
models/student/architectures/gqa_transformer.py     232     13    94%   230-231, 297, 312, 358-361, 374, 390, 434-435, 440
models/student/tokenizer/constants.py                 7      0   100%
models/teacher/teacher_client.py                    432    184    57%   35-36, 151, 161, 164-167, 196-255, 264, 277-298, 303, 309, 323-328, 339-343, 348-360, 406, 412-415, 489, 557, 568, 584-585, 590-592, 619, 622, 635, 640, 660, 674-690, 704-788, 797-809, 829-834, 863-864, 876-898, 915-967, 1013, 1023, 1036, 1039, 1073-1087, 1090-1093, 1099-1102
training/assertions.py                               84     59    30%   28, 32-34, 41-56, 64-66, 87, 105-116, 133-151, 176-217
training/caws_context.py                            136    110    19%   57-74, 88-126, 158-160, 176-250, 267-271, 295-299, 318-356, 367-378
training/caws_structure.py                           41      5    88%   58, 65, 67, 93, 131
training/claim_extraction.py                         92     29    68%   79, 83, 120-127, 133, 178, 199, 211, 217-218, 226-235, 258-275
training/config_validation.py                        81     27    67%   106-107, 128, 144-145, 216, 239, 247-254, 281-301
training/dataloader.py                                6      6     0%   1-9
training/dataset.py                                 253     78    69%   18-19, 25, 31, 84, 90, 101, 107, 123-126, 135-136, 159, 169, 183-195, 235, 239, 251-252, 257-265, 278-279, 292-299, 306-336, 405-406, 441-446, 449-454, 458-462, 481, 483, 485, 487, 489, 493
training/dataset_answer_generation.py                57      4    93%   135-141
training/dataset_post_tool.py                        55      5    91%   59, 131-137
training/dataset_tool_select.py                      56      3    95%   62, 138-140
training/distill_answer_generation.py                93     93     0%   7-232
training/distill_intermediate.py                     24     24     0%   18-54
training/distill_kd.py                             1251    902    28%   49-51, 58-60, 67-69, 76-79, 104, 121-280, 319-322, 326-329, 333-336, 372-385, 387, 394-408, 428-432, 441-449, 475, 639, 653-662, 666, 672, 675-695, 700-747, 750-753, 819, 867, 922-923, 937-942, 1045, 1055-1069, 1083-1090, 1097-1117, 1126-1130, 1145-1159, 1176, 1178, 1180, 1182, 1184, 1186, 1198-1290, 1299, 1306-1342, 1384-1413, 1421-1457, 1470-1537, 1549-1615, 1624-1645, 1656-1729, 1734-1807, 1814-1920, 1933, 1948, 1964, 1969-1972, 1982-1993, 2017, 2020, 2023, 2031, 2034, 2047, 2052, 2063-2838, 2842
training/distill_post_tool.py                        93     93     0%   7-228
training/distill_process.py                         181    114    37%   26-27, 83, 113-114, 135-281, 314-408, 412
training/distill_tool_select.py                     142    142     0%   11-345
training/examples_priority3_integration.py          122    122     0%   16-391
training/export_student.py                           81     23    72%   64-66, 112-113, 119-120, 126, 136, 138, 144-161, 165
training/extractors.py                              125     30    76%   38-39, 44-45, 102-103, 157, 188-195, 199-209, 223-228, 260, 274, 286
training/feature_flags.py                           127     48    62%   128, 135, 159, 171, 182-183, 199-202, 229-238, 251-258, 271-299, 322-325, 330-332, 339-345
training/halt_targets.py                             46     37    20%   41-44, 67-85, 105-117, 146-156, 171, 195-209
training/input_validation.py                        166     78    53%   68-69, 77-78, 89, 114-125, 140, 148-156, 171, 181, 187-189, 195, 216, 222, 230-233, 242-244, 260-282, 294-300, 320-321, 326, 364-365, 383, 390, 394, 399-422
training/json_repair.py                              91     17    81%   16, 43-44, 49, 81, 151, 157-162, 194-199, 231
training/logging_utils.py                            58     12    79%   60, 69-75, 139-146
training/losses.py                                  282     75    73%   66, 222, 243-267, 284-285, 306-318, 342-358, 384-402, 412, 500-510, 558, 571-572, 576-578, 584-586, 685, 841, 846, 853, 887-889, 940, 948, 972, 982, 994-997
training/make_toy_training.py                        68     68     0%   16-242
training/monitoring.py                              191    121    37%   60-67, 85-98, 109-115, 130-146, 178-205, 213-220, 229-242, 257-270, 285-327, 368-378, 398-411, 415-425, 440-475, 483, 511-527
training/performance_monitor.py                      91     91     0%   7-237
training/process_losses.py                          174     87    50%   38, 61-62, 68, 171, 206-296, 318-360, 403, 416, 425
training/prompt_templates.py                        121     66    45%   51-131, 147-189, 210-291, 306-346, 364-381, 399-422, 436-467, 528-533, 549-594
training/quality_scoring.py                         115     17    85%   51, 65, 89, 149, 185, 261, 264-269, 279-298
training/quant_qat_int8.py                          253    216    15%   26-29, 33-52, 58-67, 74-79, 82-109, 116-133, 137-143, 156-171, 174-209, 214, 221, 229-233, 275-291, 297, 302-317, 322-352, 356-541, 545
training/run_manifest.py                            110    110     0%   16-255
training/run_toy_distill.py                         185    185     0%   11-425
training/speed_metrics.py                            65      7    89%   83-88, 111-112, 177
training/teacher_cache.py                           102    102     0%   12-236
training/teacher_stub_toy.py                         99     99     0%   15-283
training/tokenizer_migration.py                      95     86     9%   33-56, 86-156, 177-208, 230-239, 264-291
training/tracing.py                                 143    116    19%   39, 46, 89-138, 154-187, 197-213, 225-237, 249-261, 274-279, 292-297, 306-317, 321-325, 329-338, 342, 346, 363-369
training/utils.py                                    27      0   100%
-------------------------------------------------------------------------------
TOTAL                                              9104   5744    37%
Coverage HTML written to dir htmlcov
Coverage JSON written to file coverage.json
=========================== short test summary info ============================
FAILED tests/conversion/test_convert_coreml.py::TestLoadContract::test_load_contract_file_not_found
FAILED tests/conversion/test_convert_coreml.py::TestConvertPyTorchToCoreML::test_convert_pytorch_to_coreml_success
FAILED tests/conversion/test_convert_coreml.py::TestConvertPyTorchToCoreML::test_convert_pytorch_to_coreml_with_ane_optimization
FAILED tests/conversion/test_convert_coreml.py::TestConvertPyTorchToCoreML::test_convert_pytorch_to_coreml_ane_incompatible
FAILED tests/conversion/test_convert_coreml.py::TestConvertPyTorchToCoreML::test_convert_pytorch_to_coreml_conversion_failure
FAILED tests/conversion/test_convert_coreml.py::TestConvertONNXToCoreML::test_convert_onnx_to_coreml_success
FAILED tests/conversion/test_convert_coreml.py::TestConvertONNXToCoreML::test_convert_onnx_to_coreml_with_custom_target
FAILED tests/conversion/test_convert_coreml.py::TestConvertONNXToCoreML::test_convert_onnx_to_coreml_file_not_found
FAILED tests/conversion/test_convert_coreml.py::TestCreatePlaceholder::test_create_placeholder_success
FAILED tests/conversion/test_convert_coreml.py::TestMainFunction::test_main_pytorch_conversion
FAILED tests/conversion/test_convert_coreml.py::TestMainFunction::test_main_conversion_failure_with_placeholder
FAILED tests/conversion/test_convert_coreml.py::TestMainFunction::test_main_invalid_backend
FAILED tests/conversion/test_convert_coreml.py::TestMainFunction::test_main_contract_not_found
FAILED tests/conversion/test_convert_coreml.py::TestANEOptimizations::test_detect_int64_tensors_fallback
FAILED tests/conversion/test_convert_coreml.py::TestANEOptimizations::test_check_ane_op_compatibility_fallback
FAILED tests/conversion/test_convert_coreml.py::TestCoreMLConversionIntegration::test_conversion_workflow_pytorch
FAILED tests/conversion/test_export_onnx.py::TestDecodeWrapper::test_decode_wrapper_forward_single_token
FAILED tests/conversion/test_export_onnx.py::TestDecodeWrapper::test_decode_wrapper_forward_batch_processing
FAILED tests/conversion/test_export_onnx.py::TestMainFunction::test_main_both_modes
FAILED tests/conversion/test_export_onnx.py::TestMainFunction::test_main_decode_only
FAILED tests/conversion/test_export_onnx.py::TestMainFunction::test_main_config_not_found
FAILED tests/conversion/test_export_onnx.py::TestMainFunction::test_main_invalid_config
FAILED tests/conversion/test_export_onnx.py::TestMainFunction::test_main_fallback_config
FAILED tests/conversion/test_export_onnx.py::TestMainFunction::test_main_model_loading_failure
FAILED tests/conversion/test_export_onnx.py::TestMainFunction::test_main_onnx_export_failure
FAILED tests/conversion/test_export_onnx.py::TestONNXExportIntegration::test_config_parsing_edge_cases
FAILED tests/conversion/test_export_onnx.py::TestONNXExportIntegration::test_export_file_naming
FAILED tests/conversion/test_export_onnx.py::TestONNXExportIntegration::test_checkpoint_loading_and_model_creation
FAILED tests/conversion/test_export_pytorch.py::TestPrefillWrapper::test_prefill_wrapper_forward_without_halt
FAILED tests/conversion/test_export_pytorch.py::TestPrefillWrapper::test_prefill_wrapper_forward_with_halt
FAILED tests/conversion/test_export_pytorch.py::TestDecodeWrapper::test_decode_wrapper_forward_single_token
FAILED tests/conversion/test_export_pytorch.py::TestDecodeWrapper::test_decode_wrapper_forward_with_kv_cache
FAILED tests/conversion/test_export_pytorch.py::TestExportPrefill::test_export_prefill_success
FAILED tests/conversion/test_export_pytorch.py::TestExportPrefill::test_export_prefill_with_halt_head
FAILED tests/conversion/test_export_pytorch.py::TestExportDecode::test_export_decode_success
FAILED tests/conversion/test_export_pytorch.py::TestExportDecode::test_export_decode_different_configs
FAILED tests/conversion/test_export_pytorch.py::TestMainFunction::test_main_success
FAILED tests/conversion/test_export_pytorch.py::TestMainFunction::test_main_checkpoint_without_config
FAILED tests/conversion/test_export_pytorch.py::TestModelConfigurations::test_prefill_wrapper_different_batch_sizes
FAILED tests/conversion/test_export_pytorch.py::TestModelConfigurations::test_decode_wrapper_different_kv_configs
FAILED tests/conversion/test_export_pytorch.py::TestExportEdgeCases::test_export_prefill_minimal_config
FAILED tests/conversion/test_export_pytorch.py::TestExportEdgeCases::test_export_decode_minimal_config
FAILED tests/conversion/test_export_pytorch.py::TestExportEdgeCases::test_wrapper_parameter_validation
FAILED tests/conversion/test_onnx_surgery.py::TestForceInputDtype::test_force_input_dtype_not_found
FAILED tests/conversion/test_onnx_surgery.py::TestStripRedundantCasts::test_strip_redundant_casts_redundant_cast
FAILED tests/conversion/test_onnx_surgery.py::TestStripRedundantCasts::test_strip_redundant_casts_with_cast_nodes
FAILED tests/conversion/test_onnx_surgery.py::TestCastInt64Initializers::test_cast_int64_initializers_int64_present
FAILED tests/conversion/test_onnx_surgery.py::TestCastInt64Initializers::test_cast_int64_initializers_mixed_types
FAILED tests/conversion/test_onnx_surgery.py::TestCastInt64Initializers::test_cast_int64_initializers_no_int64
FAILED tests/conversion/test_onnx_surgery.py::TestRunFunction::test_run_success_without_simplification
FAILED tests/conversion/test_onnx_surgery.py::TestRunFunction::test_run_success_with_simplification
FAILED tests/conversion/test_onnx_surgery.py::TestMainFunction::test_main_success
FAILED tests/conversion/test_onnx_surgery.py::TestMainFunction::test_main_run_failure
FAILED tests/conversion/test_onnx_surgery.py::TestMainFunction::test_main_missing_required_args
FAILED tests/conversion/test_onnx_surgery.py::TestONNXSurgeryIntegration::test_complete_surgery_workflow
FAILED tests/e2e/test_8_ball_pipeline.py::test_8_ball_pipeline_e2e - Assertio...
FAILED tests/e2e/test_token_reduction.py::TestTokenReductionE2E::test_efficiency_curves
FAILED tests/e2e/test_toy_ces_measurement.py::TestCESMeasurement::test_ces_combined_milestones
FAILED tests/e2e/test_toy_code_mode.py::test_toy_training_without_code_mode
FAILED tests/e2e/test_toy_code_mode.py::test_toy_training_with_code_mode_enabled
FAILED tests/e2e/test_toy_code_mode.py::test_toy_code_mode_with_span_targets
FAILED tests/e2e/test_toy_code_mode.py::test_toy_code_mode_weight_scheduler_integration
FAILED tests/e2e/test_toy_combined_milestones.py::TestCombinedMilestones::test_training_with_both_features
FAILED tests/e2e/test_toy_combined_milestones.py::TestCombinedMilestones::test_code_mode_with_latent_spans
FAILED tests/e2e/test_toy_combined_milestones.py::TestCombinedMilestones::test_mixed_batch_eligibility
FAILED tests/e2e/test_toy_combined_milestones.py::TestCombinedMilestones::test_full_pipeline_integration
FAILED tests/e2e/test_toy_pipeline.py::test_toy_pipeline_e2e - AssertionError...
FAILED tests/e2e/test_toy_pipeline.py::test_toy_pipeline_with_code_mode - Ass...
FAILED tests/e2e/test_toy_pipeline.py::test_toy_pipeline_with_latent_mode - A...
FAILED tests/e2e/test_toy_pipeline.py::test_toy_pipeline_with_both_features
FAILED tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_id_to_answer_mapping
FAILED tests/evaluation/test_8ball_eval.py::TestDataClasses::test_prediction_result_creation
FAILED tests/evaluation/test_8ball_eval.py::TestDataClasses::test_evaluation_metrics_creation
FAILED tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_json_file
FAILED tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_text_file
FAILED tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_nonexistent_file
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_success
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_empty_questions
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_tokenizer_failure
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_success
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_file_not_found
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_subprocess_failure
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_invalid_json
FAILED tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_identical
FAILED tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_different
FAILED tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_empty_lists
FAILED tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_different_lengths
FAILED tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_token_distribution
FAILED tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_pytorch_evaluation
FAILED tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_coreml_evaluation
FAILED tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_ollama_evaluation
FAILED tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_invalid_backend
FAILED tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_missing_eval_file
FAILED tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_prediction_result_validation
FAILED tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_evaluation_metrics_calculation
FAILED tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_evaluation_workflow
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_within_limits
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_exceeds_loc_limit
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_exceeds_files_limit
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_empty_diff
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_multiple_files
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_binary_files
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_edge_cases
FAILED tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_all_pass
FAILED tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_tests_fail
FAILED tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_lint_fail
FAILED tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_coverage_fail
FAILED tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_missing_fields
FAILED tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_complete
FAILED tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_missing_rationale
FAILED tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_missing_evidence
FAILED tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_no_diff
FAILED tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_whitespace_only
FAILED tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_all_pass
FAILED tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_gates_fail
FAILED tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_provenance_fail
FAILED tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_budget_fail
FAILED tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_multiple_failures
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_working_spec_not_found
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_json_file_invalid_json
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_tests_success
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_tests_failure
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_linter_success
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_coverage_success
FAILED tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_success
FAILED tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_missing_spec
FAILED tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_test_failure
FAILED tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_complete_caws_evaluation_workflow
FAILED tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_caws_evaluation_with_violations
FAILED tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_budget_adherence_edge_cases
FAILED tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_gate_integrity_thresholds
FAILED tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_provenance_clarity_validation
FAILED tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_success
FAILED tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_mismatched_lengths
FAILED tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_complete_evaluation_workflow
FAILED tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_metrics_calculation_edge_cases
FAILED tests/evaluation/test_classification_eval.py::TestEvaluationMetrics::test_evaluation_metrics_creation
FAILED tests/evaluation/test_classification_eval.py::TestEvaluationMetrics::test_evaluation_metrics_minimal
FAILED tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_success
FAILED tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_file_not_found
FAILED tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_invalid_json
FAILED tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_missing_fields
FAILED tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_success
FAILED tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_empty_questions
FAILED tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_tokenizer_failure
FAILED tests/evaluation/test_classification_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_success
FAILED tests/evaluation/test_classification_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_file_not_found
FAILED tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_success
FAILED tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_subprocess_failure
FAILED tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_invalid_json
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_identical
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_different
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_empty_lists
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_different_lengths
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_with_probabilities
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_without_probabilities
FAILED tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_pytorch_evaluation
FAILED tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_coreml_evaluation
FAILED tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_invalid_backend
FAILED tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_config_not_found
FAILED tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_complete_evaluation_workflow
FAILED tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_evaluation_metrics_calculation
FAILED tests/evaluation/test_perf_mem_eval.py::TestStepAdapter::test_step_adapter_creation
FAILED tests/evaluation/test_perf_mem_eval.py::TestStepAdapter::test_step_adapter_defaults
FAILED tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_macos
FAILED tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_non_macos
FAILED tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_no_coremltools
FAILED tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_empty_array
FAILED tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_negative_values
FAILED tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_invalid_syntax
FAILED tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_missing_fields
FAILED tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_success
FAILED tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_model_load_failure
FAILED tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_no_coremltools
FAILED tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_success
FAILED tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_file_not_found
FAILED tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_invalid_json
FAILED tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_missing_tokens_field
FAILED tests/evaluation/test_perf_mem_eval.py::TestMainFunction::test_main_success
FAILED tests/evaluation/test_perf_mem_eval.py::TestMainFunction::test_main_missing_required_args
FAILED tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_tokenization_workflow
FAILED tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_json_validation_comprehensive
FAILED tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_end_to_end_workflow_simulation
FAILED tests/evaluation/test_tool_use_eval.py::TestLoadModel::test_load_model_with_config
FAILED tests/evaluation/test_tool_use_eval.py::TestLoadModel::test_load_model_without_config
FAILED tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_basic
FAILED tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_with_eos
FAILED tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_temperature
FAILED tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_max_length
FAILED tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_success
FAILED tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_invalid_json
FAILED tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_wrong_tool
FAILED tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_empty_test_cases
FAILED tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_success
FAILED tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_checkpoint_not_found
FAILED tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_config_not_found
FAILED tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_complete_evaluation_workflow
FAILED tests/integration/test_process_step_integration.py::test_batch_contains_process_step_targets
FAILED tests/integration/test_process_step_integration.py::test_process_supervision_loss_with_token_ids
FAILED tests/integration/test_process_step_integration.py::test_training_step_with_process_step_targets
FAILED tests/integration/test_speed_optimization_integration.py::TestLatencyAwareLossesIntegration::test_length_aware_loss_in_training_step
FAILED tests/integration/test_speed_optimization_integration.py::TestLatencyAwareLossesIntegration::test_early_tool_loss_in_training_step
FAILED tests/integration/test_speed_optimization_integration.py::TestLatencyAwareLossesIntegration::test_latency_losses_with_combined_kd
FAILED tests/integration/test_speed_optimization_integration.py::TestSpeedMetricsIntegration::test_speed_metrics_aggregation
FAILED tests/integration/test_speed_optimization_integration.py::TestTrainingStepWithSpeedOptimizations::test_training_step_with_length_loss
FAILED tests/integration/test_speed_optimization_integration.py::TestTrainingStepWithSpeedOptimizations::test_training_step_with_early_tool_loss
FAILED tests/models/test_teacher_client.py::TestAPITier::test_tier_limits_structure
FAILED tests/models/test_teacher_client.py::TestTeacherClientInitialization::test_init_api_key_from_env
FAILED tests/models/test_teacher_client.py::TestTeacherClientInitialization::test_init_hf_backend
FAILED tests/models/test_teacher_client.py::TestTierDetection::test_update_tier_from_response_headers
FAILED tests/models/test_teacher_client.py::TestTierDetection::test_update_tier_unknown_header
FAILED tests/models/test_teacher_client.py::TestRetrySession::test_setup_retry_session
FAILED tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_single_prompt_success
FAILED tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_with_logits
FAILED tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_retry_on_failure
FAILED tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_rate_limit_handling
FAILED tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_max_retries_exceeded
FAILED tests/models/test_teacher_client.py::TestHTTPBackendSampling::test_sample_multi_prompt_batch
FAILED tests/models/test_teacher_client.py::TestCircuitBreaker::test_try_fallback_api_success
FAILED tests/models/test_teacher_client.py::TestHealthCheck::test_health_check_failure
FAILED tests/models/test_teacher_client.py::TestMultiStepSampling::test_sample_multi_step_basic
FAILED tests/models/test_teacher_client.py::TestErrorHandling::test_sample_empty_prompts
FAILED tests/models/test_teacher_client.py::TestErrorHandling::test_sample_malformed_response
FAILED tests/models/test_teacher_client.py::TestErrorHandling::test_sample_timeout
FAILED tests/runtime/test_caws_budget_enforcement.py::TestCAWSBudgetEnforcement::test_tier_1_limits
FAILED tests/runtime/test_caws_budget_enforcement.py::TestCAWSBudgetEnforcement::test_tier_2_limits
FAILED tests/runtime/test_caws_budget_enforcement.py::TestCAWSBudgetEnforcement::test_tier_3_limits
FAILED tests/runtime/test_caws_budget_enforcement.py::TestCAWSBudgetEnforcement::test_budget_breach_forces_halt
FAILED tests/runtime/test_latent_mode.py::TestLatentModeEngine::test_max_latent_spans_limit
FAILED tests/runtime/test_safety_edge_cases.py::TestSafetyEdgeCases::test_generation_ends_in_latent_mode
FAILED tests/test_claims_pipeline_toy.py::test_pipeline_determinism - TypeErr...
FAILED tests/test_claims_pipeline_toy.py::test_pipeline_outcome_distribution
FAILED tests/test_prefill_decode_consistency.py::test_kv_cache_index_advancement
FAILED tests/test_tokenizer_contract.py::test_special_token_ids_match_constants
FAILED tests/test_tokenizer_contract.py::test_special_tokens_are_single_tokens
FAILED tests/test_tokenizer_contract.py::test_round_trip_stability - Assertio...
FAILED tests/test_tokenizer_contract.py::test_masking_safety - AttributeError...
FAILED tests/training/test_distill_kd.py::TestModelCreation::test_create_model_with_quantization
FAILED tests/training/test_distill_kd.py::TestModelCreation::test_create_model_invalid_config
FAILED tests/training/test_distill_kd.py::TestOptimizerCreation::test_create_optimizer_adamw
FAILED tests/training/test_distill_kd.py::TestOptimizerCreation::test_create_optimizer_default_config
FAILED tests/training/test_distill_kd.py::TestOptimizerCreation::test_create_optimizer_invalid_type
FAILED tests/training/test_distill_kd.py::TestQATOperations::test_apply_qat_to_model
FAILED tests/training/test_distill_kd.py::TestQATOperations::test_check_qat_stability_valid
FAILED tests/training/test_distill_kd.py::TestQATOperations::test_check_qat_stability_nan_weights
FAILED tests/training/test_distill_kd.py::TestTrainingStep::test_train_step_vocab_clamping
FAILED tests/training/test_distill_kd.py::TestTrainingStep::test_train_step_with_self_evaluation
FAILED tests/training/test_distill_kd.py::TestCheckpointOperations::test_save_checkpoint_basic
FAILED tests/training/test_distill_kd.py::TestCheckpointOperations::test_save_checkpoint_with_metadata
FAILED tests/training/test_distill_process.py::TestConfigOperations::test_merge_configs_nested_merge
FAILED tests/training/test_distill_process.py::TestModelLoading::test_load_model_success
FAILED tests/training/test_distill_process.py::TestModelLoading::test_load_model_without_config
FAILED tests/training/test_distill_process.py::TestTextGeneration::test_generate_text_from_logits_temperature
FAILED tests/training/test_distill_process.py::TestTextGeneration::test_generate_text_from_logits_top_k
FAILED tests/training/test_distill_process.py::TestTextGeneration::test_generate_text_from_logits_top_p
FAILED tests/training/test_distill_process.py::TestTextGeneration::test_generate_text_from_logits_greedy
FAILED tests/training/test_distill_process.py::TestTrainingStep::test_train_step_process_basic
FAILED tests/training/test_distill_process.py::TestTrainingStep::test_train_step_process_zero_weights
FAILED tests/training/test_distill_process.py::TestTrainingStep::test_train_step_process_no_grad
FAILED tests/training/test_distill_process.py::TestTrainingStep::test_train_step_process_missing_process_labels
FAILED tests/training/test_distill_process.py::TestMainFunction::test_main_success
FAILED tests/training/test_distill_process.py::TestMainFunction::test_main_config_load_failure
FAILED tests/training/test_distill_process.py::TestMainFunction::test_main_model_load_failure
FAILED tests/training/test_export_student.py::TestTorchScriptExport::test_export_torchscript_success
FAILED tests/training/test_export_student.py::TestContractCreation::test_create_contract_success
FAILED tests/training/test_export_student.py::TestContractCreation::test_create_contract_directory_creation
FAILED tests/training/test_export_student.py::TestContractCreation::test_create_contract_file_writing
FAILED tests/training/test_export_student.py::TestMainFunction::test_main_torchscript_export
FAILED tests/training/test_export_student.py::TestMainFunction::test_main_exported_program_export
FAILED tests/training/test_export_student.py::TestMainFunction::test_main_invalid_export_type
FAILED tests/training/test_export_student.py::TestExportIntegration::test_export_workflow
FAILED tests/training/test_export_student.py::TestConfigurationHandling::test_contract_creation_with_different_configs
FAILED tests/training/test_export_student.py::TestConfigurationHandling::test_export_with_different_input_shapes
FAILED tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_enable_with_dependencies
FAILED tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_get_feature_config
FAILED tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_list_features
FAILED tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_reset_to_defaults
FAILED tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_save_to_environment
FAILED tests/training/test_feature_flags.py::TestFeatureManager::test_feature_manager_get_feature_stats
FAILED tests/training/test_feature_flags.py::TestFeatureInteractions::test_feature_manager_thread_safety
FAILED tests/training/test_latent_curriculum.py::TestLatentCurriculum::test_curriculum_applies_latent_slots
FAILED tests/training/test_latent_curriculum.py::TestLatentCurriculum::test_curriculum_creates_loss_mask
FAILED tests/training/test_latent_curriculum.py::TestLatentCurriculum::test_loss_mask_masks_latent_spans
FAILED tests/training/test_logging_utils.py::TestStructuredFormatter::test_structured_formatter_format_basic
FAILED tests/training/test_logging_utils.py::TestStructuredFormatter::test_structured_formatter_format_with_exception
FAILED tests/training/test_logging_utils.py::TestStructuredFormatter::test_structured_formatter_json_validity
FAILED tests/training/test_logging_utils.py::TestSetupTrainingLogging::test_setup_training_logging_basic
FAILED tests/training/test_logging_utils.py::TestSetupTrainingLogging::test_setup_training_logging_with_file
FAILED tests/training/test_logging_utils.py::TestSetupTrainingLogging::test_setup_training_logging_different_levels
FAILED tests/training/test_logging_utils.py::TestLogTrainingStep::test_log_training_step_basic
FAILED tests/training/test_logging_utils.py::TestLogTrainingStep::test_log_training_step_with_additional_metrics
FAILED tests/training/test_logging_utils.py::TestLogTrainingStep::test_log_training_step_zero_values
FAILED tests/training/test_logging_utils.py::TestLogValidationMetrics::test_log_validation_metrics_basic
FAILED tests/training/test_logging_utils.py::TestLogValidationMetrics::test_log_validation_metrics_empty
FAILED tests/training/test_logging_utils.py::TestLogValidationMetrics::test_log_validation_metrics_complex
FAILED tests/training/test_logging_utils.py::TestLogCheckpointSaved::test_log_checkpoint_saved_basic
FAILED tests/training/test_logging_utils.py::TestLogCheckpointSaved::test_log_checkpoint_saved_different_paths
FAILED tests/training/test_logging_utils.py::TestLogError::test_log_error_basic
FAILED tests/training/test_logging_utils.py::TestLogError::test_log_error_no_context
FAILED tests/training/test_logging_utils.py::TestLogError::test_log_error_different_exceptions
FAILED tests/training/test_logging_utils.py::TestLoggingIntegration::test_structured_logging_workflow
FAILED tests/training/test_monitoring.py::TestMetricsCollector::test_metrics_collector_initialization
FAILED tests/training/test_monitoring.py::TestMetricsCollector::test_add_metric
FAILED tests/training/test_monitoring.py::TestMetricsCollector::test_add_metric_eviction
FAILED tests/training/test_monitoring.py::TestMetricsCollector::test_get_metrics_by_name
FAILED tests/training/test_monitoring.py::TestMetricsCollector::test_get_metrics_by_tags
FAILED tests/training/test_monitoring.py::TestMetricsCollector::test_get_latest_metric
FAILED tests/training/test_monitoring.py::TestMetricsCollector::test_get_metric_statistics
FAILED tests/training/test_monitoring.py::TestMetricsCollector::test_clear_metrics
FAILED tests/training/test_monitoring.py::TestMetricsCollector::test_thread_safety
FAILED tests/training/test_monitoring.py::TestHealthChecker::test_health_checker_initialization
FAILED tests/training/test_monitoring.py::TestHealthChecker::test_add_check
FAILED tests/training/test_monitoring.py::TestHealthChecker::test_run_check
FAILED tests/training/test_monitoring.py::TestHealthChecker::test_run_check_not_found
FAILED tests/training/test_monitoring.py::TestHealthChecker::test_run_all_checks
FAILED tests/training/test_monitoring.py::TestHealthChecker::test_get_component_status
FAILED tests/training/test_monitoring.py::TestHealthChecker::test_get_overall_health
FAILED tests/training/test_monitoring.py::TestSystemHealthChecks::test_system_health_checks_initialization
FAILED tests/training/test_monitoring.py::TestSystemHealthChecks::test_cpu_usage_check
FAILED tests/training/test_monitoring.py::TestSystemHealthChecks::test_memory_usage_check
FAILED tests/training/test_monitoring.py::TestSystemHealthChecks::test_gpu_check_no_gpu
FAILED tests/training/test_monitoring.py::TestSystemHealthChecks::test_gpu_check_with_gpu
FAILED tests/training/test_monitoring.py::TestSystemHealthChecks::test_disk_usage_check
FAILED tests/training/test_monitoring.py::TestSystemHealthChecks::test_network_connectivity_check
FAILED tests/training/test_monitoring.py::TestSystemHealthChecks::test_run_system_checks
FAILED tests/training/test_monitoring.py::TestTrainingMonitor::test_training_monitor_initialization
FAILED tests/training/test_monitoring.py::TestTrainingMonitor::test_start_monitoring
FAILED tests/training/test_monitoring.py::TestTrainingMonitor::test_stop_monitoring
FAILED tests/training/test_monitoring.py::TestTrainingMonitor::test_log_metric
FAILED tests/training/test_monitoring.py::TestTrainingMonitor::test_log_training_step
FAILED tests/training/test_monitoring.py::TestTrainingMonitor::test_log_validation_metrics
FAILED tests/training/test_monitoring.py::TestTrainingMonitor::test_get_monitoring_stats
FAILED tests/training/test_monitoring.py::TestTrainingMonitor::test_save_load_metrics
FAILED tests/training/test_monitoring.py::TestTrainingMonitor::test_alert_on_condition
FAILED tests/training/test_monitoring.py::TestConcurrencyAndPerformance::test_concurrent_metric_logging
FAILED tests/training/test_monitoring.py::TestConcurrencyAndPerformance::test_monitoring_performance
FAILED tests/training/test_quality_scoring.py::TestHeuristicQualityScore::test_compute_heuristic_quality_score_structured_content
FAILED tests/training/test_quality_scoring.py::TestHeuristicQualityScore::test_compute_heuristic_quality_score_ground_truth_comparison
FAILED tests/training/test_quality_scoring.py::TestJSONValidityScore::test_compute_json_validity_score_malformed_json
FAILED tests/training/test_quality_scoring.py::TestCodeBlockScore::test_compute_code_block_score_python_code
FAILED tests/training/test_quality_scoring.py::TestCodeBlockScore::test_compute_code_block_score_mixed_languages
FAILED tests/training/test_quality_scoring.py::TestCodeBlockScore::test_compute_code_block_score_empty_code_block
FAILED tests/training/test_quality_scoring.py::TestCompositeQualityScore::test_compute_composite_quality_score_all_components
FAILED tests/training/test_quality_scoring.py::TestCompositeQualityScore::test_compute_composite_quality_score_minimal_text
FAILED tests/training/test_quality_scoring.py::TestCompositeQualityScore::test_compute_composite_quality_score_structured_content
FAILED tests/training/test_quality_scoring.py::TestCompositeQualityScore::test_compute_composite_quality_score_weights
FAILED tests/training/test_quality_scoring.py::TestCompositeQualityScore::test_compute_composite_quality_score_empty_text
FAILED tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_single_item
FAILED tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_multiple_items
FAILED tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_mixed_content
FAILED tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_error_handling
FAILED tests/training/test_quality_scoring.py::TestBatchQualityScoring::test_batch_compute_quality_scores_large_batch
FAILED tests/training/test_quality_scoring.py::TestQualityScoringIntegration::test_quality_scoring_realistic_examples
FAILED tests/training/test_utils.py::TestSHA256StateDict::test_sha256_state_dict_buffer_operations
FAILED tests/unit/test_ane_monitor.py::TestANEResidencyMonitor::test_get_model_ops_info_with_model
FAILED tests/unit/test_code_mode_loss.py::test_code_mode_loss_is_differentiable
FAILED tests/unit/test_code_mode_loss.py::test_code_mode_loss_eligibility_filtering
FAILED tests/unit/test_code_mode_loss_torchscript.py::test_code_mode_loss_torchscript
FAILED tests/unit/test_config_validation.py::TestConfigSchemaValidation::test_validate_training_config_missing_required
FAILED tests/unit/test_config_validation.py::TestConfigSchemaValidation::test_validate_config_file_nonexistent
FAILED tests/unit/test_config_validation.py::TestConfigMerging::test_merge_configs_basic
FAILED tests/unit/test_config_validation.py::TestConfigMerging::test_merge_configs_overrides
FAILED tests/unit/test_config_validation.py::TestConfigMerging::test_merge_configs_invalid_file
FAILED tests/unit/test_constrained_decode.py::TestDecoderIntegration::test_with_real_tokenizer
FAILED tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_privacy_scanning
FAILED tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_check_privacy_multiple_emails
FAILED tests/unit/test_contextual_generation.py::TestVerifyContextualSet::test_check_privacy_multiple_uuids
FAILED tests/unit/test_dataset_modules.py::TestAnswerGenerationDataset::test_dataset_getitem_basic
FAILED tests/unit/test_dataset_modules.py::TestAnswerGenerationDataset::test_dataset_prompt_formatting
FAILED tests/unit/test_dataset_modules.py::TestAnswerGenerationDataset::test_dataset_empty_tools
FAILED tests/unit/test_dataset_modules.py::TestAnswerGenerationDataset::test_dataset_missing_fields
FAILED tests/unit/test_dataset_modules.py::TestPostToolDataset::test_dataset_getitem
FAILED tests/unit/test_dataset_modules.py::TestPostToolDataset::test_dataset_tool_result_formatting
FAILED tests/unit/test_dataset_modules.py::TestToolSelectDataset::test_dataset_getitem
FAILED tests/unit/test_dataset_modules.py::TestToolSelectDataset::test_dataset_tool_formatting
FAILED tests/unit/test_dataset_modules.py::TestToolSelectDataset::test_dataset_empty_tools
FAILED tests/unit/test_dataset_modules.py::TestToolSelectDataset::test_dataset_json_formatting
FAILED tests/unit/test_dataset_modules.py::TestDatasetErrorHandling::test_answer_generation_invalid_json
FAILED tests/unit/test_enumerated_shapes.py::TestSampleEnumeratedShape::test_custom_shape_probs
FAILED tests/unit/test_gqa_transformer.py::TestMHAGQA::test_mha_gqa_with_mask
FAILED tests/unit/test_gqa_transformer.py::TestStudentLM::test_studentlm_forward_with_mask
FAILED tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_too_long
FAILED tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_suspicious_script
FAILED tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_javascript_url
FAILED tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_event_handler
FAILED tests/unit/test_input_validation.py::TestTextInputValidation::test_validate_text_input_none_input
FAILED tests/unit/test_input_validation.py::TestStructuredDataValidation::test_validate_structured_data_valid
FAILED tests/unit/test_input_validation.py::TestStructuredDataValidation::test_validate_structured_data_missing_required
FAILED tests/unit/test_input_validation.py::TestStructuredDataValidation::test_validate_structured_data_invalid_types
FAILED tests/unit/test_input_validation.py::TestStructuredDataValidation::test_validate_structured_data_suspicious_content
FAILED tests/unit/test_input_validation.py::TestStructuredDataValidation::test_validate_structured_data_too_long
FAILED tests/unit/test_input_validation.py::TestToolValidation::test_validate_tools_valid
FAILED tests/unit/test_input_validation.py::TestToolValidation::test_validate_tools_too_many
FAILED tests/unit/test_input_validation.py::TestToolValidation::test_validate_tools_missing_fields
FAILED tests/unit/test_input_validation.py::TestToolValidation::test_validate_tools_suspicious_description
FAILED tests/unit/test_input_validation.py::TestFileValidation::test_validate_file_path_valid
FAILED tests/unit/test_input_validation.py::TestFileValidation::test_validate_file_path_nonexistent
FAILED tests/unit/test_input_validation.py::TestFileValidation::test_validate_file_path_too_large
FAILED tests/unit/test_input_validation.py::TestFileValidation::test_validate_file_path_permission_error
FAILED tests/unit/test_input_validation.py::TestTrainingDataValidation::test_validate_training_data_valid
FAILED tests/unit/test_input_validation.py::TestTrainingDataValidation::test_validate_training_data_invalid_structure
FAILED tests/unit/test_input_validation.py::TestTrainingDataValidation::test_validate_tool_trace_valid
FAILED tests/unit/test_input_validation.py::TestSecurityPatterns::test_detect_suspicious_patterns_script_tags
FAILED tests/unit/test_input_validation.py::TestSecurityPatterns::test_detect_suspicious_patterns_javascript_url
FAILED tests/unit/test_input_validation.py::TestSecurityPatterns::test_detect_suspicious_patterns_iframe
FAILED tests/unit/test_input_validation.py::TestSecurityPatterns::test_detect_suspicious_patterns_event_handler
FAILED tests/unit/test_input_validation.py::TestSecurityPatterns::test_no_suspicious_patterns_normal_text
FAILED tests/unit/test_input_validation.py::TestSecurityPatterns::test_suspicious_patterns_case_insensitive
FAILED tests/unit/test_losses_speed.py::test_length_kd_completeness_exemption
FAILED tests/unit/test_losses_speed.py::test_length_kd_no_excess - TypeError:...
FAILED tests/unit/test_losses_speed.py::test_length_kd_hinge - TypeError: len...
FAILED tests/unit/test_losses_speed.py::test_early_tool_ce_only_when_needed
FAILED tests/unit/test_losses_speed.py::test_early_tool_json_prior_fallback
FAILED tests/unit/test_losses_speed.py::test_early_tool_masked_when_not_needed
FAILED tests/unit/test_losses_speed.py::test_early_tool_ramp - TypeError: ear...
FAILED tests/unit/test_qat_integration.py::TestCheckQATStability::test_stability_check_with_nan
FAILED tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_validate_tool_call_valid
FAILED tests/unit/test_schema_registry.py::TestToolSchemaRegistry::test_validate_tool_call_name_mismatch
FAILED tests/unit/test_security_contextual.py::TestPIIRedaction::test_uuid_detection
FAILED tests/unit/test_security_contextual.py::TestPIIRedaction::test_email_detection
FAILED tests/unit/test_speculative_decode.py::TestSpeculativeDecoder::test_draft_k_tokens
FAILED tests/unit/test_speculative_decode.py::TestSpeculativeDecoder::test_verify_tokens_accept
FAILED tests/unit/test_speculative_decode.py::TestSpeculativeDecoder::test_verify_tokens_reject
ERROR tests/models/test_teacher_client.py::TestHuggingFaceBackend::test_hf_sample_basic
= 423 failed, 889 passed, 16 skipped, 22 warnings, 1 error in 268.01s (0:04:28) =
