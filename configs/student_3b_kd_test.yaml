arch:
  # 3B Student Model Configuration for first real KD test
  d_model: 2560
  n_layers: 20
  n_heads: 20
  n_kv_heads: 5
  d_head: 128
  vocab_size: 32000
  rope_theta: 10000
  rope_scaling: "dynamic"
  norm: "rms"
  mlp: "swiglu"
  dropout: 0.0

model:  # Alias for validation
  d_model: 2560
  n_layers: 20
  n_heads: 20
  n_kv_heads: 5
  d_head: 128
  vocab_size: 32000
  rope_theta: 10000
  rope_scaling: "dynamic"
  dropout: 0.0

init:
  base_checkpoint: null

optimizer:
  name: adamw
  lr: 2.0e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1

train:
  seq_lengths: [1024]  # Single context length for first 3B KD run
  micro_batch_size: 2  # Reduced from 4 to avoid MPS OOM (was hitting 75GB/81GB limit)
  grad_accum: 8  # Increased from 4 to maintain effective batch size (2*8=16 vs 4*4=16)
  steps: 1500
  total_steps: 1500
  save_every: 100
  log_every: 50
  fp16: false  # Disabled: MPS doesn't support FP16 scaler properly, causes memory issues
  grad_checkpointing: true
  dataloader_workers: 0  # macOS+MPS: avoid fork deadlocks with HF tokenizers

# Multi-device parallelization for Apple Silicon (CPU + MPS)
# CPU offloading saves ~71GB MPS memory by keeping optimizer state on CPU
# Model parallelism splits layers across MPS and CPU (slower but uses less MPS memory)
multi_device:
  enabled: false  # Set to true to enable model parallelism (splits layers across devices)
  cpu_offload_optimizer: true  # Keep optimizer state on CPU (saves ~71GB MPS memory)
  strategy: "alternate"  # "alternate" | "first_half_mps" | "custom"
  # split_point: 10  # Only used if strategy="custom"

training:  # Alias for validation
  steps: 1500
  batch_size: 4
  seq_length: 1024
  grad_accum_steps: 4
  lr: 2.0e-4
  save_every: 100
  checkpoint_cleanup:
    max_checkpoints: 3  # Keep most recent N checkpoints (plus milestones)
    min_free_space_gb: 50.0  # Reduce retention if free space drops below this

io:
  tokenizer_path: models/student/tokenizer
  train_shards: [data/kd_mix_1500.jsonl]
  val_shards: []

role: "worker"

distillation:
  type: "standard_kd"
  # CE-only KD: no teacher logits available, so KL divergence disabled
  # This run validates the KD training loop, not perfect loss weighting
  kl_weight: 0.0
  ce_teacher_weight: 0.7  # Emphasize teacher guidance
  ce_ground_truth_weight: 0.3  # Ground truth still important
  # Process-step supervision weights (keep low for v0)
  w_tool: 0.0
  w_args: 0.0
  w_integr: 0.0

kd:
  teacher_endpoint: "https://api.moonshot.ai/v1"
  kd_temperature: 2.0
  teacher_logits_available: false  # Only teacher tokens/text for now

curriculum:
  schedule: []

tracing:
  log_dir: "runs"
  run_name: "worker_3b_kd_1500s"  # Unique run name
  use_tensorboard: false
  use_wandb: false
  json_log: true
  console_log: true

