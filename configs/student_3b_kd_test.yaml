arch:
  # 3B Student Model Configuration for first real KD test
  d_model: 2560
  n_layers: 20
  n_heads: 20
  n_kv_heads: 5
  d_head: 128
  vocab_size: 32000
  rope_theta: 10000
  rope_scaling: "dynamic"
  norm: "rms"
  mlp: "swiglu"
  dropout: 0.0

model:  # Alias for validation
  d_model: 2560
  n_layers: 20
  n_heads: 20
  n_kv_heads: 5
  d_head: 128
  vocab_size: 32000
  rope_theta: 10000
  rope_scaling: "dynamic"
  dropout: 0.0

init:
  base_checkpoint: null

optimizer:
  name: adamw
  lr: 2.0e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1

train:
  seq_lengths: [1024]  # Single context length for first 3B KD run
  micro_batch_size: 4
  grad_accum: 4
  steps: 1500
  total_steps: 1500
  save_every: 100
  log_every: 50
  fp16: true
  grad_checkpointing: true

training:  # Alias for validation
  steps: 1500
  batch_size: 4
  seq_length: 1024
  grad_accum_steps: 4
  lr: 2.0e-4
  save_every: 100

io:
  tokenizer_path: models/student/tokenizer
  train_shards: [data/kd_mix_1500.jsonl]
  val_shards: []

role: "worker"

distillation:
  type: "standard_kd"
  # CE-only KD: no teacher logits available, so KL divergence disabled
  # This run validates the KD training loop, not perfect loss weighting
  kl_weight: 0.0
  ce_teacher_weight: 0.7  # Emphasize teacher guidance
  ce_ground_truth_weight: 0.3  # Ground truth still important
  # Process-step supervision weights (keep low for v0)
  w_tool: 0.0
  w_args: 0.0
  w_integr: 0.0

kd:
  teacher_endpoint: "https://api.moonshot.ai/v1"
  kd_temperature: 2.0
  teacher_logits_available: false  # Only teacher tokens/text for now

curriculum:
  schedule: []

tracing:
  log_dir: "runs"
  run_name: "worker_3b_kd_1500s"  # Unique run name
  use_tensorboard: false
  use_wandb: false
  json_log: true
  console_log: true

