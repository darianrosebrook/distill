arch:
  d_model: 4096
  n_layers: 32
  n_heads: 32
  n_kv_heads: 8
  d_head: 128
  vocab_size: 32000
  rope_theta: 10000
  rope_scaling: "dynamic"
  norm: "rms"
  mlp: "swiglu"
  dropout: 0.0
init:
  base_checkpoint: null
optimizer:
  name: adamw
  lr: 2.0e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1
train:
  seq_lengths: [4096, 8192, 16384]  # Long context for 8-16k
  micro_batch_size: 2
  grad_accum: 16
  steps: 200000
  fp16: true
  grad_checkpointing: true
io:
  tokenizer_path: models/student/tokenizer  # HuggingFace tokenizer path (e.g., "meta-llama/Llama-2-7b-hf" or local path)
  train_shards: [data/kd_mix.jsonl]
  val_shards: [data/val.jsonl]
role: "worker"
distillation:
  type: "standard_kd"
  kl_weight: 0.5
  ce_teacher_weight: 0.3
  ce_ground_truth_weight: 0.2
  # Process-step supervision weights (replaces reasoning_content loss)
  w_tool: 0.15      # Tool name selection loss weight
  w_args: 0.15      # JSON argument loss weight
  w_integr: 0.10    # Integration span loss weight
kd:
  teacher_endpoint: "http://teacher.local"
  kd_temperature: 2.0
  teacher_logits_available: false
curriculum:
  schedule: [20000, 60000, 120000, 200000]  # Step boundaries for sequence length progression
tracing:
  log_dir: "runs"
  use_tensorboard: true
  use_wandb: false
  json_log: true
  console_log: true

