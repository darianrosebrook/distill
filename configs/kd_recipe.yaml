teacher:
  endpoint: "https://api.moonshot.cn/v1"  # Moonshot AI API (requires MOONSHOT_API_KEY env var)
  model: "kimi-k2-thinking"  # kimi-k2-thinking model for deep reasoning
  temperature: 1.0  # Recommended 1.0 for kimi-k2-thinking best performance
  top_p: 0.95
  max_tokens: 16384  # â‰¥16K recommended for full reasoning_content + content
losses:
  ce_on_teacher: true
  kl_div: true
  kd_temperature: 2.0
  kl_weight: 0.5
distill:
  code_mode:
    enabled: false  # Set to true to enable code-mode training (or use TRAIN_CODE_MODE=1)
    weight: 0.3
    weight_schedule:
      warmup_steps: 5000  # Ramp from 0.1 to weight over first 5k steps
      start_weight: 0.1
    code_mode_pref:
      eligibility_rules:
        min_tools: 2
        min_intermediate_chars: 10000
        pii_patterns: ["EMAIL", "PHONE", "SSN"]
      reward:
        prefer_ts_api_over_direct_tool: true
        penalize_tool_result_roundtrip: true
      weights:
        pos: 1.0
        neg: 1.0
sampling:
  mix:
    general: 0.5
    domain_specific: 0.3
    tool_traces: 0.2
data:
  curriculum:
    - name: mcp_code_mode
      p: 0.20
latent:
  enabled: false  # Set to true to enable latent training (or use TRAIN_LATENT=1)
  m: 2  # Steps to replace
  c: 1  # Latent slots (start with 1, enable c=2 when stable)
  p: 0.5  # Curriculum probability
  training_loops: 4  # More loops during training (4-6 range)
caching:
  enable_logit_cache: true
  cache_dir: data/logits/
