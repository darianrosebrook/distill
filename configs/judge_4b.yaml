arch:
  d_model: 2048
  n_layers: 24
  n_heads: 16
  n_kv_heads: 4
  d_head: 128
  vocab_size: 32000
  rope_theta: 10000
  rope_scaling: "dynamic"
  norm: "rms"
  mlp: "swiglu"
  dropout: 0.0
init:
  base_checkpoint: null
optimizer:
  name: adamw
  lr: 1.5e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1
train:
  seq_lengths: [512, 1024, 2048]  # Short context for summaries/claims
  micro_batch_size: 8
  grad_accum: 4
  steps: 100000
  fp16: true
  grad_checkpointing: true
tokenizer:
  path: models/student/tokenizer
role: "judge"
distillation:
  type: "pairwise_ranking"
  pairwise_weight: 0.6
  clause_labeling_weight: 0.4

