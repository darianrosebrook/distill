arch:
  # CoreML-friendly vanilla transformer
  d_model: 4096            # = n_heads * d_head
  n_layers: 36
  n_heads: 32
  n_kv_heads: 8            # GQA to shrink KV cache
  d_head: 128
  norm: rms
  mlp: swiglu
  mlp_hidden_mult: 3.5     # target ~14k hidden; exporter will round
  vocab_size: 32000
  rope:
    theta: 10000
    scaling: dynamic        # NTK-style scaling for long ctx

init:
  base_checkpoint: null     # or path to warm-start

optimizer:
  name: adamw
  lr: 2.0e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1
  grad_clip: 1.0

train:
  seq_lengths: [2048, 4096, 8192, 16384]  # curriculum controls schedule
  micro_batch_size: 4
  grad_accum: 8
  steps: 200000
  fp16: true
  grad_checkpointing: true
  seed: 1337
  save_every: 2000
  log_every: 50

curriculum:
  schedule: [20000, 60000, 120000, 200000]  # step boundaries per length
  mix_long_ctx_ratio: [0.1, 0.2, 0.3, 0.4]
  rope_scaling_consistency: true

kd:
  teacher_endpoint: "http://teacher.local"
  kd_temperature: 2.0
  kl_weight: 0.5
  ce_on_teacher: true
  # Process-step supervision weights (replaces reasoning_content loss)
  w_tool: 0.15      # Tool name selection loss weight
  w_args: 0.15      # JSON argument loss weight
  w_integr: 0.10    # Integration span loss weight

process_supervision:
  constrained_decoding: true
  schema_file: configs/tool_schema.json
  loss_json_validity_weight: 0.3
  loss_tool_select_weight: 0.7

quant:
  enabled: false            # turn on in quant_qat_int8.yaml stage
  weight_bits: 8
  act_bits: 8
  observers: minmax_per_channel
  clamp_pre_softmax: true
  fake_quant_in_attention: true

io:
  tokenizer_path: models/student/tokenizer
  train_shards: [data/kd_mix.jsonl]
  val_shards: [data/val.jsonl]

