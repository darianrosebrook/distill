# Toy Test Strategy

## Overview

**Toy tests** are lightweight, deterministic test suites that validate the distillation pipeline **without requiring expensive compute resources**. They enable fast iteration during development while ensuring production-ready integration when real models are available.

Toy tests simulate the full distillation and evaluation pipeline using minimal components that can run on CPU in seconds, providing confidence that the system works correctly before scaling to real models.

## Core Principles

### 1. **Deterministic & Fast**

- **Deterministic outputs**: Same inputs always produce identical results
- **CPU-only execution**: No GPU requirements
- **Sub-second to minutes**: Individual tests run quickly, full suites in <5 minutes
- **CI/CD ready**: Can run in automated pipelines without resource constraints

### 2. **Complete Pipeline Coverage**

- **Dataset generation** â†’ **Training** â†’ **Export** â†’ **Conversion** â†’ **Verification**
- **All major features** tested in isolation and combination
- **Real integration points** validated without real models

### 3. **Production Parity**

- **Same interfaces**: Toy components implement the same APIs as production components
- **Same validation**: Production gates and checks apply to toy tests
- **Easy migration**: Swap toy components for real ones with minimal code changes

## Toy Test Categories

### E2E Pipeline Tests (`tests/e2e/test_toy_pipeline.py`)

**Purpose**: Validate the complete distillation pipeline end-to-end.

**Test Variants**:

- `test_toy_pipeline_e2e`: Basic pipeline (dataset â†’ train â†’ export â†’ convert â†’ verify)
- `test_toy_pipeline_with_code_mode`: Code-mode enabled
- `test_toy_pipeline_with_latent_mode`: Latent reasoning enabled
- `test_toy_pipeline_with_both_features`: Combined code-mode + latent reasoning

**Components**:

- **Dataset**: Toy KD dataset (128 samples)
- **Training**: 2 epochs, minimal model (64 d_model, 1 layer)
- **Export**: TorchScript with enumerated shapes
- **Conversion**: CoreML (single shape, optional multi-shape)
- **Verification**: Contract validation with toy contracts

### Feature-Specific Tests

#### Code-Mode Tests (`tests/e2e/test_toy_code_mode.py`)

**Purpose**: Validate code-mode preference learning and TypeScript API orchestration.

**Key Tests**:

- `test_toy_training_with_code_mode_enabled`: Full code-mode training
- `test_toy_code_mode_with_span_targets`: Token-level span targeting
- `test_toy_code_mode_weight_scheduler_integration`: Linear warmup scheduling

**Validation**:

- Code-mode loss computation
- Eligibility rules (tool count, intermediate sizes, PII detection)
- TS API call generation within model outputs

#### Combined Milestones (`tests/e2e/test_toy_combined_milestones.py`)

**Purpose**: Test Milestone 1 (Code-Mode) + Milestone 2 (Latent Reasoning) integration.

**Key Tests**:

- `test_training_with_both_features`: Both features in same training run
- `test_code_mode_with_latent_spans`: TS API calls within latent reasoning spans
- `test_caws_budget_with_code_mode`: CAWS tier limits with code-mode scenarios

**Validation**:

- Feature interference detection
- CAWS budget enforcement
- Mixed batch eligibility handling

### Efficiency Measurement (`tests/e2e/test_toy_ces_measurement.py`)

**Purpose**: Measure Context Efficiency Score (CES) improvements from different features.

**Key Tests**:

- `test_ces_baseline_vs_code_mode`: Code-mode efficiency gains
- `test_ces_baseline_vs_latent`: Latent reasoning efficiency gains
- `test_ces_combined_milestones`: Additive efficiency improvements

**Metrics**:

- **Token reduction**: â‰¥25% at equal accuracy
- **Context pollution**: Tool results isolated from token stream
- **Efficiency curves**: Performance vs resource usage

### Claims Integration (`tests/test_claims_toy_model_integration.py`)

**Purpose**: Test claims verification pipeline with simulated model outputs.

**Test Doubles**:

- `ToyModelRunner`: Deterministic model inference simulator
- `ToyEntailmentJudge`: Keyword-based entailment classification
- `ToyEvidenceRetriever`: Simple evidence manifest adapter

**Validation**:

- Claims extraction from model outputs
- Evidence manifest construction
- Policy enforcement with model-generated claims
- Determinism across full pipeline

## Toy Components Architecture

### Test Doubles Pattern

Toy tests use **test doubles** that implement the same interfaces as production components but with simplified, deterministic behavior:

```python
class ToyModelRunner:
    """Deterministic model inference simulator."""

    def generate(self, prompt: str) -> Dict[str, Any]:
        """Return deterministic model output + tool traces."""
        # Keyword-based output generation
        if "tool.call" in prompt:
            return {
                "model_output": "tool.call{...} ok",
                "tool_trace": [{"name": "test_tool", "result": "success"}]
            }
        return {"model_output": "ok", "tool_trace": []}

class ToyEntailmentJudge:
    """Keyword-based entailment classification."""

    def triage(self, evidence: str, claim: str) -> Dict[str, float]:
        """Simple keyword overlap scoring."""
        overlap = len(set(evidence.split()) & set(claim.split()))
        return {
            "support": min(overlap * 0.1, 0.8),
            "contradict": 0.1,
            "insufficient": 0.9 - min(overlap * 0.1, 0.8)
        }
```

### Minimal Model Specifications

**Toy Models**: Designed for fast training while preserving architectural features

```python
# Minimal but complete transformer
cfg = ModelCfg(
    d_model=64,        # Very small hidden dimension
    n_layers=1,        # Single layer
    n_heads=2,         # Minimal attention heads
    vocab_size=512,    # Tiny vocabulary
    # ... other standard transformer config
)
```

**Toy Training**: Fast convergence with representative dynamics

```python
# Quick training for testing
training_config = {
    "epochs": 2,              # Minimal epochs
    "micro_batch_size": 4,    # Small batches
    "gradient_accumulation": 8,  # Effective batch size 32
    "learning_rate": 1e-3,    # Fast learning
}
```

## Quality Gates & Validation

### Toy-Specific Gates

**Pipeline Gates**:

- âœ… Dataset generation completes without errors
- âœ… Training converges (loss decreases)
- âœ… Model export produces valid TorchScript
- âœ… CoreML conversion succeeds (at least 1 shape)
- âœ… Contract verification passes (no NaN/zeros, F1 â‰¥ 0.20)

**Feature Gates**:

- âœ… Code-mode loss computes without errors
- âœ… Latent curriculum applies correctly
- âœ… CAWS budgets respected
- âœ… CES improvements measurable

### Determinism Validation

All toy tests include **determinism checks**:

```python
# Run same test twice
result1 = run_toy_test(seed=42)
result2 = run_toy_test(seed=42)

# Must be identical
assert result1 == result2, "Toy test not deterministic"
```

### Coverage Requirements

**Test Coverage**: 100% of toy components and integration points
**Feature Coverage**: All major features tested in isolation + combination
**Edge Case Coverage**: Error conditions, boundary values, malformed inputs

## Migration Strategy

### From Toy to Production

**Component Swapping**: Replace toy doubles with real implementations

```python
# Development (toy)
verifier = CAWSClaimVerification(
    retriever=ToyEvidenceRetriever(),
    entailment=ToyEntailmentJudge(),
)

# Production (real)
verifier = CAWSClaimVerification(
    retriever=EmbeddingRetriever(model="text-embedding-ada-002"),
    entailment=NLIEntailmentJudge(model="microsoft/DialoGPT-medium"),
)
```

**Same Test Structure**: Validation logic remains identical

```python
# Same test runs with toy or real components
def test_claims_verification():
    claim = AtomicClaim(...)
    evidence = {...}

    result = verifier.verify_claim_evidence(claim, evidence)

    # Same assertions work for toy and real
    assert result.status in ["VERIFIED", "UNVERIFIED", "INSUFFICIENT_EVIDENCE"]
    assert result.element_coverage is not None
```

### Gradual Migration Path

1. **Toy Components**: Full validation with test doubles
2. **Hybrid Testing**: Mix toy + real components
3. **Production Components**: Full real model validation
4. **Performance Optimization**: Real model efficiency tuning

## Running Toy Tests

### Individual Test Files

```bash
# All toy tests
pytest tests/ -k toy

# Specific categories
pytest tests/e2e/test_toy_pipeline.py -v
pytest tests/e2e/test_toy_code_mode.py -v
pytest tests/e2e/test_toy_combined_milestones.py -v
pytest tests/e2e/test_toy_ces_measurement.py -v
pytest tests/test_claims_toy_model_integration.py -v
```

### Makefile Targets

```bash
# E2E pipeline tests
make toy-e2e               # Single shape (fast)
make toy-e2e-multi         # Multiple shapes (thorough)

# Full toy test suite
pytest tests/ -k toy --tb=short
```

### CI/CD Integration

```yaml
# GitHub Actions example
- name: Run Toy Tests
  run: |
    make toy-e2e
    pytest tests/ -k toy --cov=training --cov=models --cov=evaluation
```

## Documentation & Coverage

### Coverage Analysis

See `tests/TOY_TEST_COVERAGE_ANALYSIS.md` for detailed coverage matrices:

- âœ… **Milestone 1**: Code-mode distillation (7 toy tests)
- âœ… **Milestone 2**: Latent reasoning (11 toy tests)
- âœ… **Combined**: Feature integration (5 toy tests)
- âœ… **Claims**: Framework integration (5 toy tests)

### Quality Metrics

**Test Quality**:

- **Determinism**: 100% (all tests reproducible)
- **Speed**: <5 seconds per test file
- **Coverage**: 100% of integration points
- **Migration Ready**: Zero code changes required for production

**CI Metrics**:

- **Pass Rate**: 100% (deterministic tests)
- **Runtime**: <4 minutes for full suite
- **Resource Usage**: CPU-only, minimal memory

## Recommended Practices

### Writing Toy Tests

1. **Use Test Doubles**: Implement minimal interfaces for fast execution
2. **Deterministic Seeds**: Always use fixed seeds for reproducible results
3. **Fast Convergence**: Configure training for quick convergence
4. **Same Assertions**: Use identical validation logic for toy and real tests
5. **Clear Naming**: `test_toy_*` prefix for easy filtering

### Maintenance

1. **Keep Simple**: Toy components should remain minimal and focused
2. **Update Together**: When production interfaces change, update toy doubles
3. **Regular Validation**: Ensure toy tests still pass with production changes
4. **Documentation Sync**: Keep coverage docs updated with new tests

## Summary

Toy tests provide **comprehensive, deterministic validation** of the distillation pipeline without expensive compute requirements. They enable fast development iteration while ensuring production readiness, with seamless migration paths to real models and components.

**Key Benefits**:

- âš¡ **Fast execution** (<5 minutes for full suite)
- ðŸŽ¯ **Complete coverage** (100% of integration points)
- ðŸ”’ **Deterministic results** (reproducible across environments)
- ðŸš€ **Production ready** (same interfaces, same validation)
- ðŸ“Š **Measurable quality** (CES improvements, efficiency metrics)

**See Also**:

- [`tests/TOY_TEST_COVERAGE_ANALYSIS.md`](tests/TOY_TEST_COVERAGE_ANALYSIS.md) - Detailed coverage analysis
- [`tests/TOY_MODEL_CLAIMS_INTEGRATION.md`](tests/TOY_MODEL_CLAIMS_INTEGRATION.md) - Claims framework integration
- [`README.md`](../README.md#toy-end-to-end-pipeline) - Quick start guide
