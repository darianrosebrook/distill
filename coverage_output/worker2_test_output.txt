============================= test session starts ==============================
platform darwin -- Python 3.11.13, pytest-9.0.1, pluggy-1.6.0 -- /Users/darianrosebrook/Desktop/Projects/distill/venv/bin/python3.11
cachedir: .pytest_cache
hypothesis profile 'default'
rootdir: /Users/darianrosebrook/Desktop/Projects/distill
configfile: pytest.ini
plugins: anyio-4.11.0, hypothesis-6.147.0, cov-7.0.0
collecting ... collected 189 items

tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_eight_ball_answers_count PASSED [  0%]
tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_eight_ball_answers_content PASSED [  1%]
tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_eight_ball_token_ids_range PASSED [  1%]
tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_id_to_answer_mapping FAILED [  2%]
tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_id_to_answer_coverage PASSED [  2%]
tests/evaluation/test_8ball_eval.py::TestDataClasses::test_prediction_result_creation FAILED [  3%]
tests/evaluation/test_8ball_eval.py::TestDataClasses::test_evaluation_metrics_creation FAILED [  3%]
tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_json_file FAILED [  4%]
tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_text_file FAILED [  4%]
tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_nonexistent_file FAILED [  5%]
tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_invalid_json PASSED [  5%]
tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_success FAILED [  6%]
tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_empty_questions FAILED [  6%]
tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_tokenizer_failure FAILED [  7%]
tests/evaluation/test_8ball_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_success FAILED [  7%]
tests/evaluation/test_8ball_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_file_not_found FAILED [  8%]
tests/evaluation/test_8ball_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_success PASSED [  8%]
tests/evaluation/test_8ball_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_subprocess_failure FAILED [  9%]
tests/evaluation/test_8ball_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_invalid_json FAILED [ 10%]
tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_identical FAILED [ 10%]
tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_different FAILED [ 11%]
tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_empty_lists FAILED [ 11%]
tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_different_lengths FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_token_distribution FAILED [ 12%]
tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_pytorch_evaluation FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_coreml_evaluation FAILED [ 13%]
tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_ollama_evaluation FAILED [ 14%]
tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_invalid_backend FAILED [ 14%]
tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_missing_eval_file FAILED [ 15%]
tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_eight_ball_answer_consistency PASSED [ 15%]
tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_prediction_result_validation FAILED [ 16%]
tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_evaluation_metrics_calculation FAILED [ 16%]
tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_evaluation_workflow FAILED [ 17%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_within_limits FAILED [ 17%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_exceeds_loc_limit FAILED [ 18%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_exceeds_files_limit FAILED [ 19%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_empty_diff FAILED [ 19%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_multiple_files FAILED [ 20%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_binary_files FAILED [ 20%]
tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_edge_cases FAILED [ 21%]
tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_all_pass FAILED [ 21%]
tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_tests_fail FAILED [ 22%]
tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_lint_fail FAILED [ 22%]
tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_coverage_fail FAILED [ 23%]
tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_missing_fields FAILED [ 23%]
tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_complete FAILED [ 24%]
tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_missing_rationale FAILED [ 24%]
tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_missing_evidence FAILED [ 25%]
tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_no_diff FAILED [ 25%]
tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_whitespace_only FAILED [ 26%]
tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_all_pass FAILED [ 26%]
tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_gates_fail FAILED [ 27%]
tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_provenance_fail FAILED [ 28%]
tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_budget_fail FAILED [ 28%]
tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_multiple_failures FAILED [ 29%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_working_spec_success PASSED [ 29%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_working_spec_not_found FAILED [ 30%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_file_content_success PASSED [ 30%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_file_content_not_found PASSED [ 31%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_json_file_success PASSED [ 31%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_json_file_invalid_json FAILED [ 32%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_tests_success FAILED [ 32%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_tests_failure FAILED [ 33%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_linter_success FAILED [ 33%]
tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_coverage_success FAILED [ 34%]
tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_success FAILED [ 34%]
tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_missing_spec FAILED [ 35%]
tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_test_failure FAILED [ 35%]
tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_complete_caws_evaluation_workflow FAILED [ 36%]
tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_caws_evaluation_with_violations FAILED [ 37%]
tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_budget_adherence_edge_cases FAILED [ 37%]
tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_gate_integrity_thresholds FAILED [ 38%]
tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_provenance_clarity_validation FAILED [ 38%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvalResult::test_claim_extraction_eval_result_creation PASSED [ 39%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvalResult::test_claim_extraction_eval_result_default_values PASSED [ 39%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvalResult::test_claim_extraction_eval_result_calculated_fields PASSED [ 40%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluator_initialization_with_extractor PASSED [ 40%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluator_initialization_default PASSED [ 41%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_success FAILED [ 41%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_empty_outputs PASSED [ 42%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_single_output PASSED [ 42%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_mismatched_lengths FAILED [ 43%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_with_none_outputs PASSED [ 43%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_extractor_failure PASSED [ 44%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_complete_evaluation_workflow FAILED [ 44%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_evaluation_with_realistic_outputs PASSED [ 45%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_metrics_calculation_edge_cases FAILED [ 46%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_evaluator_reuse PASSED [ 46%]
tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_evaluator_with_different_extractors PASSED [ 47%]
tests/evaluation/test_classification_eval.py::TestClassificationConfig::test_classification_config_creation PASSED [ 47%]
tests/evaluation/test_classification_eval.py::TestClassificationConfig::test_classification_config_validation PASSED [ 48%]
tests/evaluation/test_classification_eval.py::TestPredictionResult::test_prediction_result_creation PASSED [ 48%]
tests/evaluation/test_classification_eval.py::TestPredictionResult::test_prediction_result_without_probabilities PASSED [ 49%]
tests/evaluation/test_classification_eval.py::TestEvaluationMetrics::test_evaluation_metrics_creation FAILED [ 49%]
tests/evaluation/test_classification_eval.py::TestEvaluationMetrics::test_evaluation_metrics_minimal FAILED [ 50%]
tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_success FAILED [ 50%]
tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_file_not_found FAILED [ 51%]
tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_invalid_json FAILED [ 51%]
tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_missing_fields FAILED [ 52%]
tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_success FAILED [ 52%]
tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_empty_questions FAILED [ 53%]
tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_tokenizer_failure FAILED [ 53%]
tests/evaluation/test_classification_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_success FAILED [ 54%]
tests/evaluation/test_classification_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_file_not_found FAILED [ 55%]
tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_success FAILED [ 55%]
tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_subprocess_failure FAILED [ 56%]
tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_invalid_json FAILED [ 56%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_identical FAILED [ 57%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_different FAILED [ 57%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_empty_lists FAILED [ 58%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_different_lengths FAILED [ 58%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_with_probabilities FAILED [ 59%]
tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_without_probabilities FAILED [ 59%]
tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_pytorch_evaluation FAILED [ 60%]
tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_coreml_evaluation FAILED [ 60%]
tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_invalid_backend FAILED [ 61%]
tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_config_not_found FAILED [ 61%]
tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_complete_evaluation_workflow FAILED [ 62%]
tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_evaluation_metrics_calculation FAILED [ 62%]
tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_config_validation PASSED [ 63%]
tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_prediction_result_validation PASSED [ 64%]
tests/evaluation/test_perf_mem_eval.py::TestHardwareInfo::test_hardware_info_creation PASSED [ 64%]
tests/evaluation/test_perf_mem_eval.py::TestHardwareInfo::test_hardware_info_default_export_path PASSED [ 65%]
tests/evaluation/test_perf_mem_eval.py::TestStepAdapter::test_step_adapter_creation FAILED [ 65%]
tests/evaluation/test_perf_mem_eval.py::TestStepAdapter::test_step_adapter_defaults FAILED [ 66%]
tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_macos FAILED [ 66%]
tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_non_macos FAILED [ 67%]
tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_no_coremltools FAILED [ 67%]
tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_single_max PASSED [ 68%]
tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_multiple_same PASSED [ 68%]
tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_single_element PASSED [ 69%]
tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_empty_array FAILED [ 69%]
tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_negative_values FAILED [ 70%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_valid_simple PASSED [ 70%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_valid_complex PASSED [ 71%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_invalid_syntax FAILED [ 71%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_missing_fields FAILED [ 72%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_empty_json PASSED [ 73%]
tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_non_json PASSED [ 73%]
tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_success FAILED [ 74%]
tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_model_load_failure FAILED [ 74%]
tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_no_coremltools FAILED [ 75%]
tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_success FAILED [ 75%]
tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_file_not_found FAILED [ 76%]
tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_invalid_json FAILED [ 76%]
tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_missing_tokens_field FAILED [ 77%]
tests/evaluation/test_perf_mem_eval.py::TestMainFunction::test_main_success FAILED [ 77%]
tests/evaluation/test_perf_mem_eval.py::TestMainFunction::test_main_missing_required_args FAILED [ 78%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_hardware_detection_integration PASSED [ 78%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_tokenization_workflow FAILED [ 79%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_greedy_argmax_properties PASSED [ 79%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_json_validation_comprehensive FAILED [ 80%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_performance_metrics_calculation PASSED [ 80%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_end_to_end_workflow_simulation FAILED [ 81%]
tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_error_handling_robustness PASSED [ 82%]
tests/evaluation/test_tool_use_eval.py::TestLoadModel::test_load_model_with_config FAILED [ 82%]
tests/evaluation/test_tool_use_eval.py::TestLoadModel::test_load_model_without_config FAILED [ 83%]
tests/evaluation/test_tool_use_eval.py::TestLoadModel::test_load_model_checkpoint_not_found PASSED [ 83%]
tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_basic FAILED [ 84%]
tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_with_eos FAILED [ 84%]
tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_temperature FAILED [ 85%]
tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_max_length FAILED [ 85%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_valid_simple PASSED [ 86%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_valid_complex PASSED [ 86%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_invalid_syntax PASSED [ 87%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_empty_string PASSED [ 87%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_whitespace_only PASSED [ 88%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_non_json_text PASSED [ 88%]
tests/evaluation/test_tool_use_eval.py::TestValidateJSON::test_validate_json_partial_json PASSED [ 89%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_valid_json PASSED [ 89%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_nested_structure PASSED [ 90%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_invalid_json PASSED [ 91%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_no_tool_call PASSED [ 91%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_empty_json PASSED [ 92%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_malformed_tool_call PASSED [ 92%]
tests/evaluation/test_tool_use_eval.py::TestExtractToolCall::test_extract_tool_call_multiple_tools PASSED [ 93%]
tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_success FAILED [ 93%]
tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_invalid_json FAILED [ 94%]
tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_wrong_tool FAILED [ 94%]
tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_model_load_failure PASSED [ 95%]
tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_empty_test_cases FAILED [ 95%]
tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_success FAILED [ 96%]
tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_checkpoint_not_found FAILED [ 96%]
tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_config_not_found FAILED [ 97%]
tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_json_validation_edge_cases PASSED [ 97%]
tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_tool_call_extraction_variations PASSED [ 98%]
tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_complete_evaluation_workflow FAILED [ 98%]
tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_evaluation_metrics_calculation PASSED [ 99%]
tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_error_handling_robustness PASSED [100%]

=================================== FAILURES ===================================
_______________ TestEightBallConstants.test_id_to_answer_mapping _______________
tests/evaluation/test_8ball_eval.py:61: in test_id_to_answer_mapping
    assert ID_TO_ANSWER[210] == "Signs point to yes"
E   AssertionError: assert 'Reply hazy, try again' == 'Signs point to yes'
E     
E     - Signs point to yes
E     + Reply hazy, try again
_______________ TestDataClasses.test_prediction_result_creation ________________
tests/evaluation/test_8ball_eval.py:75: in test_prediction_result_creation
    result = PredictionResult(
E   TypeError: PredictionResult.__init__() got an unexpected keyword argument 'predicted_token'
_______________ TestDataClasses.test_evaluation_metrics_creation _______________
tests/evaluation/test_8ball_eval.py:91: in test_evaluation_metrics_creation
    metrics = EvaluationMetrics(
E   TypeError: EvaluationMetrics.__init__() got an unexpected keyword argument 'total_predictions'
___________ TestLoadEvalQuestions.test_load_eval_questions_json_file ___________
tests/evaluation/test_8ball_eval.py:118: in test_load_eval_questions_json_file
    result = load_eval_questions(json_file)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/8ball_eval.py:109: in load_eval_questions
    return data.get("questions", [])
           ^^^^^^^^
E   AttributeError: 'list' object has no attribute 'get'
___________ TestLoadEvalQuestions.test_load_eval_questions_text_file ___________
tests/evaluation/test_8ball_eval.py:130: in test_load_eval_questions_text_file
    result = load_eval_questions(text_file)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/8ball_eval.py:108: in load_eval_questions
    data = json.load(f)
           ^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:293: in load
    return loads(fp.read(),
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346: in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337: in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:355: in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
E   json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
_______ TestLoadEvalQuestions.test_load_eval_questions_nonexistent_file ________
tests/evaluation/test_8ball_eval.py:136: in test_load_eval_questions_nonexistent_file
    with pytest.raises(FileNotFoundError):
E   Failed: DID NOT RAISE <class 'FileNotFoundError'>
_________ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_success _________
tests/evaluation/test_8ball_eval.py:178: in test_evaluate_pytorch_model_success
    with (
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
_____ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_empty_questions _____
tests/evaluation/test_8ball_eval.py:198: in test_evaluate_pytorch_model_empty_questions
    with patch("evaluation.eightball_eval.AutoTokenizer"), patch("torch.no_grad"):
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
____ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_tokenizer_failure ____
tests/evaluation/test_8ball_eval.py:207: in test_evaluate_pytorch_model_tokenizer_failure
    with patch(
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
__________ TestEvaluateCoreMLModel.test_evaluate_coreml_model_success __________
tests/evaluation/test_8ball_eval.py:234: in test_evaluate_coreml_model_success
    with (
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
______ TestEvaluateCoreMLModel.test_evaluate_coreml_model_file_not_found _______
tests/evaluation/test_8ball_eval.py:255: in test_evaluate_coreml_model_file_not_found
    with patch("evaluation.eightball_eval.ctk.load", side_effect=FileNotFoundError):
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
____ TestEvaluateOllamaModel.test_evaluate_ollama_model_subprocess_failure _____
tests/evaluation/test_8ball_eval.py:312: in test_evaluate_ollama_model_subprocess_failure
    with pytest.raises(Exception):
E   Failed: DID NOT RAISE <class 'Exception'>
----------------------------- Captured stdout call -----------------------------
Error evaluating question 'Test question': argument of type 'Mock' is not iterable
_______ TestEvaluateOllamaModel.test_evaluate_ollama_model_invalid_json ________
tests/evaluation/test_8ball_eval.py:329: in test_evaluate_ollama_model_invalid_json
    with pytest.raises(json.JSONDecodeError):
E   Failed: DID NOT RAISE <class 'json.decoder.JSONDecodeError'>
__________ TestComparePredictions.test_compare_predictions_identical ___________
tests/evaluation/test_8ball_eval.py:339: in test_compare_predictions_identical
    PredictionResult("Q1", 200, "It is certain", 0.9, True),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
__________ TestComparePredictions.test_compare_predictions_different ___________
tests/evaluation/test_8ball_eval.py:357: in test_compare_predictions_different
    PredictionResult("Q1", 200, "It is certain", 0.9, True),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
_________ TestComparePredictions.test_compare_predictions_empty_lists __________
tests/evaluation/test_8ball_eval.py:373: in test_compare_predictions_empty_lists
    metrics = compare_predictions([], [])
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/8ball_eval.py:307: in compare_predictions
    exact_match_rate=exact_matches / len(reference),
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   ZeroDivisionError: division by zero
______ TestComparePredictions.test_compare_predictions_different_lengths _______
tests/evaluation/test_8ball_eval.py:381: in test_compare_predictions_different_lengths
    predictions1 = [PredictionResult("Q1", 200, "It is certain", 0.9, True)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
______ TestComparePredictions.test_compare_predictions_token_distribution ______
tests/evaluation/test_8ball_eval.py:393: in test_compare_predictions_token_distribution
    PredictionResult("Q1", 200, "It is certain", 0.9, True),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
________________ TestMainFunction.test_main_pytorch_evaluation _________________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
_________________ TestMainFunction.test_main_coreml_evaluation _________________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
_________________ TestMainFunction.test_main_ollama_evaluation _________________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
__________________ TestMainFunction.test_main_invalid_backend __________________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
_________________ TestMainFunction.test_main_missing_eval_file _________________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation' has no attribute 'eightball_eval'
__________ TestEightBallIntegration.test_prediction_result_validation __________
tests/evaluation/test_8ball_eval.py:569: in test_prediction_result_validation
    valid_result = PredictionResult(
E   TypeError: PredictionResult.__init__() got an unexpected keyword argument 'predicted_token'
_________ TestEightBallIntegration.test_evaluation_metrics_calculation _________
tests/evaluation/test_8ball_eval.py:585: in test_evaluation_metrics_calculation
    PredictionResult("Q1", 200, "It is certain", 0.9, True),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
______________ TestEightBallIntegration.test_evaluation_workflow _______________
tests/evaluation/test_8ball_eval.py:621: in test_evaluation_workflow
    loaded_questions = load_eval_questions(questions_file)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/8ball_eval.py:109: in load_eval_questions
    return data.get("questions", [])
           ^^^^^^^^
E   AttributeError: 'list' object has no attribute 'get'
___ TestValidateBudgetAdherence.test_validate_budget_adherence_within_limits ___
tests/evaluation/test_caws_eval.py:48: in test_validate_budget_adherence_within_limits
    assert result["files_changed_count"] == 1
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
_ TestValidateBudgetAdherence.test_validate_budget_adherence_exceeds_loc_limit _
tests/evaluation/test_caws_eval.py:65: in test_validate_budget_adherence_exceeds_loc_limit
    assert result["lines_removed"] == 1
E   assert 0 == 1
_ TestValidateBudgetAdherence.test_validate_budget_adherence_exceeds_files_limit _
tests/evaluation/test_caws_eval.py:89: in test_validate_budget_adherence_exceeds_files_limit
    assert result["files_changed_count"] == 3
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
____ TestValidateBudgetAdherence.test_validate_budget_adherence_empty_diff _____
tests/evaluation/test_caws_eval.py:100: in test_validate_budget_adherence_empty_diff
    assert result["files_changed_count"] == 1  # Default when no files detected
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
__ TestValidateBudgetAdherence.test_validate_budget_adherence_multiple_files ___
tests/evaluation/test_caws_eval.py:124: in test_validate_budget_adherence_multiple_files
    assert result["files_changed_count"] == 2
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
___ TestValidateBudgetAdherence.test_validate_budget_adherence_binary_files ____
tests/evaluation/test_caws_eval.py:136: in test_validate_budget_adherence_binary_files
    assert result["files_changed_count"] == 1
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
____ TestValidateBudgetAdherence.test_validate_budget_adherence_edge_cases _____
tests/evaluation/test_caws_eval.py:154: in test_validate_budget_adherence_edge_cases
    assert result["lines_removed"] == 1
E   assert 0 == 1
_______ TestValidateGateIntegrity.test_validate_gate_integrity_all_pass ________
tests/evaluation/test_caws_eval.py:185: in test_validate_gate_integrity_all_pass
    assert result["tests_pass"] == True
E   assert False == True
______ TestValidateGateIntegrity.test_validate_gate_integrity_tests_fail _______
tests/evaluation/test_caws_eval.py:199: in test_validate_gate_integrity_tests_fail
    assert result["overall_integrity"] == False
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'overall_integrity'
_______ TestValidateGateIntegrity.test_validate_gate_integrity_lint_fail _______
tests/evaluation/test_caws_eval.py:209: in test_validate_gate_integrity_lint_fail
    assert result["lint_clean"] == False
           ^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'lint_clean'
_____ TestValidateGateIntegrity.test_validate_gate_integrity_coverage_fail _____
tests/evaluation/test_caws_eval.py:220: in test_validate_gate_integrity_coverage_fail
    assert result["coverage_sufficient"] == False
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'coverage_sufficient'
____ TestValidateGateIntegrity.test_validate_gate_integrity_missing_fields _____
tests/evaluation/test_caws_eval.py:232: in test_validate_gate_integrity_missing_fields
    assert "overall_integrity" in result
E   AssertionError: assert 'overall_integrity' in {'tests_pass': False, 'lint_pass': False, 'coverage_pass': False, 'all_gates_pass': False}
___ TestValidateProvenanceClarity.test_validate_provenance_clarity_complete ____
tests/evaluation/test_caws_eval.py:245: in test_validate_provenance_clarity_complete
    result = validate_provenance_clarity(rationale, evidence, diff_present)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
_ TestValidateProvenanceClarity.test_validate_provenance_clarity_missing_rationale _
tests/evaluation/test_caws_eval.py:258: in test_validate_provenance_clarity_missing_rationale
    result = validate_provenance_clarity(rationale, evidence, diff_present)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
_ TestValidateProvenanceClarity.test_validate_provenance_clarity_missing_evidence _
tests/evaluation/test_caws_eval.py:269: in test_validate_provenance_clarity_missing_evidence
    result = validate_provenance_clarity(rationale, evidence, diff_present)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
____ TestValidateProvenanceClarity.test_validate_provenance_clarity_no_diff ____
tests/evaluation/test_caws_eval.py:280: in test_validate_provenance_clarity_no_diff
    result = validate_provenance_clarity(rationale, evidence, diff_present)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
_ TestValidateProvenanceClarity.test_validate_provenance_clarity_whitespace_only _
tests/evaluation/test_caws_eval.py:291: in test_validate_provenance_clarity_whitespace_only
    result = validate_provenance_clarity(rationale, evidence, diff_present)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
______ TestEvaluateCawsCompliance.test_evaluate_caws_compliance_all_pass _______
tests/evaluation/test_caws_eval.py:313: in test_evaluate_caws_compliance_all_pass
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
_____ TestEvaluateCawsCompliance.test_evaluate_caws_compliance_gates_fail ______
tests/evaluation/test_caws_eval.py:341: in test_evaluate_caws_compliance_gates_fail
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
___ TestEvaluateCawsCompliance.test_evaluate_caws_compliance_provenance_fail ___
tests/evaluation/test_caws_eval.py:364: in test_evaluate_caws_compliance_provenance_fail
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
_____ TestEvaluateCawsCompliance.test_evaluate_caws_compliance_budget_fail _____
tests/evaluation/test_caws_eval.py:387: in test_evaluate_caws_compliance_budget_fail
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
__ TestEvaluateCawsCompliance.test_evaluate_caws_compliance_multiple_failures __
tests/evaluation/test_caws_eval.py:412: in test_evaluate_caws_compliance_multiple_failures
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
_____________ TestHelperFunctions.test_load_working_spec_not_found _____________
tests/evaluation/test_caws_eval.py:449: in test_load_working_spec_not_found
    result = _load_working_spec("nonexistent.yaml")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:142: in _load_working_spec
    raise FileNotFoundError(f"Working spec not found: {spec_path}")
E   FileNotFoundError: Working spec not found: nonexistent.yaml
_____________ TestHelperFunctions.test_load_json_file_invalid_json _____________
tests/evaluation/test_caws_eval.py:489: in test_load_json_file_invalid_json
    result = _load_json_file(str(json_file))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/caws_eval.py:173: in _load_json_file
    return json.load(f)
           ^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:293: in load
    return loads(fp.read(),
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346: in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:337: in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:355: in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
E   json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
__________________ TestHelperFunctions.test_run_tests_success __________________
tests/evaluation/test_caws_eval.py:503: in test_run_tests_success
    assert result["passed"] == 150
           ^^^^^^^^^^^^^^^^
E   KeyError: 'passed'
__________________ TestHelperFunctions.test_run_tests_failure __________________
tests/evaluation/test_caws_eval.py:517: in test_run_tests_failure
    assert result["passed"] == 140
           ^^^^^^^^^^^^^^^^
E   KeyError: 'passed'
_________________ TestHelperFunctions.test_run_linter_success __________________
tests/evaluation/test_caws_eval.py:528: in test_run_linter_success
    result = _run_linter()
             ^^^^^^^^^^^^^
evaluation/caws_eval.py:213: in _run_linter
    "output": result.stdout + result.stderr,
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: can only concatenate str (not "Mock") to str
________________ TestHelperFunctions.test_run_coverage_success _________________
tests/evaluation/test_caws_eval.py:543: in test_run_coverage_success
    assert result["line_percent"] == 85.5
           ^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'line_percent'
______________________ TestMainFunction.test_main_success ______________________
tests/evaluation/test_caws_eval.py:589: in test_main_success
    main(
E   TypeError: main() got an unexpected keyword argument 'evidence'
___________________ TestMainFunction.test_main_missing_spec ____________________
tests/evaluation/test_caws_eval.py:607: in test_main_missing_spec
    main(change_diff="dummy diff", rationale="Test rationale", evidence="Test evidence")
E   TypeError: main() got an unexpected keyword argument 'evidence'
___________________ TestMainFunction.test_main_test_failure ____________________
tests/evaluation/test_caws_eval.py:618: in test_main_test_failure
    main(change_diff="dummy diff", rationale="Test rationale", evidence="Test evidence")
E   TypeError: main() got an unexpected keyword argument 'evidence'
________ TestCawsEvalIntegration.test_complete_caws_evaluation_workflow ________
tests/evaluation/test_caws_eval.py:639: in test_complete_caws_evaluation_workflow
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
_________ TestCawsEvalIntegration.test_caws_evaluation_with_violations _________
tests/evaluation/test_caws_eval.py:666: in test_caws_evaluation_with_violations
    result = evaluate_caws_compliance(
E   TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
___________ TestCawsEvalIntegration.test_budget_adherence_edge_cases ___________
tests/evaluation/test_caws_eval.py:695: in test_budget_adherence_edge_cases
    assert result["files_changed_count"] == 1
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'files_changed_count'
____________ TestCawsEvalIntegration.test_gate_integrity_thresholds ____________
tests/evaluation/test_caws_eval.py:714: in test_gate_integrity_thresholds
    assert result["overall_integrity"] == True
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'overall_integrity'
__________ TestCawsEvalIntegration.test_provenance_clarity_validation __________
tests/evaluation/test_caws_eval.py:724: in test_provenance_clarity_validation
    result = validate_provenance_clarity(
evaluation/caws_eval.py:77: in validate_provenance_clarity
    evidence_manifest is not None and len(evidence_manifest.get("evidence_items", [])) > 0
                                          ^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'get'
______________ TestClaimExtractionEvaluator.test_evaluate_success ______________
tests/evaluation/test_claim_extraction_metrics.py:159: in test_evaluate_success
    assert result.claim_ratio == 1.33
E   assert 1.3333333333333333 == 1.33
E    +  where 1.3333333333333333 = ClaimExtractionEvalResult(student_claim_count=4, teacher_claim_count=3, student_success_rate=0.75, teacher_success_rate=0.85, claim_ratio=1.3333333333333333, success_rate_ratio=0.8823529411764706, claim_extraction_loss=0.0).claim_ratio
________ TestClaimExtractionEvaluator.test_evaluate_mismatched_lengths _________
tests/evaluation/test_claim_extraction_metrics.py:231: in test_evaluate_mismatched_lengths
    result = evaluator.evaluate(student_outputs, teacher_outputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/claim_extraction_metrics.py:68: in evaluate
    raise ValueError(
E   ValueError: Student and teacher outputs must have same length: 3 != 2
___ TestClaimExtractionMetricsIntegration.test_complete_evaluation_workflow ____
tests/evaluation/test_claim_extraction_metrics.py:322: in test_complete_evaluation_workflow
    assert result.claim_ratio == 1.33
E   assert 1.3333333333333333 == 1.33
E    +  where 1.3333333333333333 = ClaimExtractionEvalResult(student_claim_count=8, teacher_claim_count=6, student_success_rate=0.75, teacher_success_rate=0.83, claim_ratio=1.3333333333333333, success_rate_ratio=0.9036144578313253, claim_extraction_loss=0.0).claim_ratio
__ TestClaimExtractionMetricsIntegration.test_metrics_calculation_edge_cases ___
tests/evaluation/test_claim_extraction_metrics.py:421: in test_metrics_calculation_edge_cases
    assert (
E   assert 1.0 < 0.001
E    +  where 1.0 = abs((1.0 - 0.0))
E    +    where 1.0 = ClaimExtractionEvalResult(student_claim_count=0, teacher_claim_count=0, student_success_rate=0.0, teacher_success_rate=0.0, claim_ratio=0.0, success_rate_ratio=0.0, claim_extraction_loss=1.0).claim_extraction_loss
____________ TestEvaluationMetrics.test_evaluation_metrics_creation ____________
tests/evaluation/test_classification_eval.py:105: in test_evaluation_metrics_creation
    metrics = EvaluationMetrics(
E   TypeError: EvaluationMetrics.__init__() got an unexpected keyword argument 'class_distribution'
____________ TestEvaluationMetrics.test_evaluation_metrics_minimal _____________
tests/evaluation/test_classification_eval.py:126: in test_evaluation_metrics_minimal
    assert metrics.class_distribution is None
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'EvaluationMetrics' object has no attribute 'class_distribution'
_____ TestLoadClassificationConfig.test_load_classification_config_success _____
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi0/config

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:145: in test_load_classification_config_success
    result = load_classification_config(str(config_file))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi0/config.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi0/config
_ TestLoadClassificationConfig.test_load_classification_config_file_not_found __
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: nonexistent

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:159: in test_load_classification_config_file_not_found
    load_classification_config("nonexistent.json")
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from nonexistent.json: Cannot find module: nonexistent
__ TestLoadClassificationConfig.test_load_classification_config_invalid_json ___
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi1/invalid

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:168: in test_load_classification_config_invalid_json
    load_classification_config(str(config_file))
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi1/invalid.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi1/invalid
_ TestLoadClassificationConfig.test_load_classification_config_missing_fields __
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi2/incomplete

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:179: in test_load_classification_config_missing_fields
    load_classification_config(str(config_file))
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi2/incomplete.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi2/incomplete
_________ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_success _________
tests/evaluation/test_classification_eval.py:219: in test_evaluate_pytorch_model_success
    with (
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.classification_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/classification_eval.py'> does not have the attribute 'AutoTokenizer'
_____ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_empty_questions _____
tests/evaluation/test_classification_eval.py:238: in test_evaluate_pytorch_model_empty_questions
    with patch("evaluation.classification_eval.AutoTokenizer"), patch("torch.no_grad"):
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.classification_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/classification_eval.py'> does not have the attribute 'AutoTokenizer'
____ TestEvaluatePyTorchModel.test_evaluate_pytorch_model_tokenizer_failure ____
tests/evaluation/test_classification_eval.py:247: in test_evaluate_pytorch_model_tokenizer_failure
    with patch(
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.classification_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/classification_eval.py'> does not have the attribute 'AutoTokenizer'
__________ TestEvaluateCoreMLModel.test_evaluate_coreml_model_success __________
tests/evaluation/test_classification_eval.py:278: in test_evaluate_coreml_model_success
    with (
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'ctk'
______ TestEvaluateCoreMLModel.test_evaluate_coreml_model_file_not_found _______
tests/evaluation/test_classification_eval.py:303: in test_evaluate_coreml_model_file_not_found
    with patch("evaluation.classification_eval.ctk.load", side_effect=FileNotFoundError):
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'ctk'
__________ TestEvaluateOllamaModel.test_evaluate_ollama_model_success __________
tests/evaluation/test_classification_eval.py:361: in test_evaluate_ollama_model_success
    assert results[1].predicted_class_name == "No"
E   AssertionError: assert 'Yes' == 'No'
E     
E     - No
E     + Yes
____ TestEvaluateOllamaModel.test_evaluate_ollama_model_subprocess_failure _____
tests/evaluation/test_classification_eval.py:373: in test_evaluate_ollama_model_subprocess_failure
    with pytest.raises(Exception):
E   Failed: DID NOT RAISE <class 'Exception'>
----------------------------- Captured stdout call -----------------------------
Error evaluating question 'Test question': argument of type 'Mock' is not iterable
_______ TestEvaluateOllamaModel.test_evaluate_ollama_model_invalid_json ________
tests/evaluation/test_classification_eval.py:390: in test_evaluate_ollama_model_invalid_json
    with pytest.raises(json.JSONDecodeError):
E   Failed: DID NOT RAISE <class 'json.decoder.JSONDecodeError'>
__________ TestComparePredictions.test_compare_predictions_identical ___________
tests/evaluation/test_classification_eval.py:419: in test_compare_predictions_identical
    metrics = compare_predictions(predictions1, predictions2, mock_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
__________ TestComparePredictions.test_compare_predictions_different ___________
tests/evaluation/test_classification_eval.py:437: in test_compare_predictions_different
    metrics = compare_predictions(predictions1, predictions2, mock_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
_________ TestComparePredictions.test_compare_predictions_empty_lists __________
tests/evaluation/test_classification_eval.py:445: in test_compare_predictions_empty_lists
    metrics = compare_predictions([], [], mock_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
______ TestComparePredictions.test_compare_predictions_different_lengths _______
tests/evaluation/test_classification_eval.py:456: in test_compare_predictions_different_lengths
    compare_predictions(predictions1, predictions2, mock_config)
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
______ TestComparePredictions.test_compare_predictions_with_probabilities ______
tests/evaluation/test_classification_eval.py:469: in test_compare_predictions_with_probabilities
    metrics = compare_predictions(predictions1, predictions2, mock_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
____ TestComparePredictions.test_compare_predictions_without_probabilities _____
tests/evaluation/test_classification_eval.py:480: in test_compare_predictions_without_probabilities
    metrics = compare_predictions(predictions1, predictions2, mock_config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: compare_predictions() takes 2 positional arguments but 3 were given
________________ TestMainFunction.test_main_pytorch_evaluation _________________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
_________________ TestMainFunction.test_main_coreml_evaluation _________________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
__________________ TestMainFunction.test_main_invalid_backend __________________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
_________________ TestMainFunction.test_main_config_not_found __________________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
_____ TestClassificationEvalIntegration.test_complete_evaluation_workflow ______
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_complete_evaluation_workf0/test_config

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:612: in test_complete_evaluation_workflow
    config = load_classification_config(str(config_file))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_complete_evaluation_workf0/test_config.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_complete_evaluation_workf0/test_config
____ TestClassificationEvalIntegration.test_evaluation_metrics_calculation _____
evaluation/classification_eval.py:74: in load_classification_config
    raise ImportError(f"Cannot find module: {module_path}")
E   ImportError: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_evaluation_metrics_calcul0/metrics_config

During handling of the above exception, another exception occurred:
tests/evaluation/test_classification_eval.py:638: in test_evaluation_metrics_calculation
    config = load_classification_config(str(config_file))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/classification_eval.py:86: in load_classification_config
    raise ValueError(f"Failed to load config from {config_path}: {e}")
E   ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_evaluation_metrics_calcul0/metrics_config.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_evaluation_metrics_calcul0/metrics_config
__________________ TestStepAdapter.test_step_adapter_creation __________________
tests/evaluation/test_perf_mem_eval.py:57: in test_step_adapter_creation
    adapter = StepAdapter(
E   TypeError: StepAdapter() takes no arguments
__________________ TestStepAdapter.test_step_adapter_defaults __________________
tests/evaluation/test_perf_mem_eval.py:73: in test_step_adapter_defaults
    adapter = StepAdapter(model_path="model.mlpackage", tokenizer_path="tokenizer.json")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: StepAdapter() takes no arguments
________________ TestDetectHardware.test_detect_hardware_macos _________________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.perf_mem_eval' has no attribute 'platform'
______________ TestDetectHardware.test_detect_hardware_non_macos _______________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.perf_mem_eval' has no attribute 'platform'
____________ TestDetectHardware.test_detect_hardware_no_coremltools ____________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1430: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pkgutil.py:715: in resolve_name
    result = getattr(result, p)
             ^^^^^^^^^^^^^^^^^^
E   AttributeError: module 'evaluation.perf_mem_eval' has no attribute 'platform'
_______________ TestGreedyArgmax.test_greedy_argmax_empty_array ________________
tests/evaluation/test_perf_mem_eval.py:165: in test_greedy_argmax_empty_array
    result = greedy_argmax(logits)
             ^^^^^^^^^^^^^^^^^^^^^
evaluation/perf_mem_eval.py:129: in greedy_argmax
    return int(np.argmax(logits))
               ^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:1229: in argmax
    return _wrapfunc(a, 'argmax', axis=axis, out=out, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: in _wrapfunc
    return bound(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^
E   ValueError: attempt to get argmax of an empty sequence
_____________ TestGreedyArgmax.test_greedy_argmax_negative_values ______________
tests/evaluation/test_perf_mem_eval.py:174: in test_greedy_argmax_negative_values
    assert result == 1  # Index of least negative (highest) value
    ^^^^^^^^^^^^^^^^^^
E   assert 3 == 1
__________ TestIsValidToolJSON.test_is_valid_tool_json_invalid_syntax __________
tests/evaluation/test_perf_mem_eval.py:217: in test_is_valid_tool_json_invalid_syntax
    assert result == False, f"Should reject invalid JSON: {invalid_json}"
E   AssertionError: Should reject invalid JSON: {"name": "calculator", "arguments": }
E   assert True == False
__________ TestIsValidToolJSON.test_is_valid_tool_json_missing_fields __________
tests/evaluation/test_perf_mem_eval.py:223: in test_is_valid_tool_json_missing_fields
    assert result == False
E   assert True == False
_______________ TestRunCoreMLSpeed.test_run_coreml_speed_success _______________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.perf_mem_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/perf_mem_eval.py'> does not have the attribute 'coremltools'
_________ TestRunCoreMLSpeed.test_run_coreml_speed_model_load_failure __________
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1375: in patched
    with self.decoration_helper(patched,
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137: in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1357: in decoration_helper
    arg = exit_stack.enter_context(patching)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:517: in enter_context
    result = _enter(cm)
             ^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1446: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:1419: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.perf_mem_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/perf_mem_eval.py'> does not have the attribute 'coremltools'
___________ TestRunCoreMLSpeed.test_run_coreml_speed_no_coremltools ____________
tests/evaluation/test_perf_mem_eval.py:292: in test_run_coreml_speed_no_coremltools
    run_coreml_speed("model.mlpackage", tokenized_prompts)
E   TypeError: run_coreml_speed() missing 1 required positional argument: 'adapter'
_________ TestLoadTokenizedPrompts.test_load_tokenized_prompts_success _________
tests/evaluation/test_perf_mem_eval.py:314: in test_load_tokenized_prompts_success
    result = load_tokenized_prompts(str(json_file))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
_____ TestLoadTokenizedPrompts.test_load_tokenized_prompts_file_not_found ______
tests/evaluation/test_perf_mem_eval.py:327: in test_load_tokenized_prompts_file_not_found
    load_tokenized_prompts("nonexistent.json")
E   TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
______ TestLoadTokenizedPrompts.test_load_tokenized_prompts_invalid_json _______
tests/evaluation/test_perf_mem_eval.py:336: in test_load_tokenized_prompts_invalid_json
    load_tokenized_prompts(str(json_file))
E   TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
__ TestLoadTokenizedPrompts.test_load_tokenized_prompts_missing_tokens_field ___
tests/evaluation/test_perf_mem_eval.py:351: in test_load_tokenized_prompts_missing_tokens_field
    load_tokenized_prompts(str(json_file))
E   TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
______________________ TestMainFunction.test_main_success ______________________
tests/evaluation/test_perf_mem_eval.py:396: in test_main_success
    main()
evaluation/perf_mem_eval.py:953: in main
    with args.out.open("w") as f:
E   TypeError: 'Mock' object does not support the context manager protocol
_______________ TestMainFunction.test_main_missing_required_args _______________
venv/lib/python3.11/site-packages/transformers/utils/hub.py:479: in cached_files
    hf_hub_download(
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106: in _inner_fn
    validate_repo_id(arg_value)
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160: in validate_repo_id
    raise HFValidationError(
E   huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13165020752'>'.

During handling of the above exception, another exception occurred:
evaluation/perf_mem_eval.py:441: in load_tokenized_prompts
    base_tokenizer = load_tokenizer(tokenizer_path)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
training/dataset.py:29: in load_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1073: in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:905: in get_tokenizer_config
    resolved_config_file = cached_file(
venv/lib/python3.11/site-packages/transformers/utils/hub.py:322: in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/utils/hub.py:531: in cached_files
    resolved_files = [
venv/lib/python3.11/site-packages/transformers/utils/hub.py:532: in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
venv/lib/python3.11/site-packages/transformers/utils/hub.py:143: in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106: in _inner_fn
    validate_repo_id(arg_value)
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160: in validate_repo_id
    raise HFValidationError(
E   huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13165020752'>'.

During handling of the above exception, another exception occurred:
tests/evaluation/test_perf_mem_eval.py:418: in test_main_missing_required_args
    main()
evaluation/perf_mem_eval.py:656: in main
    prompts = load_tokenized_prompts(
evaluation/perf_mem_eval.py:457: in load_tokenized_prompts
    raise RuntimeError(f"Failed to load tokenizer from {tokenizer_path}: {e}")
E   RuntimeError: Failed to load tokenizer from <Mock name='ArgumentParser().parse_args().tokenizer' id='13165020752'>: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13165020752'>'.
----------------------------- Captured stdout call -----------------------------
[perf_mem_eval] WARN: Failed to initialize prompt cache: unsupported operand type(s) for *: 'Mock' and 'int'
____________ TestPerfMemEvalIntegration.test_tokenization_workflow _____________
tests/evaluation/test_perf_mem_eval.py:450: in test_tokenization_workflow
    prompts = load_tokenized_prompts(str(json_file))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
________ TestPerfMemEvalIntegration.test_json_validation_comprehensive _________
tests/evaluation/test_perf_mem_eval.py:500: in test_json_validation_comprehensive
    assert is_valid_tool_json(pattern) == False
E   assert True == False
E    +  where True = is_valid_tool_json('{"name": "calculator"}')
________ TestPerfMemEvalIntegration.test_end_to_end_workflow_simulation ________
tests/evaluation/test_perf_mem_eval.py:567: in test_end_to_end_workflow_simulation
    main()
evaluation/perf_mem_eval.py:953: in main
    with args.out.open("w") as f:
E   TypeError: 'Mock' object does not support the context manager protocol
----------------------------- Captured stdout call -----------------------------
[perf_mem_eval] WARN: Failed to initialize prompt cache: unsupported operand type(s) for *: 'Mock' and 'int'
[perf_mem_eval] WARN: Failed to initialize optimized KV cache: '>' not supported between instances of 'Mock' and 'int'
[perf_mem_eval] WARN: Batch size <Mock name='ArgumentParser().parse_args().batch_size' id='13106606224'> not allowed: Batch size <Mock name='ArgumentParser().parse_args().batch_size' id='13106606224'> not in allowed list [2, 4]
[perf_mem_eval] Batch policy: workload_type=<Mock name='ArgumentParser().parse_args().workload_type' id='13106605840'>, batch_size=<Mock name='ArgumentParser().parse_args().batch_size' id='13106606224'>
[perf_mem_eval] WARN: Failed to initialize speculative decoder: [Errno 2] No such file or directory: "/Users/darianrosebrook/Desktop/Projects/distill/<Mock name='ArgumentParser().parse_args().drafter_model' id='6097732944'>"
[perf_mem_eval] WARN: Failed to load tokenizer, TTFA detection disabled: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13164774480'>'.
[perf_mem_eval] WARN: Failed to measure ANE residency: [Errno 2] No such file or directory: "/Users/darianrosebrook/Desktop/Projects/distill/<Mock name='ArgumentParser().parse_args().model' id='13153062992'>"
__________________ TestLoadModel.test_load_model_with_config ___________________
tests/evaluation/test_tool_use_eval.py:78: in test_load_model_with_config
    mock_model.load_state_dict.assert_called_once_with(mock_checkpoint["model_state_dict"])
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:951: in assert_called_once_with
    return self.assert_called_with(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:939: in assert_called_with
    raise AssertionError(_error_message()) from cause
E   AssertionError: expected call not found.
E   Expected: load_state_dict({'layer.weight': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])})
E     Actual: load_state_dict({'layer.weight': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
E           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])}, strict=False)
_________________ TestLoadModel.test_load_model_without_config _________________
tests/evaluation/test_tool_use_eval.py:106: in test_load_model_without_config
    assert result == mock_model
E   AssertionError: assert <Mock name='StudentLM().to()' id='13164981008'> == <Mock name='StudentLM()' id='13164983568'>
__________________ TestGenerateText.test_generate_text_basic ___________________
tests/evaluation/test_tool_use_eval.py:142: in test_generate_text_basic
    result = generate_text(mock_model, mock_tokenizer, prompt, max_length)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
evaluation/tool_use_eval.py:65: in generate_text
    device = next(model.parameters()).device
             ^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: 'Mock' object is not an iterator
_________________ TestGenerateText.test_generate_text_with_eos _________________
tests/evaluation/test_tool_use_eval.py:161: in test_generate_text_with_eos
    result = generate_text(mock_model, mock_tokenizer, prompt, max_length=10)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: generate_text() got an unexpected keyword argument 'max_length'
_______________ TestGenerateText.test_generate_text_temperature ________________
tests/evaluation/test_tool_use_eval.py:170: in test_generate_text_temperature
    result = generate_text(mock_model, mock_tokenizer, prompt, temperature=temperature)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: generate_text() got an unexpected keyword argument 'temperature'
________________ TestGenerateText.test_generate_text_max_length ________________
tests/evaluation/test_tool_use_eval.py:181: in test_generate_text_max_length
    result = generate_text(mock_model, mock_tokenizer, prompt, max_length=max_length)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: generate_text() got an unexpected keyword argument 'max_length'
______________ TestEvaluateToolUse.test_evaluate_tool_use_success ______________
tests/evaluation/test_tool_use_eval.py:399: in test_evaluate_tool_use_success
    results = evaluate_tool_use("dummy_checkpoint.pt", test_cases, device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
___________ TestEvaluateToolUse.test_evaluate_tool_use_invalid_json ____________
tests/evaluation/test_tool_use_eval.py:435: in test_evaluate_tool_use_invalid_json
    results = evaluate_tool_use("dummy_checkpoint.pt", test_cases, device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
____________ TestEvaluateToolUse.test_evaluate_tool_use_wrong_tool _____________
tests/evaluation/test_tool_use_eval.py:461: in test_evaluate_tool_use_wrong_tool
    results = evaluate_tool_use("dummy_checkpoint.pt", test_cases, device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
_________ TestEvaluateToolUse.test_evaluate_tool_use_empty_test_cases __________
tests/evaluation/test_tool_use_eval.py:484: in test_evaluate_tool_use_empty_test_cases
    results = evaluate_tool_use("dummy.pt", [], device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
______________________ TestMainFunction.test_main_success ______________________
venv/lib/python3.11/site-packages/transformers/utils/hub.py:479: in cached_files
    hf_hub_download(
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106: in _inner_fn
    validate_repo_id(arg_value)
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160: in validate_repo_id
    raise HFValidationError(
E   huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13164730192'>'.

During handling of the above exception, another exception occurred:
tests/evaluation/test_tool_use_eval.py:512: in test_main_success
    main()
evaluation/tool_use_eval.py:338: in main
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1073: in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:905: in get_tokenizer_config
    resolved_config_file = cached_file(
venv/lib/python3.11/site-packages/transformers/utils/hub.py:322: in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/utils/hub.py:531: in cached_files
    resolved_files = [
venv/lib/python3.11/site-packages/transformers/utils/hub.py:532: in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
venv/lib/python3.11/site-packages/transformers/utils/hub.py:143: in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106: in _inner_fn
    validate_repo_id(arg_value)
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160: in validate_repo_id
    raise HFValidationError(
E   huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13164730192'>'.
_______________ TestMainFunction.test_main_checkpoint_not_found ________________
venv/lib/python3.11/site-packages/transformers/utils/hub.py:479: in cached_files
    hf_hub_download(
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106: in _inner_fn
    validate_repo_id(arg_value)
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160: in validate_repo_id
    raise HFValidationError(
E   huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13103621200'>'.

During handling of the above exception, another exception occurred:
tests/evaluation/test_tool_use_eval.py:532: in test_main_checkpoint_not_found
    main()
evaluation/tool_use_eval.py:338: in main
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1073: in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:905: in get_tokenizer_config
    resolved_config_file = cached_file(
venv/lib/python3.11/site-packages/transformers/utils/hub.py:322: in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/utils/hub.py:531: in cached_files
    resolved_files = [
venv/lib/python3.11/site-packages/transformers/utils/hub.py:532: in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
venv/lib/python3.11/site-packages/transformers/utils/hub.py:143: in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106: in _inner_fn
    validate_repo_id(arg_value)
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160: in validate_repo_id
    raise HFValidationError(
E   huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13103621200'>'.
_________________ TestMainFunction.test_main_config_not_found __________________
venv/lib/python3.11/site-packages/transformers/utils/hub.py:479: in cached_files
    hf_hub_download(
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106: in _inner_fn
    validate_repo_id(arg_value)
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160: in validate_repo_id
    raise HFValidationError(
E   huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13169822480'>'.

During handling of the above exception, another exception occurred:
tests/evaluation/test_tool_use_eval.py:545: in test_main_config_not_found
    main()
evaluation/tool_use_eval.py:338: in main
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1073: in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:905: in get_tokenizer_config
    resolved_config_file = cached_file(
venv/lib/python3.11/site-packages/transformers/utils/hub.py:322: in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
venv/lib/python3.11/site-packages/transformers/utils/hub.py:531: in cached_files
    resolved_files = [
venv/lib/python3.11/site-packages/transformers/utils/hub.py:532: in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
venv/lib/python3.11/site-packages/transformers/utils/hub.py:143: in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106: in _inner_fn
    validate_repo_id(arg_value)
venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:160: in validate_repo_id
    raise HFValidationError(
E   huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13169822480'>'.
_________ TestToolUseEvalIntegration.test_complete_evaluation_workflow _________
tests/evaluation/test_tool_use_eval.py:642: in test_complete_evaluation_workflow
    results = evaluate_tool_use(str(config_file), test_config, device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
================================ tests coverage ================================
______________ coverage: platform darwin, python 3.11.13-final-0 _______________

Name                                              Stmts   Miss  Cover   Missing
-------------------------------------------------------------------------------
conversion/convert_coreml.py                        315    315     0%   13-665
conversion/export_onnx.py                            70     70     0%   4-151
conversion/export_pytorch.py                        152    152     0%   10-362
conversion/judge_export_coreml.py                    36     36     0%   5-86
conversion/judge_export_onnx.py                      43     43     0%   5-100
conversion/make_toy_block.py                         74     74     0%   11-122
conversion/make_toy_onnx.py                          35     35     0%   10-81
conversion/make_toy_torch.py                         55     55     0%   10-92
conversion/onnx_surgery.py                          116    116     0%   13-204
conversion/shape_validator.py                        60     60     0%   8-228
conversion/validators.py                              0      0   100%
evaluation/8ball_eval.py                            180    118    34%   81-105, 116-159, 166-210, 241-243, 278, 286-303, 314-415, 419
evaluation/__init__.py                                0      0   100%
evaluation/caws_eval.py                             169     89    47%   15-16, 34, 79-84, 105-123, 138, 151, 165, 169-170, 191-194, 216, 218-221, 250-255, 291-388, 392
evaluation/claim_extraction_metrics.py               67     26    61%   150-192, 201
evaluation/classification_eval.py                   223    154    31%   58, 64, 76-84, 93-136, 143-187, 219-221, 228-230, 264-292, 301-444, 448
evaluation/compare_8ball_pipelines.py                77     77     0%   12-207
evaluation/long_ctx_eval.py                           4      4     0%   1-6
evaluation/perf_mem_eval.py                         429    278    35%   15-17, 37-38, 55-57, 86, 104, 122, 156-412, 444-455, 459-524, 639, 648, 665-667, 689, 715, 720-723, 733-737, 746-782, 791-831, 837, 839, 845-859, 895-914, 928-929, 947, 954-955
evaluation/performance_benchmarks.py                 89     89     0%   10-350
evaluation/pipeline_preservation_eval.py            100    100     0%   15-259
evaluation/reasoning_eval.py                        106    106     0%   13-276
evaluation/tool_use_eval.py                         197    123    38%   53, 68-96, 121, 148, 170-309, 340-397, 401
evaluation/toy/__init__.py                            1      1     0%   8
evaluation/toy/binary_classifier.py                  23     23     0%   11-78
evaluation/toy/eight_ball.py                         30     30     0%   9-114
evaluation/toy/eight_ball_config.py                  22     22     0%   11-105
evaluation/toy/ternary_classifier.py                 23     23     0%   13-91
evaluation/toy_contracts.py                         151    151     0%   14-301
models/student/architectures/gqa_transformer.py     232    190    18%   33-35, 39-40, 45-48, 51-53, 62-65, 68-81, 85-97, 104-129, 142-155, 158-187, 206-240, 245-256, 259-261, 270-274, 284-308, 312, 350-396, 418-441, 454-458
training/assertions.py                               84     84     0%   8-217
training/caws_context.py                            136    136     0%   14-378
training/caws_structure.py                           41     41     0%   10-140
training/claim_extraction.py                         92     70    24%   69-104, 109-110, 115-127, 131-134, 139-158, 167-178, 189-213, 217-218, 226-235, 258-275
training/config_validation.py                        81     81     0%   7-301
training/dataloader.py                                6      6     0%   1-9
training/dataset.py                                 253    236     7%   18-19, 25, 30-32, 76-96, 100-138, 141, 155-338, 351-495
training/dataset_answer_generation.py                57     57     0%   18-166
training/dataset_post_tool.py                        55     55     0%   18-162
training/dataset_tool_select.py                      56     56     0%   18-169
training/distill_answer_generation.py                93     93     0%   7-232
training/distill_intermediate.py                     24     24     0%   18-54
training/distill_kd.py                             1251   1251     0%   13-2842
training/distill_post_tool.py                        93     93     0%   7-228
training/distill_process.py                         181    181     0%   12-412
training/distill_tool_select.py                     142    142     0%   11-345
training/examples_priority3_integration.py          122    122     0%   16-391
training/export_student.py                           81     81     0%   10-165
training/extractors.py                              125    125     0%   13-288
training/feature_flags.py                           127    127     0%   7-345
training/halt_targets.py                             46     46     0%   12-209
training/input_validation.py                        166    166     0%   7-422
training/json_repair.py                              91     91     0%   8-235
training/logging_utils.py                            58     58     0%   7-219
training/losses.py                                  282    282     0%   18-983
training/make_toy_training.py                        68     68     0%   16-242
training/monitoring.py                              191    191     0%   7-527
training/performance_monitor.py                      91     91     0%   7-237
training/process_losses.py                          174    174     0%   10-431
training/prompt_templates.py                        121    121     0%   11-594
training/quality_scoring.py                         115    115     0%   10-298
training/quant_qat_int8.py                          253    253     0%   5-545
training/run_manifest.py                            110    110     0%   16-255
training/run_toy_distill.py                         185    185     0%   11-425
training/speed_metrics.py                            65     65     0%   10-187
training/teacher_cache.py                           102    102     0%   12-236
training/teacher_stub_toy.py                         99     99     0%   15-283
training/tokenizer_migration.py                      95     95     0%   12-291
training/tracing.py                                 143    143     0%   30-369
training/utils.py                                    27     27     0%   7-64
-------------------------------------------------------------------------------
TOTAL                                              8661   8103     6%
Coverage HTML written to dir htmlcov/worker2
Coverage JSON written to file coverage_output/worker2_coverage.json
=========================== short test summary info ============================
FAILED tests/evaluation/test_8ball_eval.py::TestEightBallConstants::test_id_to_answer_mapping - AssertionError: assert 'Reply hazy, try again' == 'Signs point to yes'
  
  - Signs point to yes
  + Reply hazy, try again
FAILED tests/evaluation/test_8ball_eval.py::TestDataClasses::test_prediction_result_creation - TypeError: PredictionResult.__init__() got an unexpected keyword argument 'predicted_token'
FAILED tests/evaluation/test_8ball_eval.py::TestDataClasses::test_evaluation_metrics_creation - TypeError: EvaluationMetrics.__init__() got an unexpected keyword argument 'total_predictions'
FAILED tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_json_file - AttributeError: 'list' object has no attribute 'get'
FAILED tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_text_file - json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
FAILED tests/evaluation/test_8ball_eval.py::TestLoadEvalQuestions::test_load_eval_questions_nonexistent_file - Failed: DID NOT RAISE <class 'FileNotFoundError'>
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_success - AttributeError: module 'evaluation' has no attribute 'eightball_eval'
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_empty_questions - AttributeError: module 'evaluation' has no attribute 'eightball_eval'
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_tokenizer_failure - AttributeError: module 'evaluation' has no attribute 'eightball_eval'
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_success - AttributeError: module 'evaluation' has no attribute 'eightball_eval'
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_file_not_found - AttributeError: module 'evaluation' has no attribute 'eightball_eval'
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_subprocess_failure - Failed: DID NOT RAISE <class 'Exception'>
FAILED tests/evaluation/test_8ball_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_invalid_json - Failed: DID NOT RAISE <class 'json.decoder.JSONDecodeError'>
FAILED tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_identical - TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
FAILED tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_different - TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
FAILED tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_empty_lists - ZeroDivisionError: division by zero
FAILED tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_different_lengths - TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
FAILED tests/evaluation/test_8ball_eval.py::TestComparePredictions::test_compare_predictions_token_distribution - TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
FAILED tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_pytorch_evaluation - AttributeError: module 'evaluation' has no attribute 'eightball_eval'
FAILED tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_coreml_evaluation - AttributeError: module 'evaluation' has no attribute 'eightball_eval'
FAILED tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_ollama_evaluation - AttributeError: module 'evaluation' has no attribute 'eightball_eval'
FAILED tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_invalid_backend - AttributeError: module 'evaluation' has no attribute 'eightball_eval'
FAILED tests/evaluation/test_8ball_eval.py::TestMainFunction::test_main_missing_eval_file - AttributeError: module 'evaluation' has no attribute 'eightball_eval'
FAILED tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_prediction_result_validation - TypeError: PredictionResult.__init__() got an unexpected keyword argument 'predicted_token'
FAILED tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_evaluation_metrics_calculation - TypeError: PredictionResult.__init__() takes from 4 to 5 positional arguments but 6 were given
FAILED tests/evaluation/test_8ball_eval.py::TestEightBallIntegration::test_evaluation_workflow - AttributeError: 'list' object has no attribute 'get'
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_within_limits - KeyError: 'files_changed_count'
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_exceeds_loc_limit - assert 0 == 1
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_exceeds_files_limit - KeyError: 'files_changed_count'
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_empty_diff - KeyError: 'files_changed_count'
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_multiple_files - KeyError: 'files_changed_count'
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_binary_files - KeyError: 'files_changed_count'
FAILED tests/evaluation/test_caws_eval.py::TestValidateBudgetAdherence::test_validate_budget_adherence_edge_cases - assert 0 == 1
FAILED tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_all_pass - assert False == True
FAILED tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_tests_fail - KeyError: 'overall_integrity'
FAILED tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_lint_fail - KeyError: 'lint_clean'
FAILED tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_coverage_fail - KeyError: 'coverage_sufficient'
FAILED tests/evaluation/test_caws_eval.py::TestValidateGateIntegrity::test_validate_gate_integrity_missing_fields - AssertionError: assert 'overall_integrity' in {'tests_pass': False, 'lint_pass': False, 'coverage_pass': False, 'all_gates_pass': False}
FAILED tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_complete - AttributeError: 'str' object has no attribute 'get'
FAILED tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_missing_rationale - AttributeError: 'str' object has no attribute 'get'
FAILED tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_missing_evidence - AttributeError: 'str' object has no attribute 'get'
FAILED tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_no_diff - AttributeError: 'str' object has no attribute 'get'
FAILED tests/evaluation/test_caws_eval.py::TestValidateProvenanceClarity::test_validate_provenance_clarity_whitespace_only - AttributeError: 'str' object has no attribute 'get'
FAILED tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_all_pass - TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
FAILED tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_gates_fail - TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
FAILED tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_provenance_fail - TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
FAILED tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_budget_fail - TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
FAILED tests/evaluation/test_caws_eval.py::TestEvaluateCawsCompliance::test_evaluate_caws_compliance_multiple_failures - TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_working_spec_not_found - FileNotFoundError: Working spec not found: nonexistent.yaml
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_load_json_file_invalid_json - json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_tests_success - KeyError: 'passed'
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_tests_failure - KeyError: 'passed'
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_linter_success - TypeError: can only concatenate str (not "Mock") to str
FAILED tests/evaluation/test_caws_eval.py::TestHelperFunctions::test_run_coverage_success - KeyError: 'line_percent'
FAILED tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_success - TypeError: main() got an unexpected keyword argument 'evidence'
FAILED tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_missing_spec - TypeError: main() got an unexpected keyword argument 'evidence'
FAILED tests/evaluation/test_caws_eval.py::TestMainFunction::test_main_test_failure - TypeError: main() got an unexpected keyword argument 'evidence'
FAILED tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_complete_caws_evaluation_workflow - TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
FAILED tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_caws_evaluation_with_violations - TypeError: evaluate_caws_compliance() got an unexpected keyword argument 'evidence'
FAILED tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_budget_adherence_edge_cases - KeyError: 'files_changed_count'
FAILED tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_gate_integrity_thresholds - KeyError: 'overall_integrity'
FAILED tests/evaluation/test_caws_eval.py::TestCawsEvalIntegration::test_provenance_clarity_validation - AttributeError: 'str' object has no attribute 'get'
FAILED tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_success - assert 1.3333333333333333 == 1.33
 +  where 1.3333333333333333 = ClaimExtractionEvalResult(student_claim_count=4, teacher_claim_count=3, student_success_rate=0.75, teacher_success_rate=0.85, claim_ratio=1.3333333333333333, success_rate_ratio=0.8823529411764706, claim_extraction_loss=0.0).claim_ratio
FAILED tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionEvaluator::test_evaluate_mismatched_lengths - ValueError: Student and teacher outputs must have same length: 3 != 2
FAILED tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_complete_evaluation_workflow - assert 1.3333333333333333 == 1.33
 +  where 1.3333333333333333 = ClaimExtractionEvalResult(student_claim_count=8, teacher_claim_count=6, student_success_rate=0.75, teacher_success_rate=0.83, claim_ratio=1.3333333333333333, success_rate_ratio=0.9036144578313253, claim_extraction_loss=0.0).claim_ratio
FAILED tests/evaluation/test_claim_extraction_metrics.py::TestClaimExtractionMetricsIntegration::test_metrics_calculation_edge_cases - assert 1.0 < 0.001
 +  where 1.0 = abs((1.0 - 0.0))
 +    where 1.0 = ClaimExtractionEvalResult(student_claim_count=0, teacher_claim_count=0, student_success_rate=0.0, teacher_success_rate=0.0, claim_ratio=0.0, success_rate_ratio=0.0, claim_extraction_loss=1.0).claim_extraction_loss
FAILED tests/evaluation/test_classification_eval.py::TestEvaluationMetrics::test_evaluation_metrics_creation - TypeError: EvaluationMetrics.__init__() got an unexpected keyword argument 'class_distribution'
FAILED tests/evaluation/test_classification_eval.py::TestEvaluationMetrics::test_evaluation_metrics_minimal - AttributeError: 'EvaluationMetrics' object has no attribute 'class_distribution'
FAILED tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_success - ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi0/config.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi0/config
FAILED tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_file_not_found - ValueError: Failed to load config from nonexistent.json: Cannot find module: nonexistent
FAILED tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_invalid_json - ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi1/invalid.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi1/invalid
FAILED tests/evaluation/test_classification_eval.py::TestLoadClassificationConfig::test_load_classification_config_missing_fields - ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi2/incomplete.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_load_classification_confi2/incomplete
FAILED tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_success - AttributeError: <module 'evaluation.classification_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/classification_eval.py'> does not have the attribute 'AutoTokenizer'
FAILED tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_empty_questions - AttributeError: <module 'evaluation.classification_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/classification_eval.py'> does not have the attribute 'AutoTokenizer'
FAILED tests/evaluation/test_classification_eval.py::TestEvaluatePyTorchModel::test_evaluate_pytorch_model_tokenizer_failure - AttributeError: <module 'evaluation.classification_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/classification_eval.py'> does not have the attribute 'AutoTokenizer'
FAILED tests/evaluation/test_classification_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_success - AttributeError: module 'evaluation.classification_eval' has no attribute 'ctk'
FAILED tests/evaluation/test_classification_eval.py::TestEvaluateCoreMLModel::test_evaluate_coreml_model_file_not_found - AttributeError: module 'evaluation.classification_eval' has no attribute 'ctk'
FAILED tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_success - AssertionError: assert 'Yes' == 'No'
  
  - No
  + Yes
FAILED tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_subprocess_failure - Failed: DID NOT RAISE <class 'Exception'>
FAILED tests/evaluation/test_classification_eval.py::TestEvaluateOllamaModel::test_evaluate_ollama_model_invalid_json - Failed: DID NOT RAISE <class 'json.decoder.JSONDecodeError'>
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_identical - TypeError: compare_predictions() takes 2 positional arguments but 3 were given
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_different - TypeError: compare_predictions() takes 2 positional arguments but 3 were given
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_empty_lists - TypeError: compare_predictions() takes 2 positional arguments but 3 were given
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_different_lengths - TypeError: compare_predictions() takes 2 positional arguments but 3 were given
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_with_probabilities - TypeError: compare_predictions() takes 2 positional arguments but 3 were given
FAILED tests/evaluation/test_classification_eval.py::TestComparePredictions::test_compare_predictions_without_probabilities - TypeError: compare_predictions() takes 2 positional arguments but 3 were given
FAILED tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_pytorch_evaluation - AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
FAILED tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_coreml_evaluation - AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
FAILED tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_invalid_backend - AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
FAILED tests/evaluation/test_classification_eval.py::TestMainFunction::test_main_config_not_found - AttributeError: module 'evaluation.classification_eval' has no attribute 'argparse'
FAILED tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_complete_evaluation_workflow - ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_complete_evaluation_workf0/test_config.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_complete_evaluation_workf0/test_config
FAILED tests/evaluation/test_classification_eval.py::TestClassificationEvalIntegration::test_evaluation_metrics_calculation - ValueError: Failed to load config from /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_evaluation_metrics_calcul0/metrics_config.json: Cannot find module: /private/var/folders/2y/lzlll42d12g1gz6l_chkd9500000gn/T/pytest-of-darianrosebrook/pytest-29/test_evaluation_metrics_calcul0/metrics_config
FAILED tests/evaluation/test_perf_mem_eval.py::TestStepAdapter::test_step_adapter_creation - TypeError: StepAdapter() takes no arguments
FAILED tests/evaluation/test_perf_mem_eval.py::TestStepAdapter::test_step_adapter_defaults - TypeError: StepAdapter() takes no arguments
FAILED tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_macos - AttributeError: module 'evaluation.perf_mem_eval' has no attribute 'platform'
FAILED tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_non_macos - AttributeError: module 'evaluation.perf_mem_eval' has no attribute 'platform'
FAILED tests/evaluation/test_perf_mem_eval.py::TestDetectHardware::test_detect_hardware_no_coremltools - AttributeError: module 'evaluation.perf_mem_eval' has no attribute 'platform'
FAILED tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_empty_array - ValueError: attempt to get argmax of an empty sequence
FAILED tests/evaluation/test_perf_mem_eval.py::TestGreedyArgmax::test_greedy_argmax_negative_values - assert 3 == 1
FAILED tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_invalid_syntax - AssertionError: Should reject invalid JSON: {"name": "calculator", "arguments": }
assert True == False
FAILED tests/evaluation/test_perf_mem_eval.py::TestIsValidToolJSON::test_is_valid_tool_json_missing_fields - assert True == False
FAILED tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_success - AttributeError: <module 'evaluation.perf_mem_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/perf_mem_eval.py'> does not have the attribute 'coremltools'
FAILED tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_model_load_failure - AttributeError: <module 'evaluation.perf_mem_eval' from '/Users/darianrosebrook/Desktop/Projects/distill/evaluation/perf_mem_eval.py'> does not have the attribute 'coremltools'
FAILED tests/evaluation/test_perf_mem_eval.py::TestRunCoreMLSpeed::test_run_coreml_speed_no_coremltools - TypeError: run_coreml_speed() missing 1 required positional argument: 'adapter'
FAILED tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_success - TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
FAILED tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_file_not_found - TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
FAILED tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_invalid_json - TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
FAILED tests/evaluation/test_perf_mem_eval.py::TestLoadTokenizedPrompts::test_load_tokenized_prompts_missing_tokens_field - TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
FAILED tests/evaluation/test_perf_mem_eval.py::TestMainFunction::test_main_success - TypeError: 'Mock' object does not support the context manager protocol
FAILED tests/evaluation/test_perf_mem_eval.py::TestMainFunction::test_main_missing_required_args - RuntimeError: Failed to load tokenizer from <Mock name='ArgumentParser().parse_args().tokenizer' id='13165020752'>: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13165020752'>'.
FAILED tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_tokenization_workflow - TypeError: load_tokenized_prompts() missing 1 required positional argument: 'tokenizer_path'
FAILED tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_json_validation_comprehensive - assert True == False
 +  where True = is_valid_tool_json('{"name": "calculator"}')
FAILED tests/evaluation/test_perf_mem_eval.py::TestPerfMemEvalIntegration::test_end_to_end_workflow_simulation - TypeError: 'Mock' object does not support the context manager protocol
FAILED tests/evaluation/test_tool_use_eval.py::TestLoadModel::test_load_model_with_config - AssertionError: expected call not found.
Expected: load_state_dict({'layer.weight': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])})
  Actual: load_state_dict({'layer.weight': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])}, strict=False)
FAILED tests/evaluation/test_tool_use_eval.py::TestLoadModel::test_load_model_without_config - AssertionError: assert <Mock name='StudentLM().to()' id='13164981008'> == <Mock name='StudentLM()' id='13164983568'>
FAILED tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_basic - TypeError: 'Mock' object is not an iterator
FAILED tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_with_eos - TypeError: generate_text() got an unexpected keyword argument 'max_length'
FAILED tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_temperature - TypeError: generate_text() got an unexpected keyword argument 'temperature'
FAILED tests/evaluation/test_tool_use_eval.py::TestGenerateText::test_generate_text_max_length - TypeError: generate_text() got an unexpected keyword argument 'max_length'
FAILED tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_success - TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
FAILED tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_invalid_json - TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
FAILED tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_wrong_tool - TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
FAILED tests/evaluation/test_tool_use_eval.py::TestEvaluateToolUse::test_evaluate_tool_use_empty_test_cases - TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
FAILED tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_success - huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13164730192'>'.
FAILED tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_checkpoint_not_found - huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13103621200'>'.
FAILED tests/evaluation/test_tool_use_eval.py::TestMainFunction::test_main_config_not_found - huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: '<Mock name='ArgumentParser().parse_args().tokenizer' id='13169822480'>'.
FAILED tests/evaluation/test_tool_use_eval.py::TestToolUseEvalIntegration::test_complete_evaluation_workflow - TypeError: evaluate_tool_use() missing 1 required positional argument: 'device'
======================= 127 failed, 62 passed in 11.24s ========================
