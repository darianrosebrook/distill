{
  "prompt": "Search for best practices for training neural networks and create a summary.",
  "teacher_text": "# Best Practices for Training Neural Networks: A Comprehensive Summary\n\n## 1. Data Preparation & Management\n- **Data Quality First**: Ensure clean, well-labeled data. Garbage in, garbage out.\n- **Normalization/Standardization**: Scale inputs to similar ranges (zero mean, unit variance) for faster convergence.\n- **Data Augmentation**: Artificially expand training data (rotations, flips, noise) to improve generalization, especially for images.\n- **Strategic Splits**: Use 70/15/15 or 80/10/10 splits for train/validation/test. Ensure data distribution consistency across splits.\n- **Address Class Imbalance**: Use stratified sampling, weighted loss functions, or oversampling techniques.\n\n## 2. Architecture & Initialization\n- **Start Simple**: Begin with a small, basic architecture and increase complexity gradually.\n- **Leverage Pre-trained Models**: Use transfer learning from established architectures (ResNet, BERT) when possible.\n- **Match Capacity to Problem**: Avoid overly complex models that overfit or too simple models that underfit.\n- **Proper Initialization**: Use He initialization for ReLU variants, Xavier for tanh/sigmoid to prevent vanishing/exploding gradients.\n\n## 3. Training Configuration\n- **Optimizer Choice**: Adam/AdamW is a good default; SGD with momentum often achieves better final performance but requires tuning.\n- **Learning Rate Strategy**: \n  - Start with 0.001 (Adam) or 0.1 (SGD)\n  - Use learning rate scheduling (cosine annealing, step decay, ReduceLROnPlateau)\n  - Consider warm restarts\n- **Batch Size**: Larger batches (32-256) for stability; smaller batches (16-32) for generalization. Use largest batch your GPU allows.\n- **Gradient Clipping**: Prevent exploding gradients, especially in RNNs (clip at 1-5 range).\n\n## 4. Regularization Techniques\n- **Early Stopping**: Monitor validation loss and stop when it plateaus to prevent overfitting.\n- **Dropout**: Use 0.2-0.5 for hidden layers; consider spatial dropout for convolutions.\n- **Weight Decay**: L2 regularization (1e-4 to 1e-2) to constrain weight magnitudes.\n- **Batch Normalization**: Stabilize training, allow higher learning rates, act as regularization.\n- **Label Smoothing**: Prevent overconfidence in predictions (use 0.1 smoothing).\n\n## 5. Hyperparameter Tuning\n- **Prioritize**: Focus on learning rate, batch size, and network depth first.\n- **Systematic Search**: Use random search over grid search; Bayesian optimization for efficiency.\n- **One at a Time**: Change one hyperparameter per experiment to isolate effects.\n- **Cross-Validation**: Use k-fold CV for small datasets.\n\n## 6. Monitoring & Debugging\n- **Overfit Single Batch First**: Verify model can learn by overfitting a tiny dataset (sanity check).\n- **Track Multiple Metrics**: Monitor training/",
  "teacher_logits": null,
  "metadata": {
    "temperature": 1.0,
    "top_p": 0.95,
    "max_tokens": 1024
  }
}